{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  261034\n",
      "Added data without URLs!\n"
     ]
    }
   ],
   "source": [
    "#DEVELOPING DATASET FOR EACH YEAR\n",
    "#Goal: \n",
    "\n",
    "#To link [EIN, URL] from IRS to [EIN, NTEE1] from NCCS files, and get Mission or Purpose statements for each EIN.\n",
    "#To- do List: \n",
    "\n",
    "#[+] 1. Get [EIN, URL] from IRS. -> df_irs \n",
    "#[+] 2. Get [EIN, NTEE1] from NCCS. -> df_nccs \n",
    "#[+] 3. Intersect [df_irs, df_nccs] -> df_inter -> \"link'year'.csv\" \n",
    "#[+] 4. Visit each URL in df_inter and get data from relevant tabs. \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re, requests, string, os, gzip, pickle, sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "#use regex instead of beautifulsoup, if possible.\n",
    "\n",
    "year_list = [2015, 2014, 2013, 2012, 2011]\n",
    "year = year_list[0]\n",
    "\n",
    "#When we create a data frame with pandas â‰¤ 0.19.2 and pickle it (using pickle.dump), \n",
    "#it is not possible to unpickle it using pandas 0.20.1.\n",
    "#https://github.com/pandas-dev/pandas/issues/16474\n",
    "\n",
    "\n",
    "#Create the file if it does not exist, if the file exists: jump to step 4\n",
    "if(os.path.isfile(\"../Data/intermediary/link\"+str(year)+\".pkl.gz\")==False):\n",
    "    #Step 1. Get [EIN, URL] from IRS\n",
    "    irsfile = pd.read_json('https://s3.amazonaws.com/irs-form-990/index_'+str(year)+'.json')\n",
    "    ein_url=list(map(list, zip(*[[s['EIN'] for s in irsfile['Filings'+str(year)]], [s['URL'] for s in irsfile['Filings'+str(year)]]])))\n",
    "    df_irs = pd.DataFrame(ein_url, columns=['EIN', 'URL'])\n",
    "\n",
    "\n",
    "    #Step 2. Get [EIN, NTEE1] from NCCS\n",
    "    df_nccs1 = pd.read_csv('https://nccs-data.urban.org/data/core/'+str(year)+'/nccs.core'+str(year)+'pc.csv')\n",
    "    df_nccs = df_nccs1[['EIN', 'NTEE1']]\n",
    "\n",
    "\n",
    "    #Step 3. Get common URL from df_irs and df_nccs and make corresponding list of [EIN, URL, NTEE1]\n",
    "    df_nccs['EIN'] = df_nccs['EIN'].apply(str)\n",
    "    df_inter = pd.DataFrame(pd.merge(df_nccs, df_irs, how='outer', on=['EIN']), columns=['EIN','NTEE1','URL'])\n",
    "    df_inter.columns = ['EIN', 'NTEE', 'IRS_URL']\n",
    "\n",
    "    #file size limit on Github - 25 MB\n",
    "    df_inter.to_pickle('../Data/intermediary/link'+str(year)+'.pkl.gz', 'gzip')\n",
    "    \n",
    "    del irsfile, ein_url, df_irs, df_nccs1, df_nccs, df_inter\n",
    "\n",
    "\n",
    "#Step 4: Provide list of tags to search from:\n",
    "\n",
    "#Year\tMissionTags\tPurposeTags\n",
    "#2015, 2014, 2013\tActivityOrMissionDesc\tOtherExemptPurposeExpendGrp, TotalExemptPurposeExpendGrp\n",
    "#2012, 2011\tActivityOrMissionDescription\tOtherExemptPurposeExpenditures, TotalExemptPurposeExpenditures\n",
    "#FormType\tMission\tPurpose\n",
    "#990\tYes\tNo\n",
    "#990EZ\tNo\tYes\n",
    "#990PF\tNo\tNo\n",
    "\n",
    "\n",
    "#The original tag names are converted into small letters while parsing, for e.g. 'ActivityOrMissionDesc' is parsed as 'activityormissiondesc'.\n",
    "#So, we will provide original tags converted into small letters for comparision.\n",
    "#https://github.com/lecy/Open-Data-for-Nonprofit-Research/blob/master/Build_IRS990_E-Filer_Datasets/Data_Dictionary.md\n",
    "#year| tag| line#\n",
    "\n",
    "#alltags = ['ActivityOrMissionDesc', 'ActivityOrMissionDescription',\n",
    "#           'OtherExemptPurposeExpenGrp', 'OtherExemptPurposeExpenditures',\n",
    "#           'TotalExemptPurposeExpendGrp', 'TotalExemptPurposeExpenditures'] \n",
    "    \n",
    "    \n",
    "alltags = ['activityormissiondesc', 'activityormissiondescription',\n",
    "           'otherexemptpurposeexpengrp', 'otherexemptpurposeexpenditures',\n",
    "           'totalexemptpurposeexpendgrp', 'totalexemptpurposeexpenditures']\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"../Data/intermediary/link\"+str(year)+\".pkl.gz\", 'gzip')\n",
    "df_inter = df[~ pd.isnull(df['IRS_URL'])]\n",
    "\n",
    "masterdata=pd.DataFrame(columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "\n",
    "#Uncomment the line below while developind dataset from scratch: Only puts index values and creates such file\n",
    "#masterdata.to_csv(open('../Data/'+str(year)+'/MasterData'+str(year)+'.csv.gzip', 'a'), index=False)\n",
    "\n",
    "print(\"length: \",len(df_inter))\n",
    "\n",
    "def build_coredata(r):\n",
    "    print(\"Inside the function:\", r)\n",
    "    \n",
    "    masterdata=pd.DataFrame(columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "    \n",
    "    #for turn in tqdm(range(r,r+101)):\n",
    "    for turn in range(r[0],r[1]):\n",
    "        row = df_inter.values[int(turn)]\n",
    "        flag = 0\n",
    "        page = requests.get(row[2])\n",
    "\n",
    "        bss = bs(page.text, 'html.parser')\n",
    "\n",
    "        for tag in bss.find_all():\n",
    "            if tag.name in alltags:\n",
    "                masterdata.loc[len(masterdata)] = [str(row[0]), row[1], row[2], tag.string, tag.name, str(year)]\n",
    "                flag = 1\n",
    "\n",
    "        if(flag == 0):\n",
    "            masterdata.loc[len(masterdata)] = [str(row[0]), row[1], row[2],'','', str(year)]            \n",
    "\n",
    "    print(masterdata)    \n",
    "    masterdata.to_csv(open('../Data/'+str(year)+'/MasterData'+str(year)+'.csv.gzip', 'a'), header=False, index=False)\n",
    "    print(\"written: \", r)\n",
    "\n",
    "\n",
    "no_urls = 100\n",
    "#records = [[i, i+no_urls] for i in range(18000, 19000, no_urls)]\n",
    "records = [[i, i+no_urls] for i in range(0, len(df_inter), no_urls)]\n",
    "\n",
    "agents = 4\n",
    "#Uncomment these two lines to build data\n",
    "#with Pool(processes=agents) as pool:\n",
    "#    pool.map(build_coredata, records)\n",
    "   \n",
    "\n",
    "no_url = df[pd.isnull(df['IRS_URL'])]\n",
    "masterdata=pd.DataFrame(no_url, columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "#masterdata.to_csv(open('../Data/'+str(year)+'/MasterData'+str(year)+'.csv.gzip', 'a'), index=False)\n",
    "print(\"Added data without URLs!\")\n",
    "\n",
    "if(os.path.isfile('../Data/'+str(year)+'/MasterData'+str(year)+'.pkl.gz')==False):\n",
    "    finaldata = pd.read_csv('../Data/'+str(year)+'/MasterData'+str(year)+'.csv.gzip').drop_duplicates()\n",
    "    finaldata = finaldata[df_dist.NTEE != 'NTEE']\n",
    "    finaldata.to_pickle('../Data/'+str(year)+'/MasterData'+str(year)+'.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
