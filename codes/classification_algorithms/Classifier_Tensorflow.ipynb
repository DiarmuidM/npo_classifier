{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "import keras.backend as K\n",
    "\n",
    "#precision & recall by: https://github.com/keras-team/keras/issues/5400\n",
    "#Code Source: https://cloud.google.com/blog/products/gcp/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts\n",
    "\n",
    "trainDF = pd.concat([pd.read_pickle('../../data/raw_data/MasterData_2015.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2014.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2013.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2012.pkl.gz')])\n",
    "                                   \n",
    "trainDF = trainDF[trainDF.TEXT.notna() & trainDF.NTEECC.notna()]\n",
    "trainDF['text'] = trainDF['TEXT'].astype(str)\n",
    "trainDF['label'] = trainDF['NTEE'].astype(str)\n",
    "\n",
    "trainDF = trainDF.drop(['NTEE', 'NTEECC', 'IRS_URL', 'TEXT'], axis=1)\n",
    "trainDF = trainDF[~ (trainDF.label == 'nan')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total len:  464303\n",
      "['E' 'T' 'C' 'X' 'N' 'A' 'L' 'P' 'K' 'O' 'B' 'D' 'S' 'W' 'F' 'M' 'G' 'Q'\n",
      " 'I' 'H' 'U' 'Y' 'J' 'Z' 'R' 'V']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total len: \", len(trainDF))\n",
    "print(trainDF['label'].unique())\n",
    "\n",
    "#test_df = pd.DataFrame(trainDF['label'].unique())\n",
    "#test_df.to_csv(\"../../data/classification_results/results_nteecc/list_of_classes.csv\")\n",
    "\n",
    "#total classes: 1327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 292510 samples, validate on 32502 samples\n",
      "Epoch 1/20\n",
      "292510/292510 [==============================] - 21s 72us/step - loss: 1.5299 - acc: 0.5936 - precision: 0.7637 - recall: 0.4089 - val_loss: 1.3203 - val_acc: 0.6368 - val_precision: 0.7943 - val_recall: 0.4808\n",
      "Epoch 2/20\n",
      "292510/292510 [==============================] - 21s 71us/step - loss: 1.2763 - acc: 0.6444 - precision: 0.7937 - recall: 0.5056 - val_loss: 1.2254 - val_acc: 0.6570 - val_precision: 0.8012 - val_recall: 0.5200\n",
      "Epoch 3/20\n",
      "292510/292510 [==============================] - 22s 75us/step - loss: 1.1767 - acc: 0.6689 - precision: 0.8093 - recall: 0.5410 - val_loss: 1.1503 - val_acc: 0.6781 - val_precision: 0.8145 - val_recall: 0.5465\n",
      "Epoch 4/20\n",
      "292510/292510 [==============================] - 20s 69us/step - loss: 1.0847 - acc: 0.6935 - precision: 0.8275 - recall: 0.5731 - val_loss: 1.0779 - val_acc: 0.6976 - val_precision: 0.8276 - val_recall: 0.5767\n",
      "Epoch 5/20\n",
      "292510/292510 [==============================] - 20s 69us/step - loss: 0.9917 - acc: 0.7205 - precision: 0.8462 - recall: 0.6074 - val_loss: 1.0050 - val_acc: 0.7190 - val_precision: 0.8400 - val_recall: 0.6097\n",
      "Epoch 6/20\n",
      "292510/292510 [==============================] - 20s 70us/step - loss: 0.9000 - acc: 0.7474 - precision: 0.8645 - recall: 0.6428 - val_loss: 0.9392 - val_acc: 0.7400 - val_precision: 0.8570 - val_recall: 0.6358\n",
      "Epoch 7/20\n",
      "292510/292510 [==============================] - 21s 71us/step - loss: 0.8131 - acc: 0.7743 - precision: 0.8810 - recall: 0.6769 - val_loss: 0.8795 - val_acc: 0.7591 - val_precision: 0.8672 - val_recall: 0.6683\n",
      "Epoch 8/20\n",
      "292510/292510 [==============================] - 20s 69us/step - loss: 0.7351 - acc: 0.7975 - precision: 0.8955 - recall: 0.7097 - val_loss: 0.8289 - val_acc: 0.7748 - val_precision: 0.8753 - val_recall: 0.6905\n",
      "Epoch 9/20\n",
      "292510/292510 [==============================] - 20s 69us/step - loss: 0.6680 - acc: 0.8175 - precision: 0.9065 - recall: 0.7375 - val_loss: 0.7834 - val_acc: 0.7931 - val_precision: 0.8825 - val_recall: 0.7122\n",
      "Epoch 10/20\n",
      "292510/292510 [==============================] - 21s 71us/step - loss: 0.6075 - acc: 0.8355 - precision: 0.9175 - recall: 0.7632 - val_loss: 0.7456 - val_acc: 0.8042 - val_precision: 0.8877 - val_recall: 0.7340\n",
      "Epoch 11/20\n",
      "292510/292510 [==============================] - 20s 69us/step - loss: 0.5562 - acc: 0.8508 - precision: 0.9254 - recall: 0.7855 - val_loss: 0.7138 - val_acc: 0.8159 - val_precision: 0.8952 - val_recall: 0.7512\n",
      "Epoch 12/20\n",
      "292510/292510 [==============================] - 20s 69us/step - loss: 0.5109 - acc: 0.8636 - precision: 0.9317 - recall: 0.8051 - val_loss: 0.6926 - val_acc: 0.8253 - val_precision: 0.8967 - val_recall: 0.7658\n",
      "Epoch 13/20\n",
      "292510/292510 [==============================] - 20s 70us/step - loss: 0.4715 - acc: 0.8747 - precision: 0.9376 - recall: 0.8208 - val_loss: 0.6722 - val_acc: 0.8314 - val_precision: 0.8944 - val_recall: 0.7839\n",
      "Epoch 14/20\n",
      "292510/292510 [==============================] - 20s 70us/step - loss: 0.4370 - acc: 0.8847 - precision: 0.9419 - recall: 0.8365 - val_loss: 0.6595 - val_acc: 0.8392 - val_precision: 0.8968 - val_recall: 0.7940\n",
      "Epoch 15/20\n",
      "292510/292510 [==============================] - 20s 70us/step - loss: 0.4076 - acc: 0.8926 - precision: 0.9461 - recall: 0.8484 - val_loss: 0.6392 - val_acc: 0.8482 - val_precision: 0.8957 - val_recall: 0.8065\n",
      "Epoch 16/20\n",
      "292510/292510 [==============================] - 21s 70us/step - loss: 0.3816 - acc: 0.9000 - precision: 0.9486 - recall: 0.8601 - val_loss: 0.6298 - val_acc: 0.8524 - val_precision: 0.9077 - val_recall: 0.8101\n",
      "Epoch 17/20\n",
      "292510/292510 [==============================] - 20s 70us/step - loss: 0.3584 - acc: 0.9065 - precision: 0.9518 - recall: 0.8699 - val_loss: 0.6195 - val_acc: 0.8589 - val_precision: 0.9052 - val_recall: 0.8218\n",
      "Epoch 18/20\n",
      "292510/292510 [==============================] - 21s 70us/step - loss: 0.3395 - acc: 0.9117 - precision: 0.9543 - recall: 0.8780 - val_loss: 0.6178 - val_acc: 0.8619 - val_precision: 0.9050 - val_recall: 0.8265- acc: - ETA: 1s - loss: 0.3385 - ac\n",
      "Epoch 19/20\n",
      "292510/292510 [==============================] - 21s 70us/step - loss: 0.3241 - acc: 0.9159 - precision: 0.9557 - recall: 0.8845 - val_loss: 0.6094 - val_acc: 0.8663 - val_precision: 0.9068 - val_recall: 0.8338\n",
      "Epoch 20/20\n",
      "292510/292510 [==============================] - 21s 71us/step - loss: 0.3064 - acc: 0.9209 - precision: 0.9582 - recall: 0.8921 - val_loss: 0.6038 - val_acc: 0.8675 - val_precision: 0.9124 - val_recall: 0.8358\n",
      "139291/139291 [==============================] - 4s 30us/step\n",
      "[1.951706824926715, 0.9995391340622909, 0.8172225005468325, 0.5081794694533867]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "counts = trainDF['label'].value_counts().sort_index().to_frame()\n",
    "counts['category'] = counts.index\n",
    "counts['train_sample']=(counts['label']/2).astype(int)\n",
    "    \n",
    "train_df, test_df = np.split(trainDF, [int(.7*len(trainDF))])\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "train_posts = train_df['text']\n",
    "train_tags = train_df['label']\n",
    "test_posts = test_df['text']\n",
    "test_tags = test_df['label']\n",
    "vocab_size = 1000\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = preprocessing.LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "    \n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "     Only computes a batch-wise average of recall.\n",
    "     Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "num_labels=len(train_df['label'].drop_duplicates())\n",
    "batch_size = 500\n",
    "epochs = 20\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(num_labels))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              #metrics=['accuracy'],\n",
    "             metrics=['acc', precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)\n",
    "    \n",
    "score = model.evaluate(x_test, y_test, \n",
    "                   batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe02aa05c2b2473cbb16fe52bab8ccc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=139291), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "    \n",
    "all_pred = pd.DataFrame(columns=['EIN', 'text', 'actual_label', 'predicted_label', 'true_pred'])\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_test))):\n",
    "    prediction = model.predict(np.array([x_test[i]]))    \n",
    "    text_labels = encoder.classes_ \n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    text = test_posts.iloc[i]\n",
    "    ein = trainDF[trainDF['text']==test_posts.iloc[i]]['EIN'].astype(int).drop_duplicates().tolist()\n",
    "    actual_label = trainDF[trainDF['text']==test_posts.iloc[i]]['label'].drop_duplicates().tolist()\n",
    "    if(predicted_label == test_tags.iloc[i]):\n",
    "        true_pred = 'true'\n",
    "    else:\n",
    "        true_pred = 'false'\n",
    "\n",
    "    all_pred.loc[len(all_pred)] = [ein, text, actual_label, predicted_label, true_pred]\n",
    "\n",
    "\n",
    "if(os.path.exists('../../data/classification_results/all_predictions_V2.pkl.gz')):\n",
    "    all_pred = pd.concat([pd.read_pickle('../../data/classification_results/all_predictions_V2.pkl.gz'), all_pred]).drop_duplicates()\n",
    "    \n",
    "all_pred.to_pickle('../../data/classification_results/all_predictions_V2.pkl.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
