{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_df, test_df = np.split(trainDF, [int(.7*len(trainDF))])\\n\\n#tf.logging.set_verbosity(tf.logging.ERROR)\\n\\ntrain_posts = train_df['text']\\ntrain_tags = train_df['label']\\ntest_posts = test_df['text']\\ntest_tags = test_df['label']\\nvocab_size = 1000\\ntokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size)\\ntokenize.fit_on_texts(train_posts)\\n\\nx_train = tokenize.texts_to_matrix(train_posts)\\nx_test = tokenize.texts_to_matrix(test_posts)\\n\\nencoder = preprocessing.LabelBinarizer()\\nencoder.fit(train_tags)\\ny_train = encoder.transform(train_tags)\\ny_test = encoder.transform(test_tags)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "import keras.backend as K\n",
    "\n",
    "#precision & recall by: https://github.com/keras-team/keras/issues/5400\n",
    "#Code Source: https://cloud.google.com/blog/products/gcp/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts\n",
    "\n",
    "trainDF = pd.concat([pd.read_pickle('../../data/raw_data/MasterData_2015.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2014.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2013.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2012.pkl.gz')])\n",
    "                                   \n",
    "trainDF = trainDF[trainDF.TEXT.notna() & trainDF.NTEE.notna()]\n",
    "trainDF['text'] = trainDF['TEXT'].astype(str)\n",
    "trainDF['label'] = trainDF['NTEE'].astype(str)\n",
    "\n",
    "trainDF = trainDF.drop(['NTEE', 'IRS_URL', 'TEXT'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328851 samples, validate on 36539 samples\n",
      "Epoch 1/40\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 1.5055 - binary_accuracy: 0.9731 - precision: 0.7669 - recall: 0.4167 - val_loss: 1.2856 - val_binary_accuracy: 0.9758 - val_precision: 0.7970 - val_recall: 0.4961\n",
      "Epoch 2/40\n",
      "328851/328851 [==============================] - 23s 69us/step - loss: 1.2499 - binary_accuracy: 0.9763 - precision: 0.7968 - recall: 0.5140 - val_loss: 1.1864 - val_binary_accuracy: 0.9773 - val_precision: 0.8053 - val_recall: 0.5400\n",
      "Epoch 3/40\n",
      "328851/328851 [==============================] - 25s 77us/step - loss: 1.1350 - binary_accuracy: 0.9781 - precision: 0.8163 - recall: 0.5540 - val_loss: 1.1053 - val_binary_accuracy: 0.9787 - val_precision: 0.8194 - val_recall: 0.5733\n",
      "Epoch 4/40\n",
      "328851/328851 [==============================] - 24s 73us/step - loss: 1.0223 - binary_accuracy: 0.9800 - precision: 0.8386 - recall: 0.5949 - val_loss: 1.0272 - val_binary_accuracy: 0.9803 - val_precision: 0.8384 - val_recall: 0.6030\n",
      "Epoch 5/40\n",
      "328851/328851 [==============================] - 24s 72us/step - loss: 0.9101 - binary_accuracy: 0.9822 - precision: 0.8618 - recall: 0.6387 - val_loss: 0.9549 - val_binary_accuracy: 0.9818 - val_precision: 0.8499 - val_recall: 0.6396\n",
      "Epoch 6/40\n",
      "328851/328851 [==============================] - 22s 67us/step - loss: 0.8051 - binary_accuracy: 0.9843 - precision: 0.8827 - recall: 0.6818 - val_loss: 0.8949 - val_binary_accuracy: 0.9832 - val_precision: 0.8624 - val_recall: 0.6697\n",
      "Epoch 7/40\n",
      "328851/328851 [==============================] - 22s 67us/step - loss: 0.7126 - binary_accuracy: 0.9862 - precision: 0.8999 - recall: 0.7207 - val_loss: 0.8465 - val_binary_accuracy: 0.9845 - val_precision: 0.8718 - val_recall: 0.7005\n",
      "Epoch 8/40\n",
      "328851/328851 [==============================] - 22s 67us/step - loss: 0.6333 - binary_accuracy: 0.9878 - precision: 0.9134 - recall: 0.7543 - val_loss: 0.8119 - val_binary_accuracy: 0.9853 - val_precision: 0.8750 - val_recall: 0.7213\n",
      "Epoch 9/40\n",
      "328851/328851 [==============================] - 22s 68us/step - loss: 0.5668 - binary_accuracy: 0.9892 - precision: 0.9243 - recall: 0.7823 - val_loss: 0.7836 - val_binary_accuracy: 0.9861 - val_precision: 0.8781 - val_recall: 0.7407\n",
      "Epoch 10/40\n",
      "328851/328851 [==============================] - 22s 68us/step - loss: 0.5115 - binary_accuracy: 0.9903 - precision: 0.9322 - recall: 0.8064 - val_loss: 0.7588 - val_binary_accuracy: 0.9868 - val_precision: 0.8833 - val_recall: 0.7572\n",
      "Epoch 11/40\n",
      "328851/328851 [==============================] - 22s 68us/step - loss: 0.4630 - binary_accuracy: 0.9913 - precision: 0.9403 - recall: 0.8261 - val_loss: 0.7442 - val_binary_accuracy: 0.9874 - val_precision: 0.8856 - val_recall: 0.7717\n",
      "Epoch 12/40\n",
      "328851/328851 [==============================] - 22s 68us/step - loss: 0.4233 - binary_accuracy: 0.9921 - precision: 0.9452 - recall: 0.8434 - val_loss: 0.7329 - val_binary_accuracy: 0.9879 - val_precision: 0.8886 - val_recall: 0.7825\n",
      "Epoch 13/40\n",
      "328851/328851 [==============================] - 22s 68us/step - loss: 0.3900 - binary_accuracy: 0.9928 - precision: 0.9492 - recall: 0.8574 - val_loss: 0.7297 - val_binary_accuracy: 0.9881 - val_precision: 0.8882 - val_recall: 0.7909\n",
      "Epoch 14/40\n",
      "328851/328851 [==============================] - 23s 69us/step - loss: 0.3613 - binary_accuracy: 0.9933 - precision: 0.9529 - recall: 0.8695 - val_loss: 0.7215 - val_binary_accuracy: 0.9884 - val_precision: 0.8856 - val_recall: 0.8026\n",
      "Epoch 15/40\n",
      "328851/328851 [==============================] - 23s 69us/step - loss: 0.3364 - binary_accuracy: 0.9938 - precision: 0.9561 - recall: 0.8796 - val_loss: 0.7237 - val_binary_accuracy: 0.9888 - val_precision: 0.8912 - val_recall: 0.8066\n",
      "Epoch 16/40\n",
      "328851/328851 [==============================] - 23s 69us/step - loss: 0.3152 - binary_accuracy: 0.9942 - precision: 0.9584 - recall: 0.8886 - val_loss: 0.7263 - val_binary_accuracy: 0.9888 - val_precision: 0.8857 - val_recall: 0.8147\n",
      "Epoch 17/40\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 0.2970 - binary_accuracy: 0.9946 - precision: 0.9609 - recall: 0.8956 - val_loss: 0.7265 - val_binary_accuracy: 0.9891 - val_precision: 0.8875 - val_recall: 0.8201\n",
      "Epoch 18/40\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 0.2809 - binary_accuracy: 0.9949 - precision: 0.9625 - recall: 0.9020 - val_loss: 0.7304 - val_binary_accuracy: 0.9894 - val_precision: 0.8940 - val_recall: 0.8227\n",
      "Epoch 19/40\n",
      "328851/328851 [==============================] - 24s 72us/step - loss: 0.2669 - binary_accuracy: 0.9952 - precision: 0.9648 - recall: 0.9077 - val_loss: 0.7367 - val_binary_accuracy: 0.9893 - val_precision: 0.8852 - val_recall: 0.8292\n",
      "Epoch 20/40\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 0.2558 - binary_accuracy: 0.9954 - precision: 0.9655 - recall: 0.9121 - val_loss: 0.7388 - val_binary_accuracy: 0.9897 - val_precision: 0.8954 - val_recall: 0.8284\n",
      "Epoch 21/40\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 0.2457 - binary_accuracy: 0.9956 - precision: 0.9667 - recall: 0.9165 - val_loss: 0.7431 - val_binary_accuracy: 0.9897 - val_precision: 0.8897 - val_recall: 0.8360\n",
      "Epoch 22/40\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 0.2372 - binary_accuracy: 0.9957 - precision: 0.9676 - recall: 0.9197 - val_loss: 0.7525 - val_binary_accuracy: 0.9899 - val_precision: 0.8908 - val_recall: 0.8387\n",
      "Epoch 23/40\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.2281 - binary_accuracy: 0.9959 - precision: 0.9687 - recall: 0.9233 - val_loss: 0.7565 - val_binary_accuracy: 0.9900 - val_precision: 0.8948 - val_recall: 0.8381\n",
      "Epoch 24/40\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.2216 - binary_accuracy: 0.9960 - precision: 0.9695 - recall: 0.9257 - val_loss: 0.7683 - val_binary_accuracy: 0.9898 - val_precision: 0.8889 - val_recall: 0.8393\n",
      "Epoch 25/40\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.2152 - binary_accuracy: 0.9961 - precision: 0.9697 - recall: 0.9283 - val_loss: 0.7703 - val_binary_accuracy: 0.9900 - val_precision: 0.8925 - val_recall: 0.8411\n",
      "Epoch 26/40\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 0.2100 - binary_accuracy: 0.9962 - precision: 0.9702 - recall: 0.9301 - val_loss: 0.7761 - val_binary_accuracy: 0.9900 - val_precision: 0.8890 - val_recall: 0.8439\n",
      "Epoch 27/40\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.2057 - binary_accuracy: 0.9963 - precision: 0.9704 - recall: 0.9323 - val_loss: 0.7843 - val_binary_accuracy: 0.9901 - val_precision: 0.8952 - val_recall: 0.8422\n",
      "Epoch 28/40\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.2000 - binary_accuracy: 0.9964 - precision: 0.9714 - recall: 0.9338 - val_loss: 0.7846 - val_binary_accuracy: 0.9904 - val_precision: 0.8968 - val_recall: 0.8469\n",
      "Epoch 29/40\n",
      "328851/328851 [==============================] - 24s 72us/step - loss: 0.1976 - binary_accuracy: 0.9965 - precision: 0.9716 - recall: 0.9357 - val_loss: 0.7935 - val_binary_accuracy: 0.9901 - val_precision: 0.8907 - val_recall: 0.8462\n",
      "Epoch 30/40\n",
      "328851/328851 [==============================] - 24s 72us/step - loss: 0.1930 - binary_accuracy: 0.9965 - precision: 0.9725 - recall: 0.9366 - val_loss: 0.7982 - val_binary_accuracy: 0.9901 - val_precision: 0.8893 - val_recall: 0.8493\n",
      "Epoch 31/40\n",
      "328851/328851 [==============================] - 24s 72us/step - loss: 0.1907 - binary_accuracy: 0.9966 - precision: 0.9721 - recall: 0.9378 - val_loss: 0.8031 - val_binary_accuracy: 0.9902 - val_precision: 0.8901 - val_recall: 0.8493\n",
      "Epoch 32/40\n",
      "328851/328851 [==============================] - 24s 74us/step - loss: 0.1873 - binary_accuracy: 0.9966 - precision: 0.9727 - recall: 0.9391 - val_loss: 0.8147 - val_binary_accuracy: 0.9904 - val_precision: 0.8950 - val_recall: 0.8483\n",
      "Epoch 33/40\n",
      "328851/328851 [==============================] - 24s 73us/step - loss: 0.1853 - binary_accuracy: 0.9967 - precision: 0.9728 - recall: 0.9397 - val_loss: 0.8154 - val_binary_accuracy: 0.9903 - val_precision: 0.8936 - val_recall: 0.8488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40\n",
      "328851/328851 [==============================] - 21s 65us/step - loss: 0.1827 - binary_accuracy: 0.9967 - precision: 0.9732 - recall: 0.9404 - val_loss: 0.8195 - val_binary_accuracy: 0.9904 - val_precision: 0.8947 - val_recall: 0.8504\n",
      "Epoch 35/40\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 0.1797 - binary_accuracy: 0.9968 - precision: 0.9734 - recall: 0.9417 - val_loss: 0.8192 - val_binary_accuracy: 0.9903 - val_precision: 0.8897 - val_recall: 0.8534\n",
      "Epoch 36/40\n",
      "328851/328851 [==============================] - 23s 69us/step - loss: 0.1786 - binary_accuracy: 0.9968 - precision: 0.9734 - recall: 0.9423 - val_loss: 0.8299 - val_binary_accuracy: 0.9905 - val_precision: 0.8942 - val_recall: 0.8527\n",
      "Epoch 37/40\n",
      "328851/328851 [==============================] - 20s 62us/step - loss: 0.1766 - binary_accuracy: 0.9968 - precision: 0.9737 - recall: 0.9428 - val_loss: 0.8368 - val_binary_accuracy: 0.9905 - val_precision: 0.8954 - val_recall: 0.8514\n",
      "Epoch 38/40\n",
      "328851/328851 [==============================] - 22s 66us/step - loss: 0.1745 - binary_accuracy: 0.9969 - precision: 0.9739 - recall: 0.9438 - val_loss: 0.8393 - val_binary_accuracy: 0.9903 - val_precision: 0.8891 - val_recall: 0.8556\n",
      "Epoch 39/40\n",
      "328851/328851 [==============================] - 22s 68us/step - loss: 0.1737 - binary_accuracy: 0.9969 - precision: 0.9734 - recall: 0.9443 - val_loss: 0.8413 - val_binary_accuracy: 0.9903 - val_precision: 0.8894 - val_recall: 0.8551\n",
      "Epoch 40/40\n",
      "328851/328851 [==============================] - 22s 68us/step - loss: 0.1723 - binary_accuracy: 0.9969 - precision: 0.9737 - recall: 0.9446 - val_loss: 0.8464 - val_binary_accuracy: 0.9904 - val_precision: 0.8893 - val_recall: 0.8561\n",
      "156597/156597 [==============================] - 6s 37us/step\n",
      "[0.8761785990406392, 0.9899172798229502, 0.8838560463373563, 0.8492244433651621]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "counts = trainDF['label'].value_counts().sort_index().to_frame()\n",
    "counts['category'] = counts.index\n",
    "counts['train_sample']=(counts['label']/2).astype(int)\n",
    "\n",
    "def dataformodel(trainDF):\n",
    "    \n",
    "    train_df, test_df = np.split(trainDF, [int(.7*len(trainDF))])\n",
    "\n",
    "    #tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    train_posts = train_df['text']\n",
    "    train_tags = train_df['label']\n",
    "    test_posts = test_df['text']\n",
    "    test_tags = test_df['label']\n",
    "    vocab_size = 1000\n",
    "    tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "    tokenize.fit_on_texts(train_posts)\n",
    "\n",
    "    x_train = tokenize.texts_to_matrix(train_posts)\n",
    "    x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "    encoder = preprocessing.LabelBinarizer()\n",
    "    encoder.fit(train_tags)\n",
    "    y_train = encoder.transform(train_tags)\n",
    "    y_test = encoder.transform(test_tags)\n",
    "    \n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    num_labels=26\n",
    "    batch_size = 500\n",
    "    epochs = 40\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(512, input_shape=(vocab_size,)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dense(num_labels))\n",
    "    model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  #metrics=['accuracy'],\n",
    "                 metrics=['binary_accuracy', precision, recall])\n",
    "\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs, \n",
    "                        verbose=1, \n",
    "                        validation_split=0.1)\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, \n",
    "                       batch_size=batch_size, verbose=1)\n",
    "    return score\n",
    "\n",
    "\n",
    "result = dataformodel(trainDF)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253c2f95db734c58be342c8ba7ccff83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=105650), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7dc01cb5bef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/classification_results/all_predictions.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mall_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/classification_results/all_predictions.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mall_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/classification_results/all_predictions.pkl.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[0;34m(self, subset, keep, inplace)\u001b[0m\n\u001b[1;32m   4329\u001b[0m         \"\"\"\n\u001b[1;32m   4330\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inplace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4331\u001b[0;31m         \u001b[0mduplicated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mduplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   4383\u001b[0m         vals = (col.values for name, col in self.iteritems()\n\u001b[1;32m   4384\u001b[0m                 if name in subset)\n\u001b[0;32m-> 4385\u001b[0;31m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_group_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxnull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(vals)\u001b[0m\n\u001b[1;32m   4364\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4365\u001b[0m             labels, shape = algorithms.factorize(\n\u001b[0;32m-> 4366\u001b[0;31m                 vals, size_hint=min(len(self), _SIZE_HINT_LIMIT))\n\u001b[0m\u001b[1;32m   4367\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, order, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    628\u001b[0m                                            \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                                            \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                                            na_value=na_value)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msort\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_factorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     labels = table.get_labels(values, uniques, 0, na_sentinel,\n\u001b[0;32m--> 476\u001b[0;31m                               na_value=na_value)\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "all_pred = pd.DataFrame(columns=['EIN', 'text', 'actual_label', 'predicted_label', 'true_pred'])\n",
    "\n",
    "\n",
    "for i in tqdm(range(50947, len(x_test))):\n",
    "    prediction = model.predict(np.array([x_test[i]]))    \n",
    "    text_labels = encoder.classes_ \n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    text = test_posts.iloc[i]\n",
    "    ein = trainDF[trainDF['text']==test_posts.iloc[i]]['EIN'].astype(int).drop_duplicates().tolist()\n",
    "    actual_label = trainDF[trainDF['text']==test_posts.iloc[i]]['label'].drop_duplicates().tolist()\n",
    "    if(predicted_label == test_tags.iloc[i]):\n",
    "        true_pred = 'true'\n",
    "    else:\n",
    "        true_pred = 'false'\n",
    "\n",
    "    all_pred.loc[len(all_pred)] = [ein, text, actual_label, predicted_label, true_pred]\n",
    "\n",
    "\n",
    "if(os.path.exists('../../data/classification_results/all_predictions.pkl.gz')):\n",
    "    all_pred = pd.concat([pd.read_pickle('../../data/classification_results/all_predictions.pkl.gz'), all_pred]).drop_duplicates()\n",
    "    \n",
    "#all_pred.to_pickle('../../data/classification_results/all_predictions.pkl.gz', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
