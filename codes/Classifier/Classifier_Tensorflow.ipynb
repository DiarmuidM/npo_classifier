{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "import keras.backend as K\n",
    "\n",
    "#to-do list\n",
    "#1. Record the amount of time a classifier takes: start(timestamp) - end(timestamp), put it in .pkl.gz\n",
    "\n",
    "#Observations:\n",
    "# 1. RF gives highest accuracy, but takes a lot of time to train: 25 minutes and 15 minutes\n",
    "# 2. Neural Network is weakest\n",
    "# 3. NB gives satisfactory results within a minute.\n",
    "\n",
    "trainDF = pd.concat([pd.read_pickle('../../data/2015/MasterData_2015.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2014/MasterData_2014.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2013/MasterData_2013.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2012/MasterData_2012.pkl.gz')])\n",
    "                                   \n",
    "trainDF = trainDF[trainDF.TEXT.notna() & trainDF.NTEE.notna()]\n",
    "trainDF['text'] = trainDF['TEXT'].astype(str)\n",
    "trainDF['label'] = trainDF['NTEE'].astype(str)\n",
    "trainDF['category'] = (trainDF['NTEE'].apply(ord)-64).astype('float32')\n",
    "\n",
    "trainDF = trainDF.drop(['EIN', 'NTEE', 'IRS_URL', 'TEXT','TEXTTYPE', 'YEAR', 'category'], axis=1)\n",
    "train_df, test_df = np.split(trainDF, [int(.7*len(trainDF))])\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "train_posts = train_df['text']\n",
    "train_tags = train_df['label']\n",
    "test_posts = test_df['text']\n",
    "test_tags = test_df['label']\n",
    "vocab_size = 1000\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = preprocessing.LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328851 samples, validate on 36539 samples\n",
      "Epoch 1/20\n",
      "328851/328851 [==============================] - 23s 69us/step - loss: 1.5042 - binary_accuracy: 0.9731 - precision: 0.7688 - recall: 0.4167 - val_loss: 1.2900 - val_binary_accuracy: 0.9756 - val_precision: 0.7987 - val_recall: 0.4870\n",
      "Epoch 2/20\n",
      "328851/328851 [==============================] - 22s 68us/step - loss: 1.2499 - binary_accuracy: 0.9763 - precision: 0.7975 - recall: 0.5137 - val_loss: 1.1893 - val_binary_accuracy: 0.9773 - val_precision: 0.8086 - val_recall: 0.5361\n",
      "Epoch 3/20\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 1.1339 - binary_accuracy: 0.9781 - precision: 0.8171 - recall: 0.5553 - val_loss: 1.1080 - val_binary_accuracy: 0.9787 - val_precision: 0.8245 - val_recall: 0.5674\n",
      "Epoch 4/20\n",
      "328851/328851 [==============================] - 24s 73us/step - loss: 1.0215 - binary_accuracy: 0.9801 - precision: 0.8406 - recall: 0.5958 - val_loss: 1.0269 - val_binary_accuracy: 0.9803 - val_precision: 0.8381 - val_recall: 0.6034\n",
      "Epoch 5/20\n",
      "328851/328851 [==============================] - 25s 76us/step - loss: 0.9086 - binary_accuracy: 0.9823 - precision: 0.8634 - recall: 0.6402 - val_loss: 0.9533 - val_binary_accuracy: 0.9818 - val_precision: 0.8512 - val_recall: 0.6392\n",
      "Epoch 6/20\n",
      "328851/328851 [==============================] - 23s 70us/step - loss: 0.8039 - binary_accuracy: 0.9844 - precision: 0.8838 - recall: 0.6829 - val_loss: 0.8980 - val_binary_accuracy: 0.9831 - val_precision: 0.8615 - val_recall: 0.6670\n",
      "Epoch 7/20\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.7116 - binary_accuracy: 0.9862 - precision: 0.9009 - recall: 0.7216 - val_loss: 0.8433 - val_binary_accuracy: 0.9844 - val_precision: 0.8699 - val_recall: 0.6980\n",
      "Epoch 8/20\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.6319 - binary_accuracy: 0.9879 - precision: 0.9149 - recall: 0.7556 - val_loss: 0.8071 - val_binary_accuracy: 0.9853 - val_precision: 0.8744 - val_recall: 0.7220\n",
      "Epoch 9/20\n",
      "328851/328851 [==============================] - 24s 73us/step - loss: 0.5665 - binary_accuracy: 0.9892 - precision: 0.9241 - recall: 0.7832 - val_loss: 0.7763 - val_binary_accuracy: 0.9862 - val_precision: 0.8805 - val_recall: 0.7418\n",
      "Epoch 10/20\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.5104 - binary_accuracy: 0.9903 - precision: 0.9331 - recall: 0.8068 - val_loss: 0.7537 - val_binary_accuracy: 0.9868 - val_precision: 0.8832 - val_recall: 0.7575\n",
      "Epoch 11/20\n",
      "328851/328851 [==============================] - 23s 71us/step - loss: 0.4632 - binary_accuracy: 0.9913 - precision: 0.9400 - recall: 0.8260 - val_loss: 0.7407 - val_binary_accuracy: 0.9874 - val_precision: 0.8861 - val_recall: 0.7711\n",
      "Epoch 12/20\n",
      "328851/328851 [==============================] - 24s 73us/step - loss: 0.4228 - binary_accuracy: 0.9921 - precision: 0.9454 - recall: 0.8433 - val_loss: 0.7305 - val_binary_accuracy: 0.9878 - val_precision: 0.8876 - val_recall: 0.7813\n",
      "Epoch 13/20\n",
      "328851/328851 [==============================] - 24s 73us/step - loss: 0.3893 - binary_accuracy: 0.9928 - precision: 0.9499 - recall: 0.8569 - val_loss: 0.7224 - val_binary_accuracy: 0.9883 - val_precision: 0.8910 - val_recall: 0.7935\n",
      "Epoch 14/20\n",
      "328851/328851 [==============================] - 24s 74us/step - loss: 0.3599 - binary_accuracy: 0.9933 - precision: 0.9536 - recall: 0.8694 - val_loss: 0.7187 - val_binary_accuracy: 0.9886 - val_precision: 0.8913 - val_recall: 0.8009\n",
      "Epoch 15/20\n",
      "328851/328851 [==============================] - 24s 74us/step - loss: 0.3362 - binary_accuracy: 0.9939 - precision: 0.9564 - recall: 0.8803 - val_loss: 0.7152 - val_binary_accuracy: 0.9890 - val_precision: 0.8933 - val_recall: 0.8091\n",
      "Epoch 16/20\n",
      "328851/328851 [==============================] - 24s 72us/step - loss: 0.3144 - binary_accuracy: 0.9943 - precision: 0.9591 - recall: 0.8892 - val_loss: 0.7154 - val_binary_accuracy: 0.9891 - val_precision: 0.8934 - val_recall: 0.8136\n",
      "Epoch 17/20\n",
      "328851/328851 [==============================] - 24s 73us/step - loss: 0.2966 - binary_accuracy: 0.9946 - precision: 0.9609 - recall: 0.8958 - val_loss: 0.7185 - val_binary_accuracy: 0.9891 - val_precision: 0.8878 - val_recall: 0.8210\n",
      "Epoch 18/20\n",
      "328851/328851 [==============================] - 24s 72us/step - loss: 0.2804 - binary_accuracy: 0.9949 - precision: 0.9626 - recall: 0.9024 - val_loss: 0.7216 - val_binary_accuracy: 0.9893 - val_precision: 0.8881 - val_recall: 0.8245\n",
      "Epoch 19/20\n",
      "328851/328851 [==============================] - 24s 72us/step - loss: 0.2672 - binary_accuracy: 0.9952 - precision: 0.9641 - recall: 0.9078 - val_loss: 0.7273 - val_binary_accuracy: 0.9894 - val_precision: 0.8874 - val_recall: 0.8288\n",
      "Epoch 20/20\n",
      "328851/328851 [==============================] - 24s 74us/step - loss: 0.2546 - binary_accuracy: 0.9954 - precision: 0.9655 - recall: 0.9126 - val_loss: 0.7345 - val_binary_accuracy: 0.9897 - val_precision: 0.8937 - val_recall: 0.8306\n"
     ]
    }
   ],
   "source": [
    "#precision & recall by: https://github.com/keras-team/keras/issues/5400\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "num_labels=26\n",
    "batch_size = 500\n",
    "epochs = 20\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dense(num_labels))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              #metrics=['accuracy'],\n",
    "             metrics=['binary_accuracy', precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156597/156597 [==============================] - 5s 30us/step\n",
      "[0.7746244333082344, 0.9888899364459225, 0.8865628756221476, 0.8151944162737179]\n",
      "156597/156597 [==============================] - 4s 27us/step\n",
      "[0.7746244333082344, 0.9888899364459225, 0.8865628756221476, 0.8151944162737179]\n",
      "156597/156597 [==============================] - 4s 26us/step\n",
      "[0.7746244333082344, 0.9888899364459225, 0.8865628756221476, 0.8151944162737179]\n",
      "156597/156597 [==============================] - 4s 28us/step\n",
      "[0.7746244333082344, 0.9888899364459225, 0.8865628756221476, 0.8151944162737179]\n",
      "156597/156597 [==============================] - 5s 29us/step\n",
      "[0.7746244333082344, 0.9888899364459225, 0.8865628756221476, 0.8151944162737179]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-49a20038a68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/results/classifier_results.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/results/classifier_results.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime, os\n",
    "\n",
    "results = pd.DataFrame(columns=['classifier', 'accuracy', 'precision', 'recall', 'time'])\n",
    "\n",
    "for i in range(0,5):\n",
    "    score = model.evaluate(x_test, y_test, \n",
    "                       batch_size=batch_size, verbose=1)\n",
    "    results.loc[len(results)] = [\"NN\", score[1], score[2], score[3], 'nan']\n",
    "    print(score)\n",
    "\n",
    "if(os.path.exists('../../data/results/classifier_results.pkl.gz')):\n",
    "    results = pd.concat([pd.read_pickle('../../data/results/classifier_results.pkl.gz'), results]).drop_duplicates()\n",
    "\n",
    "results.to_pickle('../../data/results/classifier_results.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173995/173995 [==============================] - 5s 28us/step\n",
      "[0.43095523711268136, 0.993348422638044, 0.9414036889135017, 0.8819448832222201]\n",
      "173995/173995 [==============================] - 5s 28us/step\n",
      "[0.4250271705163718, 0.9933707478053337, 0.9415287255466647, 0.8824334050643758]\n",
      "173995/173995 [==============================] - 5s 29us/step\n",
      "[0.423503542605678, 0.993393739632051, 0.9419808366448787, 0.8826000754023191]\n",
      "173995/173995 [==============================] - 5s 29us/step\n",
      "[0.429151980115537, 0.993391968555884, 0.9421158624523012, 0.8824104152005576]\n",
      "173995/173995 [==============================] - 6s 32us/step\n",
      "[0.42732889867417445, 0.9934092116645706, 0.941672274936475, 0.8833472237165209]\n",
      "173995/173995 [==============================] - 5s 31us/step\n",
      "[0.42944978341935963, 0.9933475421956189, 0.9413402659111507, 0.8820023574998055]\n",
      "173995/173995 [==============================] - 6s 34us/step\n",
      "[0.42800540081995436, 0.9933605809270515, 0.941514153332062, 0.8821747774190438]\n",
      "173995/173995 [==============================] - 6s 32us/step\n",
      "[0.4246890280011324, 0.9933868870282572, 0.9414356140597921, 0.8829793952817173]\n",
      "173995/173995 [==============================] - 6s 32us/step\n",
      "[0.4286768204951965, 0.9933997099692597, 0.9423419012002943, 0.8823701837987146]\n",
      "173995/173995 [==============================] - 5s 29us/step\n",
      "[0.42747247811038486, 0.9933824662730736, 0.9416585997718617, 0.8826345623390794]\n",
      "[0.993348422638044, 0.9933707478053337, 0.993393739632051, 0.993391968555884, 0.9934092116645706, 0.9933475421956189, 0.9933605809270515, 0.9933868870282572, 0.9933997099692597, 0.9933824662730736]\n",
      "[0.9414036889135017, 0.9415287255466647, 0.9419808366448787, 0.9421158624523012, 0.941672274936475, 0.9413402659111507, 0.941514153332062, 0.9414356140597921, 0.9423419012002943, 0.9416585997718617]\n",
      "[0.8819448832222201, 0.8824334050643758, 0.8826000754023191, 0.8824104152005576, 0.8833472237165209, 0.8820023574998055, 0.8821747774190438, 0.8829793952817173, 0.8823701837987146, 0.8826345623390794]\n",
      "[[0.9933791276689145, 2.1429663899405124e-05], [0.9416991922768982, 0.0003362054278664529], [0.8824897278944355, 0.0004301885632659358]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nif(os.path.exists('../../data/results/classifier_stats_10.pkl.gz')):\\n    stats = pd.concat([pd.read_pickle('../../data/results/classifier_stats_10.pkl.gz'), stats]).drop_duplicates()\\n    \\nstats.to_pickle('../../data/results/classifier_stats_10.pkl.gz')\\n\\nresults = pd.DataFrame(results, columns=['acNB','acRF', 'acNN', 'prNB', 'prRF', 'prNN', 'rcNB', 'rcRF', 'rcNN'])\\nif(os.path.exists('../../data/results/classifier_results_10.pkl.gz')):\\n    results = pd.concat([pd.read_pickle('../../data/results/classifier_results_10.pkl.gz'), results]).drop_duplicates()\\n    \\nresults.to_pickle('../../data/results/classifier_results_10.pkl.gz')\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "counts = trainDF['label'].value_counts().sort_index().to_frame()\n",
    "counts['category'] = counts.index\n",
    "counts['train_sample']=(counts['label']/2).astype(int)\n",
    "\n",
    "def dataformodel(trainDF):\n",
    "    \n",
    "    train_df, test_df = np.split(trainDF, [int(.7*len(trainDF))])\n",
    "\n",
    "    #tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    train_posts = train_df['text']\n",
    "    train_tags = train_df['label']\n",
    "    test_posts = test_df['text']\n",
    "    test_tags = test_df['label']\n",
    "    vocab_size = 1000\n",
    "    tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "    tokenize.fit_on_texts(train_posts)\n",
    "\n",
    "    x_train = tokenize.texts_to_matrix(train_posts)\n",
    "    x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "    encoder = preprocessing.LabelBinarizer()\n",
    "    encoder.fit(train_tags)\n",
    "    y_train = encoder.transform(train_tags)\n",
    "    y_test = encoder.transform(test_tags)\n",
    "    \n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    num_labels=26\n",
    "    batch_size = 500\n",
    "    epochs = 20\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(512, input_shape=(vocab_size,)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dense(num_labels))\n",
    "    model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  #metrics=['accuracy'],\n",
    "                 metrics=['binary_accuracy', precision, recall])\n",
    "\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs, \n",
    "                        verbose=1, \n",
    "                        validation_split=0.1)\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, \n",
    "                       batch_size=batch_size, verbose=1)\n",
    "    return score\n",
    "\n",
    "def test(train):\n",
    "    test_posts1 = train['text']\n",
    "    test_tags1 = train['label']\n",
    "    x_test1 = tokenize.texts_to_matrix(test_posts1)\n",
    "    y_test1 = encoder.transform(test_tags1)\n",
    "    return x_test1, y_test1\n",
    "    \n",
    "def get_random(trainDF):\n",
    "    train = pd.DataFrame()\n",
    "    for rec in counts.values:\n",
    "        train=pd.concat([train, trainDF.loc[trainDF['label']==rec[1]].sample(n=rec[2])])\n",
    "    return train\n",
    "        \n",
    "accuracy, precision, recall = [],[],[]\n",
    "for iterate in range(0, 10):\n",
    "     #train = train.sample(n=rec[2])\n",
    "    #train = get_random(trainDF)\n",
    "    train = trainDF.sample(n=(int(len(trainDF)/3)))\n",
    "    #x_result, y_result =dataformodel(train)\n",
    "    x_result, y_result =test(train)\n",
    "    result = model.evaluate(x_result, y_result, batch_size=batch_size, verbose=1)\n",
    "    print(result)\n",
    "    accuracy.append(result[1])\n",
    "    precision.append(result[2])\n",
    "    recall.append(result[3])\n",
    "            \n",
    "stats = []                                                      \n",
    "stats.append([statistics.mean(accuracy), statistics.stdev(accuracy)])                                                                                                                                     \n",
    "stats.append([statistics.mean(precision), statistics.stdev(precision)])                                                                                                                                             \n",
    "stats.append([statistics.mean(recall), statistics.stdev(recall)])                                                                                                                                         \n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(stats)\n",
    "stats = pd.DataFrame(stats)  \n",
    "\n",
    "'''\n",
    "if(os.path.exists('../../data/results/classifier_stats_10.pkl.gz')):\n",
    "    stats = pd.concat([pd.read_pickle('../../data/results/classifier_stats_10.pkl.gz'), stats]).drop_duplicates()\n",
    "    \n",
    "stats.to_pickle('../../data/results/classifier_stats_10.pkl.gz')\n",
    "\n",
    "results = pd.DataFrame(results, columns=['acNB','acRF', 'acNN', 'prNB', 'prRF', 'prNN', 'rcNB', 'rcRF', 'rcNN'])\n",
    "if(os.path.exists('../../data/results/classifier_results_10.pkl.gz')):\n",
    "    results = pd.concat([pd.read_pickle('../../data/results/classifier_results_10.pkl.gz'), results]).drop_duplicates()\n",
    "    \n",
    "results.to_pickle('../../data/results/classifier_results_10.pkl.gz')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TO ADVANCE, PROMOTE, AND SUPPORT STUDIES AND RESEARCH OF THE SOCIAL, ECONOMICAL, POLITICAL, SOCIOLOG ...\n",
      "Actual label:X\n",
      "Predicted label: V\n",
      "THE MISSION OF THE OWENS COMMUNITY COLLEGE FOUNDATION IS TO DEVELOP AND PROVIDE RESOURCES TO ADVANCE ...\n",
      "Actual label:B\n",
      "Predicted label: B\n",
      "SHS provides management services for the delivery of healthcare through its affiliated entities. ...\n",
      "Actual label:E\n",
      "Predicted label: E\n",
      "THE PRINCIPAL PURPOSE OF THE ORGANIZATION, HEREINAFTER REFERRED TO AS THE FOUNDATION, SHALL BE TO FO ...\n",
      "Actual label:B\n",
      "Predicted label: B\n",
      "THE PRINCIPAL PURPOSE OF THE ORGANIZATION, HEREINAFTER REFERRED TO AS THE FOUNDATION, SHALL BE TO FO ...\n",
      "Actual label:B\n",
      "Predicted label: B\n",
      "THE PROMOTION AND DEVELOPMENT OF SCHOLARSHIP, LEADERSHIP SKILLS, CHARACTER, ARTISTIC & PHYSICAL/ATHL ...\n",
      "Actual label:O\n",
      "Predicted label: O\n",
      "THE PURPOSE OF THE FOUNDATION IS TO SUPPORT LAKEVIEW VILLAGE, INC., A 501(C)(3) ORGANIZATION. ...\n",
      "Actual label:T\n",
      "Predicted label: T\n",
      "TO PROVIDE AFFORDABLE, SAFE HOUSING FOR THE ELDERLY. ...\n",
      "Actual label:L\n",
      "Predicted label: L\n",
      "TO SUPPORT AND ENRICH THE PROGRAMS AVAILABLE TO THE STUDENTS OF KENTER CANYON SCHOOL. ...\n",
      "Actual label:B\n",
      "Predicted label: B\n",
      "TO DEVELOP AND MAINTAIN SCHOLARSHIP FUNDS FOR THOSE PROPERLY SELECTED AS SCHOLARSHIP RECIPIENTS BY T ...\n",
      "Actual label:B\n",
      "Predicted label: B\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    \n",
    "    text_labels = encoder.classes_ \n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    print(test_posts.iloc[i][:100], \"...\")\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
