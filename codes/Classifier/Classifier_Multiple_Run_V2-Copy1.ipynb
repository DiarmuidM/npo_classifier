{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "#Can't use keras on 129.114.32.33:8000\n",
    "#from keras import layers, models, optimizers\n",
    "import datetime, os\n",
    "import statistics, tqdm\n",
    "\n",
    "#to-do list\n",
    "#1. Record the amount of time a classifier takes: start(timestamp) - end(timestamp), put it in .pkl.gz\n",
    "\n",
    "#Observations:\n",
    "# 1. RF gives highest accuracy, but takes a lot of time to train: 25 minutes and 15 minutes\n",
    "# 2. Neural Network is weakest\n",
    "# 3. NB gives satisfactory results within a minute.\n",
    "\n",
    "trainDF = pd.concat([pd.read_pickle('../../data/2015/MasterData_2015.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2014/MasterData_2014.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2013/MasterData_2013.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2012/MasterData_2012.pkl.gz')])\n",
    "                                   \n",
    "trainDF = trainDF[trainDF.TEXT.notna() & trainDF.NTEE.notna()]\n",
    "trainDF['text'] = trainDF['TEXT'].astype(str)\n",
    "trainDF['label'] = trainDF['NTEE'].astype(str)\n",
    "\n",
    "counts = trainDF['label'].value_counts().sort_index().to_frame()\n",
    "counts['category'] = counts.index\n",
    "counts['train_sample']=(counts['label']/2).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class & samplesize:  A 22622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class & samplesize:  B 39643\n"
     ]
    }
   ],
   "source": [
    "def dataformodel(trainDF):\n",
    "    # split the dataset into training and validation datasets \n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    count_vect.fit(trainDF['text'])\n",
    "\n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)  \n",
    "    \n",
    "    # word level tf-idf\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "    \n",
    "    def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "        if is_neural_net:\n",
    "            predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "        return metrics.accuracy_score(predictions, valid_y), metrics.precision_score(predictions, valid_y, average='weighted'), metrics.recall_score(predictions, valid_y, average='weighted')\n",
    "\n",
    "\n",
    "    # Naive Bayes on Count Vectors\n",
    "    acNB, prNB, rcNB = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "    \n",
    "    # RF on Count Vectors\n",
    "    acRF, prRF, rcRF = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    \n",
    "    acNN, prNN, rcNN = 0, 0, 0\n",
    "    return [acNB, acRF, acNN, prNB, prRF, prNN, rcNB, rcRF, rcNN]\n",
    "    \n",
    "\n",
    "for rec in counts.values:\n",
    "    print('class & samplesize: ', rec[1], rec[2]) \n",
    "    \n",
    "    train=trainDF.loc[trainDF['label']==rec[1]]\n",
    "    accuracy, precision, recall = [[],[],[]], [[],[],[]], [[],[],[]]\n",
    "    results = [[rec[1], rec[2]]]\n",
    "    for iterate in range(0, 10):\n",
    "        train = train.sample(n=rec[2])\n",
    "        \n",
    "        result=dataformodel(train)\n",
    "        results.append(result)\n",
    "        for j in range(0,3):\n",
    "            accuracy[j].append(result[j])\n",
    "            precision[j].append(result[j+3])\n",
    "            recall[j].append(result[j+6])\n",
    "            \n",
    "    stats = [[], [], []]                                                                                                                                        \n",
    "    for j in range(0,3):\n",
    "        stats[0].append([statistics.mean(accuracy[j]), statistics.stdev(accuracy[j])])                                                                                                                                     \n",
    "        stats[1].append([statistics.mean(precision[j]), statistics.stdev(precision[j])])                                                                                                                                             \n",
    "        stats[2].append([statistics.mean(recall[j]), statistics.stdev(recall[j])])                                                                                                                                         \n",
    "    \n",
    "    stats = pd.DataFrame(stats)                                                                                                                                                             \n",
    "    if(os.path.exists('../../data/results/classifier_stats_10.pkl.gz')):\n",
    "        stats = pd.concat([pd.read_pickle('../../data/results/classifier_stats_10.pkl.gz'), stats]).drop_duplicates()\n",
    "    \n",
    "    stats.to_pickle('../../data/results/classifier_stats_10.pkl.gz')\n",
    "\n",
    "    results = pd.DataFrame(results, columns=['acNB','acRF', 'acNN', 'prNB', 'prRF', 'prNN', 'rcNB', 'rcRF', 'rcNN'])\n",
    "    if(os.path.exists('../../data/results/classifier_results_10.pkl.gz')):\n",
    "        results = pd.concat([pd.read_pickle('../../data/results/classifier_results_10.pkl.gz'), results]).drop_duplicates()\n",
    "    \n",
    "    results.to_pickle('../../data/results/classifier_results_10.pkl.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
