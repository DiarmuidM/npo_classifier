{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  [0.6451042830229573, 0.6659661298547271, 0.6451042830229573]\n",
      "NB, WordLevel TF-IDF:  [0.6196295493572965, 0.6661719314247055, 0.6196295493572965]\n",
      "NB, N-Gram Vectors:  [0.5338579439044586, 0.6007282836695058, 0.5338579439044586]\n",
      "NB, CharLevel Vectors:  [0.5761824577782965, 0.622573367036641, 0.5761824577782965]\n",
      "RF, Count Vectors:  [0.8889426096041524, 0.892335117353861, 0.8889426096041524]\n",
      "RF, WordLevel TF-IDF:  [0.8888648071889431, 0.8923074915133422, 0.8888648071889431]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "#Can't use keras on 129.114.32.33:8000\n",
    "#from keras import layers, models, optimizers\n",
    "import datetime, os\n",
    "\n",
    "#to-do list\n",
    "#1. Record the amount of time a classifier takes: start(timestamp) - end(timestamp), put it in .pkl.gz\n",
    "\n",
    "#Observations:\n",
    "# 1. RF gives highest accuracy, but takes a lot of time to train: 25 minutes and 15 minutes\n",
    "# 2. Neural Network is weakest\n",
    "# 3. NB gives satisfactory results within a minute.\n",
    "\n",
    "trainDF = pd.concat([pd.read_pickle('../../data/2015/MasterData_2015.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2014/MasterData_2014.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2013/MasterData_2013.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2012/MasterData_2012.pkl.gz')])\n",
    "                                   \n",
    "trainDF = trainDF[trainDF.TEXT.notna()]\n",
    "trainDF['text'] = trainDF['TEXT'].astype(str)\n",
    "trainDF['label'] = trainDF['NTEE'].astype(str)\n",
    "\n",
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return [metrics.accuracy_score(predictions, valid_y), \n",
    "            metrics.precision_score(predictions, valid_y, average='weighted'), \n",
    "            metrics.recall_score(predictions, valid_y, average='weighted')]\n",
    "\n",
    "results = pd.DataFrame(columns=['classifier', 'accuracy', 'precision', 'recall', 'time'])\n",
    "\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "results.loc[len(results)] = [\"NB, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "results.loc[len(results)] = [\"NB, WordLevel TF-IDF\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "results.loc[len(results)] = [\"NB, N-Gram Vectors\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "results.loc[len(results)] = [\"NB, CharLevel Vectors\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "results.loc[len(results)] = [\"RF, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "results.loc[len(results)] = [\"RF, WordLevel TF-IDF\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "\n",
    "classifier = create_model_architecture(xtrain_count.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_count, train_y, xvalid_count, is_neural_net=True)\n",
    "results.loc[len(results)] = [\"NN, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"NN, Count Vectors\",  accuracy)\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram_chars.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, is_neural_net=True)\n",
    "results.loc[len(results)] = [\"NN, CharLevel Vectors\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"NN, CharLevel Vectors\",  accuracy)\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "results.loc[len(results)] = [\"NN, Ngram Level TF IDF Vectors\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf, train_y, xvalid_tfidf, is_neural_net=True)\n",
    "results.loc[len(results)] = [\"NN, World Level TF-IDF\", accuracy[0], accuracy[1], accuracy[2], datetime.datetime.now()]\n",
    "print(\"NN, World Level TF-IDF\",  accuracy)\n",
    "\n",
    "\n",
    "if(os.path.exists('../../data/results/classifier_results.pkl.gz')):\n",
    "    results = pd.concat([pd.read_pickle('../../data/results/classifier_results.pkl.gz'), results]).drop_duplicates()\n",
    "\n",
    "results.to_pickle('../../data/results/classifier_results.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       classifier  accuracy  precision    recall  \\\n",
      "0               NB, Count Vectors  0.645104   0.665966  0.645104   \n",
      "1            NB, WordLevel TF-IDF  0.619630   0.666172  0.619630   \n",
      "2              NB, N-Gram Vectors  0.533858   0.600728  0.533858   \n",
      "3           NB, CharLevel Vectors  0.576182   0.622573  0.576182   \n",
      "4               RF, Count Vectors  0.888943   0.892335  0.888943   \n",
      "5            RF, WordLevel TF-IDF  0.888865   0.892307  0.888865   \n",
      "0               NB, Count Vectors  0.645882   0.666515  0.645882   \n",
      "1            NB, WordLevel TF-IDF  0.619741   0.666131  0.619741   \n",
      "2              NB, N-Gram Vectors  0.535358   0.600996  0.535358   \n",
      "3           NB, CharLevel Vectors  0.579117   0.624497  0.579117   \n",
      "4  NN, Ngram Level TF IDF Vectors  0.062820   1.000000  0.062820   \n",
      "5          NN, World Level TF-IDF  0.062820   1.000000  0.062820   \n",
      "6               NN, Count Vectors  0.062820   1.000000  0.062820   \n",
      "7           NN, CharLevel Vectors  0.062820   1.000000  0.062820   \n",
      "\n",
      "                        time  \n",
      "0 2018-10-30 21:39:33.995847  \n",
      "1 2018-10-30 21:39:38.685984  \n",
      "2 2018-10-30 21:39:43.415527  \n",
      "3 2018-10-30 21:39:52.270574  \n",
      "4 2018-10-30 22:05:05.399057  \n",
      "5 2018-10-30 22:16:22.406043  \n",
      "0 2018-10-30 16:05:06.224376  \n",
      "1 2018-10-30 16:05:13.795031  \n",
      "2 2018-10-30 16:05:20.493147  \n",
      "3 2018-10-30 16:05:30.424554  \n",
      "4 2018-10-30 16:38:39.194332  \n",
      "5 2018-10-30 16:54:37.266082  \n",
      "6 2018-10-30 17:27:06.401213  \n",
      "7 2018-10-30 17:33:46.752297  \n"
     ]
    }
   ],
   "source": [
    "temp = pd.concat([pd.read_pickle('../../data/results/classifier_results.pkl.gz'),\n",
    "                pd.read_pickle('../../data/results/classifier_results1.pkl.gz')]).drop_duplicates()\n",
    "\n",
    "temp.to_pickle('../../data/results/classifier_results.pkl.gz')\n",
    "   \n",
    "print(pd.read_pickle('../../data/results/classifier_results.pkl.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
