{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# split the dataset into training and validation datasets \\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF[\\'text\\'], trainDF[\\'label\\'])\\n\\n# create a count vectorizer object \\ncount_vect = CountVectorizer(analyzer=\\'word\\', token_pattern=r\\'\\\\w{1,}\\')\\ncount_vect.fit(trainDF[\\'text\\'])\\n\\n# transform the training and validation data using count vectorizer object\\nxtrain_count =  count_vect.transform(train_x)\\nxvalid_count =  count_vect.transform(valid_x)\\n\\n# word level tf-idf\\ntfidf_vect = TfidfVectorizer(analyzer=\\'word\\', token_pattern=r\\'\\\\w{1,}\\', max_features=5000)\\ntfidf_vect.fit(trainDF[\\'text\\'])\\nxtrain_tfidf =  tfidf_vect.transform(train_x)\\nxvalid_tfidf =  tfidf_vect.transform(valid_x)\\n\\n# ngram level tf-idf \\ntfidf_vect_ngram = TfidfVectorizer(analyzer=\\'word\\', token_pattern=r\\'\\\\w{1,}\\', ngram_range=(2,3), max_features=5000)\\ntfidf_vect_ngram.fit(trainDF[\\'text\\'])\\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\\nxvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\\n\\n# characters level tf-idf\\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer=\\'char\\', token_pattern=r\\'\\\\w{1,}\\', ngram_range=(2,3), max_features=5000)\\ntfidf_vect_ngram_chars.fit(trainDF[\\'text\\'])\\nxtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \\nxvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \\n\\ndef train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\\n    \\n    time1 = datetime.datetime.now()\\n    # fit the training dataset on the classifier\\n    classifier.fit(feature_vector_train, label)\\n    \\n    # predict the labels on validation dataset\\n    predictions = classifier.predict(feature_vector_valid)\\n    \\n    if is_neural_net:\\n        predictions = predictions.argmax(axis=-1)\\n    \\n    return [metrics.accuracy_score(predictions, valid_y), \\n            metrics.precision_score(predictions, valid_y, average=\\'weighted\\'), \\n            metrics.recall_score(predictions, valid_y, average=\\'weighted\\'),\\n            datetime.datetime.now()-time1]\\n\\nresults = pd.DataFrame(columns=[\\'classifier\\', \\'accuracy\\', \\'precision\\', \\'recall\\', \\'time\\'])\\n\\n# Naive Bayes on Count Vectors\\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\\nresults.loc[len(results)] = [\"NB, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"NB, Count Vectors: \", accuracy)\\n\\n# Naive Bayes on Word Level TF IDF Vectors\\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\\nresults.loc[len(results)] = [\"NB, WordLevel TF-IDF\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"NB, WordLevel TF-IDF: \", accuracy)\\n\\n# Naive Bayes on Ngram Level TF IDF Vectors\\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\\nresults.loc[len(results)] = [\"NB, N-Gram Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"NB, N-Gram Vectors: \", accuracy)\\n\\n# Naive Bayes on Character Level TF IDF Vectors\\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\\nresults.loc[len(results)] = [\"NB, CharLevel Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"NB, CharLevel Vectors: \", accuracy)\\n\\n# RF on Count Vectors\\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\\nresults.loc[len(results)] = [\"RF, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"RF, Count Vectors: \", accuracy)\\n\\n# RF on Word Level TF IDF Vectors\\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\\nresults.loc[len(results)] = [\"RF, WordLevel TF-IDF\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"RF, WordLevel TF-IDF: \", accuracy)\\n\\n# label encode the target variable \\nencoder = preprocessing.LabelEncoder()\\ntrain_y = encoder.fit_transform(train_y)\\nvalid_y = encoder.fit_transform(valid_y)\\n\\n\\ndef create_model_architecture(input_size):\\n    # create input layer \\n    input_layer = layers.Input((input_size, ), sparse=True)\\n    \\n    # create hidden layer\\n    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\\n    \\n    # create output layer\\n    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\\n\\n    classifier = models.Model(inputs = input_layer, outputs = output_layer)\\n    classifier.compile(optimizer=optimizers.Adam(), loss=\\'binary_crossentropy\\')\\n    return classifier \\n\\n\\nclassifier = create_model_architecture(xtrain_count.shape[1])\\naccuracy = train_model(classifier, xtrain_count, train_y, xvalid_count, is_neural_net=True)\\nresults.loc[len(results)] = [\"NN, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"NN, Count Vectors\",  accuracy)\\n\\nclassifier = create_model_architecture(xtrain_tfidf_ngram_chars.shape[1])\\naccuracy = train_model(classifier, xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, is_neural_net=True)\\nresults.loc[len(results)] = [\"NN, CharLevel Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"NN, CharLevel Vectors\",  accuracy)\\n\\nclassifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\\naccuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\\nresults.loc[len(results)] = [\"NN, Ngram Level TF IDF Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"NN, Ngram Level TF IDF Vectors\",  accuracy)\\n\\nclassifier = create_model_architecture(xtrain_tfidf.shape[1])\\naccuracy = train_model(classifier, xtrain_tfidf, train_y, xvalid_tfidf, is_neural_net=True)\\nresults.loc[len(results)] = [\"NN, World Level TF-IDF\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\\nprint(\"NN, World Level TF-IDF\",  accuracy)\\n\\n\\nif(os.path.exists(\\'../../data/results/classifier_results.pkl.gz\\')):\\n    results = pd.concat([pd.read_pickle(\\'../../data/results/classifier_results.pkl.gz\\'), results]).drop_duplicates()\\n\\nresults.to_pickle(\\'../../data/results/classifier_results.pkl.gz\\')\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "#Can't use keras on 129.114.32.33:8000\n",
    "#from keras import layers, models, optimizers\n",
    "import datetime, os\n",
    "\n",
    "#to-do list\n",
    "#1. Record the amount of time a classifier takes: start(timestamp) - end(timestamp), put it in .pkl.gz\n",
    "\n",
    "#Observations:\n",
    "# 1. RF gives highest accuracy, but takes a lot of time to train: 25 minutes and 15 minutes\n",
    "# 2. Neural Network is weakest\n",
    "# 3. NB gives satisfactory results within a minute.\n",
    "\n",
    "trainDF = pd.concat([pd.read_pickle('../../data/2015/MasterData_2015.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2014/MasterData_2014.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2013/MasterData_2013.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2012/MasterData_2012.pkl.gz')])\n",
    "                                   \n",
    "trainDF = trainDF[trainDF.TEXT.notna() & trainDF.NTEE.notna()]\n",
    "trainDF['text'] = trainDF['TEXT'].astype(str)\n",
    "trainDF['label'] = trainDF['NTEE'].astype(str)\n",
    "\n",
    "'''\n",
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    \n",
    "    time1 = datetime.datetime.now()\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return [metrics.accuracy_score(predictions, valid_y), \n",
    "            metrics.precision_score(predictions, valid_y, average='weighted'), \n",
    "            metrics.recall_score(predictions, valid_y, average='weighted'),\n",
    "            datetime.datetime.now()-time1]\n",
    "\n",
    "results = pd.DataFrame(columns=['classifier', 'accuracy', 'precision', 'recall', 'time'])\n",
    "\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "results.loc[len(results)] = [\"NB, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "results.loc[len(results)] = [\"NB, WordLevel TF-IDF\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "results.loc[len(results)] = [\"NB, N-Gram Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "results.loc[len(results)] = [\"NB, CharLevel Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "results.loc[len(results)] = [\"RF, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "results.loc[len(results)] = [\"RF, WordLevel TF-IDF\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "\n",
    "classifier = create_model_architecture(xtrain_count.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_count, train_y, xvalid_count, is_neural_net=True)\n",
    "results.loc[len(results)] = [\"NN, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NN, Count Vectors\",  accuracy)\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram_chars.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, is_neural_net=True)\n",
    "results.loc[len(results)] = [\"NN, CharLevel Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NN, CharLevel Vectors\",  accuracy)\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "results.loc[len(results)] = [\"NN, Ngram Level TF IDF Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf, train_y, xvalid_tfidf, is_neural_net=True)\n",
    "results.loc[len(results)] = [\"NN, World Level TF-IDF\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NN, World Level TF-IDF\",  accuracy)\n",
    "\n",
    "\n",
    "if(os.path.exists('../../data/results/classifier_results.pkl.gz')):\n",
    "    results = pd.concat([pd.read_pickle('../../data/results/classifier_results.pkl.gz'), results]).drop_duplicates()\n",
    "\n",
    "results.to_pickle('../../data/results/classifier_results.pkl.gz')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [0 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras, Session\n",
    "import numpy as np\n",
    "\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "print(type(train_data),test_labels)\n",
    "#print(train_x.values[0])\n",
    "\n",
    "sets = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=pd.DataFrame(trainDF['text']),\n",
    "    y=pd.DataFrame(trainDF['label']),\n",
    "    batch_size=256,\n",
    "    num_epochs=1,\n",
    "    shuffle=True,\n",
    "    queue_capacity=1000,\n",
    "    num_threads=1,\n",
    "    target_column='target')()\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainDF['text'], trainDF['label'], test_size = 0.25, random_state = 0)\n",
    "\n",
    "\n",
    "# write an input function\n",
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train, y=y_train, batch_size=10, num_epochs=1000, shuffle=True)\n",
    "eval_input_func = tf.estimator.inputs.pandas_input_fn(x=X_test, y=y_test, batch_size=10, num_epochs=1, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
