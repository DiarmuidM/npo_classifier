{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'EIN2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cefa1e23ca2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtrainDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labelcc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_devdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EIN2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-cefa1e23ca2e>\u001b[0m in \u001b[0;36m_devdataset\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_devdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelscc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'EIN2.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import csv\n",
    "import pandas as pd, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "def _devdataset(file):\n",
    "    train = csv.reader(open(file))\n",
    "    texts, labels1, labelscc = [], [], []\n",
    "    \n",
    "    for row in train:\n",
    "        texts.append(row[1])\n",
    "        labels1.append(row[2])\n",
    "        labelscc.append(int(row[3][1:]))\n",
    "            \n",
    "    return texts, labels1, labelscc\n",
    "    \n",
    "\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'], trainDF['label'], trainDF['labelcc'] = _devdataset('EIN2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "data=pd.read_csv(\"/Users/Rushi/Documents/GRAFall2018/%Dist/DistributionMajorGroup.csv\")\n",
    "samplesize = pd.DataFrame()\n",
    "\n",
    "for yr in range(1989,2016):\n",
    "    count = pd.to_numeric(data[str(yr)].drop(labels=[0,27])).apply(lambda x: int(x/2))\n",
    "    samplesize.insert(loc=yr-1989, column=yr, value=count)\n",
    "'''    \n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y), metrics.precision_score(predictions, valid_y, average='weighted'), metrics.recall_score(predictions, valid_y, average='weighted')\n",
    "\n",
    "'''\n",
    "def dataformodel(trainDF):\n",
    "    # split the dataset into training and validation datasets \n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    count_vect.fit(trainDF['text'])\n",
    "\n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)  \n",
    "    \n",
    "    # word level tf-idf\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "    \n",
    "    # ngram level tf-idf \n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram.fit(trainDF['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    \n",
    "    def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "        if is_neural_net:\n",
    "            predictions = predictions.argmax(axis=-1)\n",
    "        \n",
    "        return metrics.accuracy_score(predictions, valid_y), metrics.precision_score(predictions, valid_y, average='weighted'), metrics.recall_score(predictions, valid_y, average='weighted')\n",
    "\n",
    "    def create_model_architecture(input_size):\n",
    "        # create input layer \n",
    "        input_layer = layers.Input((input_size, ), sparse=True)\n",
    "\n",
    "        # create hidden layer\n",
    "        hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "\n",
    "        # create output layer\n",
    "        output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "        classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "        classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "        return classifier\n",
    "\n",
    "    # Naive Bayes on Count Vectors\n",
    "    acNB, prNB, rcNB = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "    #print(\"NB, Count Vectors: \", accuracyNB)\n",
    "\n",
    "    # RF on Count Vectors\n",
    "    acRF, prRF, rcRF = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "    #print(\"RF, Count Vectors: \", accuracyRF)\n",
    "    \n",
    "    # label encode the target variable \n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    train_y = encoder.fit_transform(train_y)\n",
    "    valid_y = encoder.fit_transform(valid_y)\n",
    "        \n",
    "    classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "    acNN, prNN, rcNN = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "    #print(\"NN, Ngram Level TF IDF Vectors\",  accuracyNN)\n",
    "    \n",
    "    return [acNB, acRF, acNN, prNB, prRF, prNN, rcNB, rcRF, rcNN]\n",
    "    \n",
    "\n",
    "year = 2015\n",
    "for i in range(1,27):\n",
    "    print('class: ', i) \n",
    "    #7\n",
    "    size=int(samplesize[year][i])\n",
    "    train=trainDF.loc[trainDF['label']==chr(i+64)]\n",
    "    accuracy, precision, recall = [[],[],[]], [[],[],[]], [[],[],[]]\n",
    "    #8\n",
    "    for count in range(1, 1001):\n",
    "        train = train.sample(frac=1)\n",
    "        result=dataformodel(train)\n",
    "        for j in range(0,3):\n",
    "            accuracy[j].append(result[j])\n",
    "            precision[j].append(result[j+3])\n",
    "            recall[j].append(result[j+6])\n",
    "    stats = [[], [], []]                                                                                                                                        \n",
    "    for j in range(0,3):\n",
    "        stats[0].append([statistics.mean(accuracy[j]), statistics.stdev(accuracy[j])])                                                                                                                                     \n",
    "        stats[1].append([statistcis.mean(precision[j]), statistics.stdev(precision[j])])                                                                                                                                             \n",
    "        stats[2].append([statistics.mane(recall[j]), statistics.stdev(recall[j])])                                                                                                                                         \n",
    "    \n",
    "    stats = pd.DataFrame(stats)                                                                                                                                             \n",
    "    print(stats)\n",
    "    with open('statsbyyear.csv', 'a') as f:\n",
    "          stats.to_csv(f, header=False)                \n",
    "        \n",
    "    result = pd.DataFrame(result, columns=['acNB','acRF', 'acNN', 'prNB', 'prRF', 'prNN', 'rcNB', 'rcRF', 'rcNN'])\n",
    "    result.to_csv(results.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
