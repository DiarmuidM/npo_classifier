{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code as 10 broad categories.\n",
    "broad_cat_dict={'I': ['A'],\n",
    "                  'II': ['B'],\n",
    "                  'III': ['C', 'D'],\n",
    "                  'IV': ['E', 'F', 'G', 'H'],\n",
    "                  'V': ['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'],\n",
    "                  'VI': ['Q'],\n",
    "                  'VII': ['R', 'S', 'T', 'U', 'V', 'W'],\n",
    "                  'VIII': ['X'],\n",
    "                  'IX': ['Y'],\n",
    "                  'X': ['Z'],\n",
    "                 }\n",
    "def ntee2cat(string):\n",
    "    global broad_cat_dict\n",
    "    return [s for s in broad_cat_dict.keys() if string in broad_cat_dict[s]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234027"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234027, 25, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['input_text']=df_train['mission_spellchk']+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_train['input_text']=[' '.join(s) for s in df_train['input_text']]\n",
    "df_train['broad_cat']=df_train['NTEE1'].apply(ntee2cat)\n",
    "\n",
    "len(df_train['input_text']), len(df_train['NTEE1'].drop_duplicates()), len(df_train['broad_cat'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broad_cat\n",
      "I       10710\n",
      "II      15835\n",
      "III      4908\n",
      "IV      11121\n",
      "IX       3904\n",
      "V       28621\n",
      "VI       1311\n",
      "VII     16813\n",
      "VIII     2777\n",
      "Name: EIN, dtype: int64 \n",
      "\n",
      " broad_cat\n",
      "I       2598\n",
      "II      4031\n",
      "III     1224\n",
      "IV      2830\n",
      "IX       953\n",
      "V       7162\n",
      "VI       340\n",
      "VII     4167\n",
      "VIII     695\n",
      "Name: EIN, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the sampling criteria can be satisfied.\n",
    "small_num=0\n",
    "while small_num<500: # Make sure each category has at least 500 records.\n",
    "    sampleDF = df_train[df_train.input_text.notna() & df_train.broad_cat.notna()].sample(120000)\n",
    "    trainDF, valDF =train_test_split(sampleDF, test_size=.2)\n",
    "    small_num=trainDF.groupby('broad_cat').count().sort_values('EIN').iloc[0]['EIN']\n",
    "# See the composition by broad category.\n",
    "print(trainDF.groupby('broad_cat').count()['EIN'], '\\n'*2, valDF.groupby('broad_cat').count()['EIN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Parrallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "print(c.ids)\n",
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.execute('from sklearn import model_selection, preprocessing, naive_bayes, metrics')\n",
    "dview.execute('from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer')\n",
    "dview.execute('from sklearn import decomposition, ensemble')\n",
    "dview.execute('from nltk.stem import PorterStemmer')\n",
    "dview.execute('from nltk import word_tokenize')\n",
    "dview.execute('from nltk.stem import WordNetLemmatizer')\n",
    "dview.execute('from nltk.corpus import wordnet')\n",
    "dview.execute('import pandas as pd')\n",
    "dview.execute('import nltk')\n",
    "dview.execute('from sklearn.model_selection import train_test_split')\n",
    "dview['df_train']=df_train\n",
    "dview['df_performance']=pd.DataFrame(columns=['trial', 'classifier', 'tokenizer', 'vect_type', 'average_mtd',\n",
    "                                              'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_naive_bayes(trial):\n",
    "    global df_train, df_performance, classifier, tokenizer, vect_type, average_mtd\n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    # Build training and testing data frame.\n",
    "    small_num=0\n",
    "    while small_num<500: # Make sure each category has at least 500 records.\n",
    "        sampleDF = df_train[df_train.input_text.notna() & df_train.broad_cat.notna()].sample(120000)\n",
    "        trainDF, valDF =train_test_split(sampleDF, test_size=.2)\n",
    "        small_num=trainDF.groupby('broad_cat').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    x_train=trainDF['input_text']\n",
    "    y_train=trainDF['broad_cat']\n",
    "    x_valid=valDF['input_text']\n",
    "    y_valid=valDF['broad_cat']\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "\n",
    "    def porter_tokenizer(token_list):\n",
    "        return [PorterStemmer().stem(token) for token in token_list]\n",
    "    \n",
    "    # Lemmatize using POS tags, assume to improve accuracy.\n",
    "    # Ref: \n",
    "    #   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    #   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemma_tokenizer(token_list):\n",
    "        return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(token_list)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=lemma_tokenizer\n",
    "    elif tokenizer=='porter':\n",
    "        tokenizer=porter_tokenizer\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab.\n",
    "        vectorizer.fit(trainDF['input_text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['input_text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':tokenizer.__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                                            'f1':metrics.f1_score(y_pred=predictions, y_true=y_valid, average=average_mtd)\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [naive_bayes.MultinomialNB(), naive_bayes.ComplementNB()]:\n",
    "    for tokenizer in ['lemma', 'porter']:\n",
    "        for vect_type in ['count', 'tfidf']:\n",
    "            for average_mtd in ['macro', 'weighted']:\n",
    "                dview['classifier']=classifier\n",
    "                dview['tokenizer']=tokenizer\n",
    "                dview['vect_type']=vect_type\n",
    "                dview['average_mtd']=average_mtd\n",
    "                t=func_naive_bayes.map(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>7</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.29950</td>\n",
       "      <td>0.089723</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.138080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.32025</td>\n",
       "      <td>0.306693</td>\n",
       "      <td>0.199972</td>\n",
       "      <td>0.181066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>21</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.33600</td>\n",
       "      <td>0.275419</td>\n",
       "      <td>0.216099</td>\n",
       "      <td>0.203548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>20</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.29750</td>\n",
       "      <td>0.088506</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.136426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>20</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.30275</td>\n",
       "      <td>0.033639</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.051643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>35</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.30125</td>\n",
       "      <td>0.090774</td>\n",
       "      <td>0.301250</td>\n",
       "      <td>0.139510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>46</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.32900</td>\n",
       "      <td>0.317654</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.311619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>42</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.31975</td>\n",
       "      <td>0.342044</td>\n",
       "      <td>0.319750</td>\n",
       "      <td>0.299996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>16</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.31700</td>\n",
       "      <td>0.276929</td>\n",
       "      <td>0.207232</td>\n",
       "      <td>0.189518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>24</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.29175</td>\n",
       "      <td>0.032417</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.050190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial                                         classifier  \\\n",
       "111     7  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "81      5  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "345    21  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "334    20  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "324    20  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "559    35  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "762    46  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "690    42  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "272    16  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "396    24  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "            tokenizer vect_type average_mtd  accuracy  precision    recall  \\\n",
       "111  porter_tokenizer     tfidf    weighted   0.29950   0.089723  0.299500   \n",
       "81    lemma_tokenizer     count       macro   0.32025   0.306693  0.199972   \n",
       "345  porter_tokenizer     count       macro   0.33600   0.275419  0.216099   \n",
       "334  porter_tokenizer     tfidf    weighted   0.29750   0.088506  0.297500   \n",
       "324   lemma_tokenizer     tfidf       macro   0.30275   0.033639  0.111111   \n",
       "559  porter_tokenizer     tfidf    weighted   0.30125   0.090774  0.301250   \n",
       "762  porter_tokenizer     count    weighted   0.32900   0.317654  0.329000   \n",
       "690   lemma_tokenizer     count    weighted   0.31975   0.342044  0.319750   \n",
       "272   lemma_tokenizer     count       macro   0.31700   0.276929  0.207232   \n",
       "396  porter_tokenizer     tfidf       macro   0.29175   0.032417  0.111111   \n",
       "\n",
       "           f1  \n",
       "111  0.138080  \n",
       "81   0.181066  \n",
       "345  0.203548  \n",
       "334  0.136426  \n",
       "324  0.051643  \n",
       "559  0.139510  \n",
       "762  0.311619  \n",
       "690  0.299996  \n",
       "272  0.189518  \n",
       "396  0.050190  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"8\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.330771</td>\n",
       "      <td>0.010741</td>\n",
       "      <td>0.31150</td>\n",
       "      <td>0.321375</td>\n",
       "      <td>0.330250</td>\n",
       "      <td>0.337313</td>\n",
       "      <td>0.35725</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.195657</td>\n",
       "      <td>0.008216</td>\n",
       "      <td>0.179390</td>\n",
       "      <td>0.189915</td>\n",
       "      <td>0.195040</td>\n",
       "      <td>0.201633</td>\n",
       "      <td>0.214091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.330302</td>\n",
       "      <td>0.009065</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.322688</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.336750</td>\n",
       "      <td>0.34925</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.212937</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.190653</td>\n",
       "      <td>0.207223</td>\n",
       "      <td>0.213442</td>\n",
       "      <td>0.218571</td>\n",
       "      <td>0.234282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>0.010073</td>\n",
       "      <td>0.31450</td>\n",
       "      <td>0.325750</td>\n",
       "      <td>0.332250</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.35675</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.196362</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.180416</td>\n",
       "      <td>0.191152</td>\n",
       "      <td>0.196314</td>\n",
       "      <td>0.201690</td>\n",
       "      <td>0.216636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.330859</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>0.31275</td>\n",
       "      <td>0.326125</td>\n",
       "      <td>0.331375</td>\n",
       "      <td>0.336813</td>\n",
       "      <td>0.34650</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.213189</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>0.199063</td>\n",
       "      <td>0.206591</td>\n",
       "      <td>0.211648</td>\n",
       "      <td>0.218394</td>\n",
       "      <td>0.233240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.338005</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.31525</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.337375</td>\n",
       "      <td>0.345125</td>\n",
       "      <td>0.36275</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.245097</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.220836</td>\n",
       "      <td>0.237919</td>\n",
       "      <td>0.243467</td>\n",
       "      <td>0.252375</td>\n",
       "      <td>0.271376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.298083</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>0.28275</td>\n",
       "      <td>0.294750</td>\n",
       "      <td>0.297250</td>\n",
       "      <td>0.302375</td>\n",
       "      <td>0.31925</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.048983</td>\n",
       "      <td>0.050591</td>\n",
       "      <td>0.051068</td>\n",
       "      <td>0.051643</td>\n",
       "      <td>0.053776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.336583</td>\n",
       "      <td>0.010510</td>\n",
       "      <td>0.31000</td>\n",
       "      <td>0.331625</td>\n",
       "      <td>0.334875</td>\n",
       "      <td>0.341875</td>\n",
       "      <td>0.36625</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.242004</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.226910</td>\n",
       "      <td>0.235184</td>\n",
       "      <td>0.241661</td>\n",
       "      <td>0.248324</td>\n",
       "      <td>0.264726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.299016</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>0.28475</td>\n",
       "      <td>0.294563</td>\n",
       "      <td>0.299625</td>\n",
       "      <td>0.303063</td>\n",
       "      <td>0.31475</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.049253</td>\n",
       "      <td>0.050564</td>\n",
       "      <td>0.051249</td>\n",
       "      <td>0.051684</td>\n",
       "      <td>0.053200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              accuracy  \\\n",
       "                                                                                 count   \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         48.0   \n",
       "                                                                    tfidf         48.0   \n",
       "                                                   porter_tokenizer count         48.0   \n",
       "                                                                    tfidf         48.0   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         48.0   \n",
       "                                                                    tfidf         48.0   \n",
       "                                                   porter_tokenizer count         48.0   \n",
       "                                                                    tfidf         48.0   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                   mean   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.330771   \n",
       "                                                                    tfidf      0.330302   \n",
       "                                                   porter_tokenizer count      0.332313   \n",
       "                                                                    tfidf      0.330859   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.338005   \n",
       "                                                                    tfidf      0.298083   \n",
       "                                                   porter_tokenizer count      0.336583   \n",
       "                                                                    tfidf      0.299016   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    std   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.010741   \n",
       "                                                                    tfidf      0.009065   \n",
       "                                                   porter_tokenizer count      0.010073   \n",
       "                                                                    tfidf      0.008206   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.010555   \n",
       "                                                                    tfidf      0.007340   \n",
       "                                                   porter_tokenizer count      0.010510   \n",
       "                                                                    tfidf      0.006964   \n",
       "\n",
       "                                                                                        \\\n",
       "                                                                                   min   \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.31150   \n",
       "                                                                    tfidf      0.31500   \n",
       "                                                   porter_tokenizer count      0.31450   \n",
       "                                                                    tfidf      0.31275   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.31525   \n",
       "                                                                    tfidf      0.28275   \n",
       "                                                   porter_tokenizer count      0.31000   \n",
       "                                                                    tfidf      0.28475   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    25%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.321375   \n",
       "                                                                    tfidf      0.322688   \n",
       "                                                   porter_tokenizer count      0.325750   \n",
       "                                                                    tfidf      0.326125   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.329000   \n",
       "                                                                    tfidf      0.294750   \n",
       "                                                   porter_tokenizer count      0.331625   \n",
       "                                                                    tfidf      0.294563   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    50%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.330250   \n",
       "                                                                    tfidf      0.331000   \n",
       "                                                   porter_tokenizer count      0.332250   \n",
       "                                                                    tfidf      0.331375   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.337375   \n",
       "                                                                    tfidf      0.297250   \n",
       "                                                   porter_tokenizer count      0.334875   \n",
       "                                                                    tfidf      0.299625   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    75%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.337313   \n",
       "                                                                    tfidf      0.336750   \n",
       "                                                   porter_tokenizer count      0.339000   \n",
       "                                                                    tfidf      0.336813   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.345125   \n",
       "                                                                    tfidf      0.302375   \n",
       "                                                   porter_tokenizer count      0.341875   \n",
       "                                                                    tfidf      0.303063   \n",
       "\n",
       "                                                                                        \\\n",
       "                                                                                   max   \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.35725   \n",
       "                                                                    tfidf      0.34925   \n",
       "                                                   porter_tokenizer count      0.35675   \n",
       "                                                                    tfidf      0.34650   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.36275   \n",
       "                                                                    tfidf      0.31925   \n",
       "                                                   porter_tokenizer count      0.36625   \n",
       "                                                                    tfidf      0.31475   \n",
       "\n",
       "                                                                                 f1  \\\n",
       "                                                                              count   \n",
       "classifier                                         tokenizer        vect_type         \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      48.0   \n",
       "                                                                    tfidf      48.0   \n",
       "                                                   porter_tokenizer count      48.0   \n",
       "                                                                    tfidf      48.0   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      48.0   \n",
       "                                                                    tfidf      48.0   \n",
       "                                                   porter_tokenizer count      48.0   \n",
       "                                                                    tfidf      48.0   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                   mean   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.195657   \n",
       "                                                                    tfidf      0.212937   \n",
       "                                                   porter_tokenizer count      0.196362   \n",
       "                                                                    tfidf      0.213189   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.245097   \n",
       "                                                                    tfidf      0.051062   \n",
       "                                                   porter_tokenizer count      0.242004   \n",
       "                                                                    tfidf      0.051181   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    std   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.008216   \n",
       "                                                                    tfidf      0.009549   \n",
       "                                                   porter_tokenizer count      0.007966   \n",
       "                                                                    tfidf      0.008556   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.011396   \n",
       "                                                                    tfidf      0.000964   \n",
       "                                                   porter_tokenizer count      0.008690   \n",
       "                                                                    tfidf      0.000938   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    min   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.179390   \n",
       "                                                                    tfidf      0.190653   \n",
       "                                                   porter_tokenizer count      0.180416   \n",
       "                                                                    tfidf      0.199063   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.220836   \n",
       "                                                                    tfidf      0.048983   \n",
       "                                                   porter_tokenizer count      0.226910   \n",
       "                                                                    tfidf      0.049253   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    25%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.189915   \n",
       "                                                                    tfidf      0.207223   \n",
       "                                                   porter_tokenizer count      0.191152   \n",
       "                                                                    tfidf      0.206591   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.237919   \n",
       "                                                                    tfidf      0.050591   \n",
       "                                                   porter_tokenizer count      0.235184   \n",
       "                                                                    tfidf      0.050564   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    50%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.195040   \n",
       "                                                                    tfidf      0.213442   \n",
       "                                                   porter_tokenizer count      0.196314   \n",
       "                                                                    tfidf      0.211648   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.243467   \n",
       "                                                                    tfidf      0.051068   \n",
       "                                                   porter_tokenizer count      0.241661   \n",
       "                                                                    tfidf      0.051249   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    75%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.201633   \n",
       "                                                                    tfidf      0.218571   \n",
       "                                                   porter_tokenizer count      0.201690   \n",
       "                                                                    tfidf      0.218394   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.252375   \n",
       "                                                                    tfidf      0.051643   \n",
       "                                                   porter_tokenizer count      0.248324   \n",
       "                                                                    tfidf      0.051684   \n",
       "\n",
       "                                                                                         \n",
       "                                                                                    max  \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.214091  \n",
       "                                                                    tfidf      0.234282  \n",
       "                                                   porter_tokenizer count      0.216636  \n",
       "                                                                    tfidf      0.233240  \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.271376  \n",
       "                                                                    tfidf      0.053776  \n",
       "                                                   porter_tokenizer count      0.264726  \n",
       "                                                                    tfidf      0.053200  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance[df_performance.average_mtd=='macro'].groupby(['classifier', 'tokenizer', 'vect_type']).describe()[['accuracy','f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Manually check if acc is correct.\n",
    "In :func_naive_bayes(2)\n",
    "    df_performance['accuracy']\n",
    "Out:0    0.340417\n",
    "    Name: accuracy, dtype: float64\n",
    "In :t=pd.DataFrame([classifier.predict(x_valid_vect), y_valid]).T.rename(columns={0:'a', 1:'b'})\n",
    "    len(t[t.a==t.b])/len(t)\n",
    "Out:0.34041666666666665\n",
    "''' Looks correct, scale the computing '''\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "print(c.ids)\n",
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_naive_bayes(trial):\n",
    "    global df_train, df_performance, txt_field, classifier, tokenizer, vect_type, average_mtd\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    # Build training and testing data frame.\n",
    "    small_num=0\n",
    "    while small_num<200: # Make sure each category has at least 500 records.\n",
    "        sampleDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "        trainDF, valDF =train_test_split(sampleDF, test_size=.3)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    x_train=trainDF['mission_prgrm']\n",
    "    y_train=trainDF['NTEE1']\n",
    "    x_valid=valDF['mission_prgrm']\n",
    "    y_valid=valDF['NTEE1']\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "\n",
    "    def porter_tokenizer(token_list):\n",
    "        return [PorterStemmer().stem(token) for token in token_list]\n",
    "    \n",
    "    # Lemmatize using POS tags, assume to improve accuracy.\n",
    "    # Ref: \n",
    "    #   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    #   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemma_tokenizer(token_list):\n",
    "        return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(token_list)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=lemma_tokenizer\n",
    "    elif tokenizer=='porter':\n",
    "        tokenizer=porter_tokenizer\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab.\n",
    "        vectorizer.fit(trainDF['mission_prgrm'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['mission_prgrm'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':tokenizer.__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                                            'f1':metrics.f1_score(y_pred=predictions, y_true=y_valid, average=average_mtd)\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Iterate different configurations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt_field in [#'mission', 'prgrm_dsc', \n",
    "                  'mission_prgrm']:\n",
    "    for classifier in [naive_bayes.MultinomialNB(), naive_bayes.ComplementNB()]:\n",
    "        for tokenizer in ['lemma', 'porter']:\n",
    "            for vect_type in ['count', 'tfidf']:\n",
    "                for average_mtd in ['macro', 'weighted']:\n",
    "    #                     dview['txt_field']=txt_field\n",
    "                    dview['classifier']=classifier\n",
    "                    dview['tokenizer']=tokenizer\n",
    "                    dview['vect_type']=vect_type\n",
    "                    dview['average_mtd']=average_mtd\n",
    "                    t=func_naive_bayes.map(range(48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>16</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.697367</td>\n",
       "      <td>0.691182</td>\n",
       "      <td>0.697367</td>\n",
       "      <td>0.686212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>28</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.705033</td>\n",
       "      <td>0.630531</td>\n",
       "      <td>0.536715</td>\n",
       "      <td>0.557126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>7</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>0.674739</td>\n",
       "      <td>0.526701</td>\n",
       "      <td>0.555144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>18</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.703033</td>\n",
       "      <td>0.662888</td>\n",
       "      <td>0.525579</td>\n",
       "      <td>0.552885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>40</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.652567</td>\n",
       "      <td>0.664243</td>\n",
       "      <td>0.652567</td>\n",
       "      <td>0.637895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>22</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.675567</td>\n",
       "      <td>0.642447</td>\n",
       "      <td>0.482073</td>\n",
       "      <td>0.508054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>30</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.561767</td>\n",
       "      <td>0.632718</td>\n",
       "      <td>0.561767</td>\n",
       "      <td>0.532677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>16</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.665079</td>\n",
       "      <td>0.512803</td>\n",
       "      <td>0.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>36</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.655987</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.468865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>9</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.676767</td>\n",
       "      <td>0.630881</td>\n",
       "      <td>0.481489</td>\n",
       "      <td>0.504812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trial      txt_field                                         classifier  \\\n",
       "779     16        mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "1352    28        mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "376      7  mission_prgrm  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "904     18  mission_prgrm  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "1945    40      prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "1082    22      prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "1447    30        mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "814     16  mission_prgrm  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "1763    36  mission_prgrm  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "458      9      prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "\n",
       "             tokenizer vect_type average_mtd  accuracy  precision    recall  \\\n",
       "779    lemma_tokenizer     tfidf    weighted  0.697367   0.691182  0.697367   \n",
       "1352   lemma_tokenizer     count       macro  0.705033   0.630531  0.536715   \n",
       "376    lemma_tokenizer     count       macro  0.708200   0.674739  0.526701   \n",
       "904    lemma_tokenizer     count       macro  0.703033   0.662888  0.525579   \n",
       "1945   lemma_tokenizer     count    weighted  0.652567   0.664243  0.652567   \n",
       "1082   lemma_tokenizer     tfidf       macro  0.675567   0.642447  0.482073   \n",
       "1447  porter_tokenizer     tfidf    weighted  0.561767   0.632718  0.561767   \n",
       "814   porter_tokenizer     tfidf       macro  0.707000   0.665079  0.512803   \n",
       "1763   lemma_tokenizer     tfidf    weighted  0.496400   0.655987  0.496400   \n",
       "458    lemma_tokenizer     tfidf       macro  0.676767   0.630881  0.481489   \n",
       "\n",
       "            f1  \n",
       "779   0.686212  \n",
       "1352  0.557126  \n",
       "376   0.555144  \n",
       "904   0.552885  \n",
       "1945  0.637895  \n",
       "1082  0.508054  \n",
       "1447  0.532677  \n",
       "814   0.539000  \n",
       "1763  0.468865  \n",
       "458   0.504812  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"8\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">mission</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.699215</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.693233</td>\n",
       "      <td>0.697108</td>\n",
       "      <td>0.699450</td>\n",
       "      <td>0.700842</td>\n",
       "      <td>0.705033</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.549106</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.541369</td>\n",
       "      <td>0.546137</td>\n",
       "      <td>0.548990</td>\n",
       "      <td>0.552141</td>\n",
       "      <td>0.557126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.697278</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.692533</td>\n",
       "      <td>0.695600</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.699258</td>\n",
       "      <td>0.702433</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.550577</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>0.540493</td>\n",
       "      <td>0.547210</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>0.554747</td>\n",
       "      <td>0.560542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.695537</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.693758</td>\n",
       "      <td>0.695583</td>\n",
       "      <td>0.697275</td>\n",
       "      <td>0.700300</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.542215</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>0.532686</td>\n",
       "      <td>0.538377</td>\n",
       "      <td>0.542811</td>\n",
       "      <td>0.545817</td>\n",
       "      <td>0.553073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.692885</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.688133</td>\n",
       "      <td>0.691025</td>\n",
       "      <td>0.692950</td>\n",
       "      <td>0.694183</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.545736</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.538274</td>\n",
       "      <td>0.543361</td>\n",
       "      <td>0.545791</td>\n",
       "      <td>0.548439</td>\n",
       "      <td>0.553280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.659657</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.653700</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.659517</td>\n",
       "      <td>0.661217</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.448671</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.442153</td>\n",
       "      <td>0.446622</td>\n",
       "      <td>0.448347</td>\n",
       "      <td>0.450588</td>\n",
       "      <td>0.456381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.558638</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.553233</td>\n",
       "      <td>0.556575</td>\n",
       "      <td>0.558317</td>\n",
       "      <td>0.560433</td>\n",
       "      <td>0.566233</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.330431</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.325534</td>\n",
       "      <td>0.329370</td>\n",
       "      <td>0.330260</td>\n",
       "      <td>0.331961</td>\n",
       "      <td>0.336480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.660235</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.654567</td>\n",
       "      <td>0.658283</td>\n",
       "      <td>0.660283</td>\n",
       "      <td>0.662492</td>\n",
       "      <td>0.663967</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.450428</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.448206</td>\n",
       "      <td>0.450119</td>\n",
       "      <td>0.453286</td>\n",
       "      <td>0.457872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.557409</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.549567</td>\n",
       "      <td>0.555692</td>\n",
       "      <td>0.557233</td>\n",
       "      <td>0.559500</td>\n",
       "      <td>0.565500</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.329525</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.324931</td>\n",
       "      <td>0.327550</td>\n",
       "      <td>0.329791</td>\n",
       "      <td>0.330996</td>\n",
       "      <td>0.335323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">mission_prgrm</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.705483</td>\n",
       "      <td>0.707400</td>\n",
       "      <td>0.711200</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.551936</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.540308</td>\n",
       "      <td>0.549120</td>\n",
       "      <td>0.552409</td>\n",
       "      <td>0.554888</td>\n",
       "      <td>0.562413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.712499</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.707633</td>\n",
       "      <td>0.710617</td>\n",
       "      <td>0.712417</td>\n",
       "      <td>0.714275</td>\n",
       "      <td>0.717267</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.545001</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.534078</td>\n",
       "      <td>0.541805</td>\n",
       "      <td>0.545272</td>\n",
       "      <td>0.548460</td>\n",
       "      <td>0.551154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.700760</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.698842</td>\n",
       "      <td>0.701333</td>\n",
       "      <td>0.702283</td>\n",
       "      <td>0.704867</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.545133</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.536489</td>\n",
       "      <td>0.541949</td>\n",
       "      <td>0.545266</td>\n",
       "      <td>0.548758</td>\n",
       "      <td>0.554124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.709045</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.703967</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.708400</td>\n",
       "      <td>0.711125</td>\n",
       "      <td>0.715467</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.533963</td>\n",
       "      <td>0.538523</td>\n",
       "      <td>0.540764</td>\n",
       "      <td>0.543153</td>\n",
       "      <td>0.550647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.658112</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.653033</td>\n",
       "      <td>0.655650</td>\n",
       "      <td>0.658050</td>\n",
       "      <td>0.660217</td>\n",
       "      <td>0.664933</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.433701</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.423814</td>\n",
       "      <td>0.431687</td>\n",
       "      <td>0.433676</td>\n",
       "      <td>0.436707</td>\n",
       "      <td>0.440243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.499955</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.497017</td>\n",
       "      <td>0.499817</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.271215</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.266073</td>\n",
       "      <td>0.269414</td>\n",
       "      <td>0.270807</td>\n",
       "      <td>0.273052</td>\n",
       "      <td>0.279372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.657779</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.652967</td>\n",
       "      <td>0.655417</td>\n",
       "      <td>0.658083</td>\n",
       "      <td>0.660100</td>\n",
       "      <td>0.663167</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.432929</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.426950</td>\n",
       "      <td>0.430742</td>\n",
       "      <td>0.432629</td>\n",
       "      <td>0.435277</td>\n",
       "      <td>0.438181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.488991</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.480167</td>\n",
       "      <td>0.485908</td>\n",
       "      <td>0.488650</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>0.498733</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.262150</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.257057</td>\n",
       "      <td>0.259958</td>\n",
       "      <td>0.262426</td>\n",
       "      <td>0.263803</td>\n",
       "      <td>0.268357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">prgrm_dsc</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.655181</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.649933</td>\n",
       "      <td>0.652875</td>\n",
       "      <td>0.654533</td>\n",
       "      <td>0.657333</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.499922</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.492773</td>\n",
       "      <td>0.496991</td>\n",
       "      <td>0.500149</td>\n",
       "      <td>0.502033</td>\n",
       "      <td>0.511826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.672953</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.667033</td>\n",
       "      <td>0.670783</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.675017</td>\n",
       "      <td>0.679033</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.504880</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.498290</td>\n",
       "      <td>0.502799</td>\n",
       "      <td>0.505008</td>\n",
       "      <td>0.507092</td>\n",
       "      <td>0.515543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.650536</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>0.648233</td>\n",
       "      <td>0.650917</td>\n",
       "      <td>0.652575</td>\n",
       "      <td>0.655733</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.494407</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>0.485106</td>\n",
       "      <td>0.491006</td>\n",
       "      <td>0.494850</td>\n",
       "      <td>0.497452</td>\n",
       "      <td>0.503668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.669812</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.663867</td>\n",
       "      <td>0.668492</td>\n",
       "      <td>0.669850</td>\n",
       "      <td>0.671483</td>\n",
       "      <td>0.675933</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.501595</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.495570</td>\n",
       "      <td>0.498441</td>\n",
       "      <td>0.501681</td>\n",
       "      <td>0.503771</td>\n",
       "      <td>0.508528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.599706</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.597758</td>\n",
       "      <td>0.598900</td>\n",
       "      <td>0.601775</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.371951</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.363599</td>\n",
       "      <td>0.369612</td>\n",
       "      <td>0.371905</td>\n",
       "      <td>0.374090</td>\n",
       "      <td>0.381560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.472119</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.458133</td>\n",
       "      <td>0.469425</td>\n",
       "      <td>0.472350</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.480133</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.245859</td>\n",
       "      <td>0.250055</td>\n",
       "      <td>0.251484</td>\n",
       "      <td>0.252531</td>\n",
       "      <td>0.261158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.599452</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.594767</td>\n",
       "      <td>0.597750</td>\n",
       "      <td>0.599717</td>\n",
       "      <td>0.601067</td>\n",
       "      <td>0.605600</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.371320</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.365751</td>\n",
       "      <td>0.369749</td>\n",
       "      <td>0.371534</td>\n",
       "      <td>0.372979</td>\n",
       "      <td>0.377309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.461773</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.457867</td>\n",
       "      <td>0.462250</td>\n",
       "      <td>0.466433</td>\n",
       "      <td>0.470933</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.242711</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.237111</td>\n",
       "      <td>0.240438</td>\n",
       "      <td>0.242654</td>\n",
       "      <td>0.244967</td>\n",
       "      <td>0.248375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            accuracy  \\\n",
       "                                                                                               count   \n",
       "txt_field     classifier                                         tokenizer        vect_type            \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                 mean   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.699215   \n",
       "                                                                                  tfidf      0.697278   \n",
       "                                                                 porter_tokenizer count      0.695537   \n",
       "                                                                                  tfidf      0.692885   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.659657   \n",
       "                                                                                  tfidf      0.558638   \n",
       "                                                                 porter_tokenizer count      0.660235   \n",
       "                                                                                  tfidf      0.557409   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.705263   \n",
       "                                                                                  tfidf      0.712499   \n",
       "                                                                 porter_tokenizer count      0.700760   \n",
       "                                                                                  tfidf      0.709045   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.658112   \n",
       "                                                                                  tfidf      0.499955   \n",
       "                                                                 porter_tokenizer count      0.657779   \n",
       "                                                                                  tfidf      0.488991   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.655181   \n",
       "                                                                                  tfidf      0.672953   \n",
       "                                                                 porter_tokenizer count      0.650536   \n",
       "                                                                                  tfidf      0.669812   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.599706   \n",
       "                                                                                  tfidf      0.472119   \n",
       "                                                                 porter_tokenizer count      0.599452   \n",
       "                                                                                  tfidf      0.461773   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  std   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002681   \n",
       "                                                                                  tfidf      0.002618   \n",
       "                                                                 porter_tokenizer count      0.002181   \n",
       "                                                                                  tfidf      0.002552   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002575   \n",
       "                                                                                  tfidf      0.002645   \n",
       "                                                                 porter_tokenizer count      0.002438   \n",
       "                                                                                  tfidf      0.003316   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002722   \n",
       "                                                                                  tfidf      0.002465   \n",
       "                                                                 porter_tokenizer count      0.002371   \n",
       "                                                                                  tfidf      0.002653   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.003063   \n",
       "                                                                                  tfidf      0.004204   \n",
       "                                                                 porter_tokenizer count      0.002695   \n",
       "                                                                                  tfidf      0.004215   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.003299   \n",
       "                                                                                  tfidf      0.002795   \n",
       "                                                                 porter_tokenizer count      0.003258   \n",
       "                                                                                  tfidf      0.002472   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002807   \n",
       "                                                                                  tfidf      0.004488   \n",
       "                                                                 porter_tokenizer count      0.002400   \n",
       "                                                                                  tfidf      0.005277   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  min   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.693233   \n",
       "                                                                                  tfidf      0.692533   \n",
       "                                                                 porter_tokenizer count      0.690900   \n",
       "                                                                                  tfidf      0.688133   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.653700   \n",
       "                                                                                  tfidf      0.553233   \n",
       "                                                                 porter_tokenizer count      0.654567   \n",
       "                                                                                  tfidf      0.549567   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.699000   \n",
       "                                                                                  tfidf      0.707633   \n",
       "                                                                 porter_tokenizer count      0.694800   \n",
       "                                                                                  tfidf      0.703967   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.653033   \n",
       "                                                                                  tfidf      0.490000   \n",
       "                                                                 porter_tokenizer count      0.652967   \n",
       "                                                                                  tfidf      0.480167   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.649933   \n",
       "                                                                                  tfidf      0.667033   \n",
       "                                                                 porter_tokenizer count      0.641500   \n",
       "                                                                                  tfidf      0.663867   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.595000   \n",
       "                                                                                  tfidf      0.458133   \n",
       "                                                                 porter_tokenizer count      0.594767   \n",
       "                                                                                  tfidf      0.450900   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  25%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.697108   \n",
       "                                                                                  tfidf      0.695600   \n",
       "                                                                 porter_tokenizer count      0.693758   \n",
       "                                                                                  tfidf      0.691025   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.657900   \n",
       "                                                                                  tfidf      0.556575   \n",
       "                                                                 porter_tokenizer count      0.658283   \n",
       "                                                                                  tfidf      0.555692   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.703025   \n",
       "                                                                                  tfidf      0.710617   \n",
       "                                                                 porter_tokenizer count      0.698842   \n",
       "                                                                                  tfidf      0.707300   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.655650   \n",
       "                                                                                  tfidf      0.497017   \n",
       "                                                                 porter_tokenizer count      0.655417   \n",
       "                                                                                  tfidf      0.485908   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.652875   \n",
       "                                                                                  tfidf      0.670783   \n",
       "                                                                 porter_tokenizer count      0.648233   \n",
       "                                                                                  tfidf      0.668492   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.597758   \n",
       "                                                                                  tfidf      0.469425   \n",
       "                                                                 porter_tokenizer count      0.597750   \n",
       "                                                                                  tfidf      0.457867   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  50%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.699450   \n",
       "                                                                                  tfidf      0.696800   \n",
       "                                                                 porter_tokenizer count      0.695583   \n",
       "                                                                                  tfidf      0.692950   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.659517   \n",
       "                                                                                  tfidf      0.558317   \n",
       "                                                                 porter_tokenizer count      0.660283   \n",
       "                                                                                  tfidf      0.557233   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.705483   \n",
       "                                                                                  tfidf      0.712417   \n",
       "                                                                 porter_tokenizer count      0.701333   \n",
       "                                                                                  tfidf      0.708400   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.658050   \n",
       "                                                                                  tfidf      0.499817   \n",
       "                                                                 porter_tokenizer count      0.658083   \n",
       "                                                                                  tfidf      0.488650   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.654533   \n",
       "                                                                                  tfidf      0.673100   \n",
       "                                                                 porter_tokenizer count      0.650917   \n",
       "                                                                                  tfidf      0.669850   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.598900   \n",
       "                                                                                  tfidf      0.472350   \n",
       "                                                                 porter_tokenizer count      0.599717   \n",
       "                                                                                  tfidf      0.462250   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  75%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.700842   \n",
       "                                                                                  tfidf      0.699258   \n",
       "                                                                 porter_tokenizer count      0.697275   \n",
       "                                                                                  tfidf      0.694183   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.661217   \n",
       "                                                                                  tfidf      0.560433   \n",
       "                                                                 porter_tokenizer count      0.662492   \n",
       "                                                                                  tfidf      0.559500   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.707400   \n",
       "                                                                                  tfidf      0.714275   \n",
       "                                                                 porter_tokenizer count      0.702283   \n",
       "                                                                                  tfidf      0.711125   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.660217   \n",
       "                                                                                  tfidf      0.502650   \n",
       "                                                                 porter_tokenizer count      0.660100   \n",
       "                                                                                  tfidf      0.491900   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.657333   \n",
       "                                                                                  tfidf      0.675017   \n",
       "                                                                 porter_tokenizer count      0.652575   \n",
       "                                                                                  tfidf      0.671483   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.601775   \n",
       "                                                                                  tfidf      0.475342   \n",
       "                                                                 porter_tokenizer count      0.601067   \n",
       "                                                                                  tfidf      0.466433   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  max   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.705033   \n",
       "                                                                                  tfidf      0.702433   \n",
       "                                                                 porter_tokenizer count      0.700300   \n",
       "                                                                                  tfidf      0.699333   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.666500   \n",
       "                                                                                  tfidf      0.566233   \n",
       "                                                                 porter_tokenizer count      0.663967   \n",
       "                                                                                  tfidf      0.565500   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.711200   \n",
       "                                                                                  tfidf      0.717267   \n",
       "                                                                 porter_tokenizer count      0.704867   \n",
       "                                                                                  tfidf      0.715467   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.664933   \n",
       "                                                                                  tfidf      0.510000   \n",
       "                                                                 porter_tokenizer count      0.663167   \n",
       "                                                                                  tfidf      0.498733   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.663333   \n",
       "                                                                                  tfidf      0.679033   \n",
       "                                                                 porter_tokenizer count      0.655733   \n",
       "                                                                                  tfidf      0.675933   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.607000   \n",
       "                                                                                  tfidf      0.480133   \n",
       "                                                                 porter_tokenizer count      0.605600   \n",
       "                                                                                  tfidf      0.470933   \n",
       "\n",
       "                                                                                               f1  \\\n",
       "                                                                                            count   \n",
       "txt_field     classifier                                         tokenizer        vect_type         \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                 mean   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.549106   \n",
       "                                                                                  tfidf      0.550577   \n",
       "                                                                 porter_tokenizer count      0.542215   \n",
       "                                                                                  tfidf      0.545736   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.448671   \n",
       "                                                                                  tfidf      0.330431   \n",
       "                                                                 porter_tokenizer count      0.450428   \n",
       "                                                                                  tfidf      0.329525   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.551936   \n",
       "                                                                                  tfidf      0.545001   \n",
       "                                                                 porter_tokenizer count      0.545133   \n",
       "                                                                                  tfidf      0.540788   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.433701   \n",
       "                                                                                  tfidf      0.271215   \n",
       "                                                                 porter_tokenizer count      0.432929   \n",
       "                                                                                  tfidf      0.262150   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.499922   \n",
       "                                                                                  tfidf      0.504880   \n",
       "                                                                 porter_tokenizer count      0.494407   \n",
       "                                                                                  tfidf      0.501595   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.371951   \n",
       "                                                                                  tfidf      0.251600   \n",
       "                                                                 porter_tokenizer count      0.371320   \n",
       "                                                                                  tfidf      0.242711   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  std   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.003985   \n",
       "                                                                                  tfidf      0.004786   \n",
       "                                                                 porter_tokenizer count      0.004703   \n",
       "                                                                                  tfidf      0.003444   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.003129   \n",
       "                                                                                  tfidf      0.002431   \n",
       "                                                                 porter_tokenizer count      0.003606   \n",
       "                                                                                  tfidf      0.002330   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.004547   \n",
       "                                                                                  tfidf      0.004446   \n",
       "                                                                 porter_tokenizer count      0.004174   \n",
       "                                                                                  tfidf      0.003785   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.003436   \n",
       "                                                                                  tfidf      0.002744   \n",
       "                                                                 porter_tokenizer count      0.003005   \n",
       "                                                                                  tfidf      0.002563   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.004508   \n",
       "                                                                                  tfidf      0.003324   \n",
       "                                                                 porter_tokenizer count      0.004578   \n",
       "                                                                                  tfidf      0.003363   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.003794   \n",
       "                                                                                  tfidf      0.002791   \n",
       "                                                                 porter_tokenizer count      0.002668   \n",
       "                                                                                  tfidf      0.002976   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  min   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.541369   \n",
       "                                                                                  tfidf      0.540493   \n",
       "                                                                 porter_tokenizer count      0.532686   \n",
       "                                                                                  tfidf      0.538274   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.442153   \n",
       "                                                                                  tfidf      0.325534   \n",
       "                                                                 porter_tokenizer count      0.442857   \n",
       "                                                                                  tfidf      0.324931   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.540308   \n",
       "                                                                                  tfidf      0.534078   \n",
       "                                                                 porter_tokenizer count      0.536489   \n",
       "                                                                                  tfidf      0.533963   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.423814   \n",
       "                                                                                  tfidf      0.266073   \n",
       "                                                                 porter_tokenizer count      0.426950   \n",
       "                                                                                  tfidf      0.257057   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.492773   \n",
       "                                                                                  tfidf      0.498290   \n",
       "                                                                 porter_tokenizer count      0.485106   \n",
       "                                                                                  tfidf      0.495570   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.363599   \n",
       "                                                                                  tfidf      0.245859   \n",
       "                                                                 porter_tokenizer count      0.365751   \n",
       "                                                                                  tfidf      0.237111   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  25%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.546137   \n",
       "                                                                                  tfidf      0.547210   \n",
       "                                                                 porter_tokenizer count      0.538377   \n",
       "                                                                                  tfidf      0.543361   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.446622   \n",
       "                                                                                  tfidf      0.329370   \n",
       "                                                                 porter_tokenizer count      0.448206   \n",
       "                                                                                  tfidf      0.327550   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.549120   \n",
       "                                                                                  tfidf      0.541805   \n",
       "                                                                 porter_tokenizer count      0.541949   \n",
       "                                                                                  tfidf      0.538523   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.431687   \n",
       "                                                                                  tfidf      0.269414   \n",
       "                                                                 porter_tokenizer count      0.430742   \n",
       "                                                                                  tfidf      0.259958   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.496991   \n",
       "                                                                                  tfidf      0.502799   \n",
       "                                                                 porter_tokenizer count      0.491006   \n",
       "                                                                                  tfidf      0.498441   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.369612   \n",
       "                                                                                  tfidf      0.250055   \n",
       "                                                                 porter_tokenizer count      0.369749   \n",
       "                                                                                  tfidf      0.240438   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  50%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.548990   \n",
       "                                                                                  tfidf      0.549587   \n",
       "                                                                 porter_tokenizer count      0.542811   \n",
       "                                                                                  tfidf      0.545791   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.448347   \n",
       "                                                                                  tfidf      0.330260   \n",
       "                                                                 porter_tokenizer count      0.450119   \n",
       "                                                                                  tfidf      0.329791   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.552409   \n",
       "                                                                                  tfidf      0.545272   \n",
       "                                                                 porter_tokenizer count      0.545266   \n",
       "                                                                                  tfidf      0.540764   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.433676   \n",
       "                                                                                  tfidf      0.270807   \n",
       "                                                                 porter_tokenizer count      0.432629   \n",
       "                                                                                  tfidf      0.262426   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.500149   \n",
       "                                                                                  tfidf      0.505008   \n",
       "                                                                 porter_tokenizer count      0.494850   \n",
       "                                                                                  tfidf      0.501681   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.371905   \n",
       "                                                                                  tfidf      0.251484   \n",
       "                                                                 porter_tokenizer count      0.371534   \n",
       "                                                                                  tfidf      0.242654   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  75%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.552141   \n",
       "                                                                                  tfidf      0.554747   \n",
       "                                                                 porter_tokenizer count      0.545817   \n",
       "                                                                                  tfidf      0.548439   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.450588   \n",
       "                                                                                  tfidf      0.331961   \n",
       "                                                                 porter_tokenizer count      0.453286   \n",
       "                                                                                  tfidf      0.330996   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.554888   \n",
       "                                                                                  tfidf      0.548460   \n",
       "                                                                 porter_tokenizer count      0.548758   \n",
       "                                                                                  tfidf      0.543153   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.436707   \n",
       "                                                                                  tfidf      0.273052   \n",
       "                                                                 porter_tokenizer count      0.435277   \n",
       "                                                                                  tfidf      0.263803   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.502033   \n",
       "                                                                                  tfidf      0.507092   \n",
       "                                                                 porter_tokenizer count      0.497452   \n",
       "                                                                                  tfidf      0.503771   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.374090   \n",
       "                                                                                  tfidf      0.252531   \n",
       "                                                                 porter_tokenizer count      0.372979   \n",
       "                                                                                  tfidf      0.244967   \n",
       "\n",
       "                                                                                                       \n",
       "                                                                                                  max  \n",
       "txt_field     classifier                                         tokenizer        vect_type            \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.557126  \n",
       "                                                                                  tfidf      0.560542  \n",
       "                                                                 porter_tokenizer count      0.553073  \n",
       "                                                                                  tfidf      0.553280  \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.456381  \n",
       "                                                                                  tfidf      0.336480  \n",
       "                                                                 porter_tokenizer count      0.457872  \n",
       "                                                                                  tfidf      0.335323  \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.562413  \n",
       "                                                                                  tfidf      0.551154  \n",
       "                                                                 porter_tokenizer count      0.554124  \n",
       "                                                                                  tfidf      0.550647  \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.440243  \n",
       "                                                                                  tfidf      0.279372  \n",
       "                                                                 porter_tokenizer count      0.438181  \n",
       "                                                                                  tfidf      0.268357  \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.511826  \n",
       "                                                                                  tfidf      0.515543  \n",
       "                                                                 porter_tokenizer count      0.503668  \n",
       "                                                                                  tfidf      0.508528  \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.381560  \n",
       "                                                                                  tfidf      0.261158  \n",
       "                                                                 porter_tokenizer count      0.377309  \n",
       "                                                                                  tfidf      0.248375  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance[df_performance.average_mtd=='macro'].groupby(['txt_field', 'classifier', 'tokenizer', 'vect_type']).describe()[['accuracy','f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt_field in ['mission', 'prgrm_dsc', 'mission_prgrm']:\n",
    "    for classifier in [ensemble.RandomForestClassifier()]:\n",
    "        for tokenizer in ['lemma', 'porter']:\n",
    "            for vect_type in ['count', 'tfidf']:\n",
    "                for average_mtd in ['macro', 'weighted']:\n",
    "                    dview['txt_field']=txt_field\n",
    "                    dview['classifier']=classifier\n",
    "                    dview['tokenizer']=tokenizer\n",
    "                    dview['vect_type']=vect_type\n",
    "                    dview['average_mtd']=average_mtd\n",
    "                    t=func_naive_bayes.map(range(48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>28</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.587900</td>\n",
       "      <td>0.593079</td>\n",
       "      <td>0.587900</td>\n",
       "      <td>0.567848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>45</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.468333</td>\n",
       "      <td>0.618234</td>\n",
       "      <td>0.468333</td>\n",
       "      <td>0.439715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>14</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.498467</td>\n",
       "      <td>0.467002</td>\n",
       "      <td>0.242660</td>\n",
       "      <td>0.271207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.661833</td>\n",
       "      <td>0.638844</td>\n",
       "      <td>0.427347</td>\n",
       "      <td>0.453889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>35</td>\n",
       "      <td>mission</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.644700</td>\n",
       "      <td>0.610814</td>\n",
       "      <td>0.470108</td>\n",
       "      <td>0.502467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>26</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.620300</td>\n",
       "      <td>0.627897</td>\n",
       "      <td>0.620300</td>\n",
       "      <td>0.600389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>38</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.665867</td>\n",
       "      <td>0.615089</td>\n",
       "      <td>0.471902</td>\n",
       "      <td>0.497325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>38</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.692533</td>\n",
       "      <td>0.682893</td>\n",
       "      <td>0.692533</td>\n",
       "      <td>0.680818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>33</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.659600</td>\n",
       "      <td>0.637621</td>\n",
       "      <td>0.423779</td>\n",
       "      <td>0.450652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3</td>\n",
       "      <td>mission</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.645533</td>\n",
       "      <td>0.640535</td>\n",
       "      <td>0.645533</td>\n",
       "      <td>0.630355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trial      txt_field                                         classifier  \\\n",
       "2079    28      prgrm_dsc  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "3259    45      prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "1042    14  mission_prgrm  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "220      3        mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "2570    35        mission  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "1943    26  mission_prgrm  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "2766    38      prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "2751    38        mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "2380    33        mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "271      3        mission  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "\n",
       "             tokenizer vect_type average_mtd  accuracy  precision    recall  \\\n",
       "2079  porter_tokenizer     tfidf    weighted  0.587900   0.593079  0.587900   \n",
       "3259   lemma_tokenizer     tfidf    weighted  0.468333   0.618234  0.468333   \n",
       "1042   lemma_tokenizer     tfidf       macro  0.498467   0.467002  0.242660   \n",
       "220   porter_tokenizer     count       macro  0.661833   0.638844  0.427347   \n",
       "2570   lemma_tokenizer     tfidf       macro  0.644700   0.610814  0.470108   \n",
       "1943  porter_tokenizer     tfidf    weighted  0.620300   0.627897  0.620300   \n",
       "2766  porter_tokenizer     tfidf       macro  0.665867   0.615089  0.471902   \n",
       "2751  porter_tokenizer     tfidf    weighted  0.692533   0.682893  0.692533   \n",
       "2380  porter_tokenizer     count       macro  0.659600   0.637621  0.423779   \n",
       "271   porter_tokenizer     tfidf    weighted  0.645533   0.640535  0.645533   \n",
       "\n",
       "            f1  \n",
       "2079  0.567848  \n",
       "3259  0.439715  \n",
       "1042  0.271207  \n",
       "220   0.453889  \n",
       "2570  0.502467  \n",
       "1943  0.600389  \n",
       "2766  0.497325  \n",
       "2751  0.680818  \n",
       "2380  0.450652  \n",
       "271   0.630355  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"8\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">mission</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.699215</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.693233</td>\n",
       "      <td>0.697108</td>\n",
       "      <td>0.699450</td>\n",
       "      <td>0.700842</td>\n",
       "      <td>0.705033</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.549106</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.541369</td>\n",
       "      <td>0.546137</td>\n",
       "      <td>0.548990</td>\n",
       "      <td>0.552141</td>\n",
       "      <td>0.557126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.697278</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.692533</td>\n",
       "      <td>0.695600</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.699258</td>\n",
       "      <td>0.702433</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.550577</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>0.540493</td>\n",
       "      <td>0.547210</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>0.554747</td>\n",
       "      <td>0.560542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.695537</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.693758</td>\n",
       "      <td>0.695583</td>\n",
       "      <td>0.697275</td>\n",
       "      <td>0.700300</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.542215</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>0.532686</td>\n",
       "      <td>0.538377</td>\n",
       "      <td>0.542811</td>\n",
       "      <td>0.545817</td>\n",
       "      <td>0.553073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.692885</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.688133</td>\n",
       "      <td>0.691025</td>\n",
       "      <td>0.692950</td>\n",
       "      <td>0.694183</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.545736</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.538274</td>\n",
       "      <td>0.543361</td>\n",
       "      <td>0.545791</td>\n",
       "      <td>0.548439</td>\n",
       "      <td>0.553280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.659657</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.653700</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.659517</td>\n",
       "      <td>0.661217</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.448671</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.442153</td>\n",
       "      <td>0.446622</td>\n",
       "      <td>0.448347</td>\n",
       "      <td>0.450588</td>\n",
       "      <td>0.456381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.558638</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.553233</td>\n",
       "      <td>0.556575</td>\n",
       "      <td>0.558317</td>\n",
       "      <td>0.560433</td>\n",
       "      <td>0.566233</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.330431</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.325534</td>\n",
       "      <td>0.329370</td>\n",
       "      <td>0.330260</td>\n",
       "      <td>0.331961</td>\n",
       "      <td>0.336480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.660235</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.654567</td>\n",
       "      <td>0.658283</td>\n",
       "      <td>0.660283</td>\n",
       "      <td>0.662492</td>\n",
       "      <td>0.663967</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.450428</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.448206</td>\n",
       "      <td>0.450119</td>\n",
       "      <td>0.453286</td>\n",
       "      <td>0.457872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.557409</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.549567</td>\n",
       "      <td>0.555692</td>\n",
       "      <td>0.557233</td>\n",
       "      <td>0.559500</td>\n",
       "      <td>0.565500</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.329525</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.324931</td>\n",
       "      <td>0.327550</td>\n",
       "      <td>0.329791</td>\n",
       "      <td>0.330996</td>\n",
       "      <td>0.335323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\\n            oob_score=False, random_state=None, verbose=0,\\n            warm_start=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.640355</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.632967</td>\n",
       "      <td>0.637517</td>\n",
       "      <td>0.640950</td>\n",
       "      <td>0.642383</td>\n",
       "      <td>0.646700</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.499362</td>\n",
       "      <td>0.005806</td>\n",
       "      <td>0.489057</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.499688</td>\n",
       "      <td>0.503812</td>\n",
       "      <td>0.510094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.643485</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.641025</td>\n",
       "      <td>0.644267</td>\n",
       "      <td>0.646342</td>\n",
       "      <td>0.650667</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.502417</td>\n",
       "      <td>0.006553</td>\n",
       "      <td>0.492290</td>\n",
       "      <td>0.497264</td>\n",
       "      <td>0.502105</td>\n",
       "      <td>0.507314</td>\n",
       "      <td>0.518759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.642662</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.634933</td>\n",
       "      <td>0.640717</td>\n",
       "      <td>0.642050</td>\n",
       "      <td>0.643925</td>\n",
       "      <td>0.653067</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.503231</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>0.491848</td>\n",
       "      <td>0.500426</td>\n",
       "      <td>0.503675</td>\n",
       "      <td>0.505869</td>\n",
       "      <td>0.517862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.634233</td>\n",
       "      <td>0.642367</td>\n",
       "      <td>0.645483</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.505267</td>\n",
       "      <td>0.006155</td>\n",
       "      <td>0.490982</td>\n",
       "      <td>0.501541</td>\n",
       "      <td>0.504355</td>\n",
       "      <td>0.508536</td>\n",
       "      <td>0.520476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">mission_prgrm</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.703025</td>\n",
       "      <td>0.705483</td>\n",
       "      <td>0.707400</td>\n",
       "      <td>0.711200</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.551936</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.540308</td>\n",
       "      <td>0.549120</td>\n",
       "      <td>0.552409</td>\n",
       "      <td>0.554888</td>\n",
       "      <td>0.562413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.712499</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.707633</td>\n",
       "      <td>0.710617</td>\n",
       "      <td>0.712417</td>\n",
       "      <td>0.714275</td>\n",
       "      <td>0.717267</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.545001</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.534078</td>\n",
       "      <td>0.541805</td>\n",
       "      <td>0.545272</td>\n",
       "      <td>0.548460</td>\n",
       "      <td>0.551154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.700760</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.698842</td>\n",
       "      <td>0.701333</td>\n",
       "      <td>0.702283</td>\n",
       "      <td>0.704867</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.545133</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.536489</td>\n",
       "      <td>0.541949</td>\n",
       "      <td>0.545266</td>\n",
       "      <td>0.548758</td>\n",
       "      <td>0.554124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.709045</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.703967</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.708400</td>\n",
       "      <td>0.711125</td>\n",
       "      <td>0.715467</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.533963</td>\n",
       "      <td>0.538523</td>\n",
       "      <td>0.540764</td>\n",
       "      <td>0.543153</td>\n",
       "      <td>0.550647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.658112</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.653033</td>\n",
       "      <td>0.655650</td>\n",
       "      <td>0.658050</td>\n",
       "      <td>0.660217</td>\n",
       "      <td>0.664933</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.433701</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.423814</td>\n",
       "      <td>0.431687</td>\n",
       "      <td>0.433676</td>\n",
       "      <td>0.436707</td>\n",
       "      <td>0.440243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.499955</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.497017</td>\n",
       "      <td>0.499817</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.271215</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.266073</td>\n",
       "      <td>0.269414</td>\n",
       "      <td>0.270807</td>\n",
       "      <td>0.273052</td>\n",
       "      <td>0.279372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.657779</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.652967</td>\n",
       "      <td>0.655417</td>\n",
       "      <td>0.658083</td>\n",
       "      <td>0.660100</td>\n",
       "      <td>0.663167</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.432929</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.426950</td>\n",
       "      <td>0.430742</td>\n",
       "      <td>0.432629</td>\n",
       "      <td>0.435277</td>\n",
       "      <td>0.438181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.488991</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.480167</td>\n",
       "      <td>0.485908</td>\n",
       "      <td>0.488650</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>0.498733</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.262150</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.257057</td>\n",
       "      <td>0.259958</td>\n",
       "      <td>0.262426</td>\n",
       "      <td>0.263803</td>\n",
       "      <td>0.268357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\\n            oob_score=False, random_state=None, verbose=0,\\n            warm_start=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.615981</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.607267</td>\n",
       "      <td>0.612792</td>\n",
       "      <td>0.615983</td>\n",
       "      <td>0.618883</td>\n",
       "      <td>0.625800</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.453727</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.444073</td>\n",
       "      <td>0.450424</td>\n",
       "      <td>0.454316</td>\n",
       "      <td>0.457178</td>\n",
       "      <td>0.463289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.622331</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.609367</td>\n",
       "      <td>0.619692</td>\n",
       "      <td>0.621967</td>\n",
       "      <td>0.625450</td>\n",
       "      <td>0.630700</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.460062</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.448287</td>\n",
       "      <td>0.456206</td>\n",
       "      <td>0.459959</td>\n",
       "      <td>0.464170</td>\n",
       "      <td>0.471572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.612688</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.604833</td>\n",
       "      <td>0.610883</td>\n",
       "      <td>0.612367</td>\n",
       "      <td>0.615708</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.451950</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.435236</td>\n",
       "      <td>0.449188</td>\n",
       "      <td>0.452470</td>\n",
       "      <td>0.455692</td>\n",
       "      <td>0.461533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.618903</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.610467</td>\n",
       "      <td>0.616750</td>\n",
       "      <td>0.619133</td>\n",
       "      <td>0.621242</td>\n",
       "      <td>0.626900</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.458185</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.448544</td>\n",
       "      <td>0.455987</td>\n",
       "      <td>0.458283</td>\n",
       "      <td>0.460990</td>\n",
       "      <td>0.468428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">prgrm_dsc</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.655181</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.649933</td>\n",
       "      <td>0.652875</td>\n",
       "      <td>0.654533</td>\n",
       "      <td>0.657333</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.499922</td>\n",
       "      <td>0.004508</td>\n",
       "      <td>0.492773</td>\n",
       "      <td>0.496991</td>\n",
       "      <td>0.500149</td>\n",
       "      <td>0.502033</td>\n",
       "      <td>0.511826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.672953</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.667033</td>\n",
       "      <td>0.670783</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.675017</td>\n",
       "      <td>0.679033</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.504880</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.498290</td>\n",
       "      <td>0.502799</td>\n",
       "      <td>0.505008</td>\n",
       "      <td>0.507092</td>\n",
       "      <td>0.515543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.650536</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>0.648233</td>\n",
       "      <td>0.650917</td>\n",
       "      <td>0.652575</td>\n",
       "      <td>0.655733</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.494407</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>0.485106</td>\n",
       "      <td>0.491006</td>\n",
       "      <td>0.494850</td>\n",
       "      <td>0.497452</td>\n",
       "      <td>0.503668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.669812</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.663867</td>\n",
       "      <td>0.668492</td>\n",
       "      <td>0.669850</td>\n",
       "      <td>0.671483</td>\n",
       "      <td>0.675933</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.501595</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.495570</td>\n",
       "      <td>0.498441</td>\n",
       "      <td>0.501681</td>\n",
       "      <td>0.503771</td>\n",
       "      <td>0.508528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.599706</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.597758</td>\n",
       "      <td>0.598900</td>\n",
       "      <td>0.601775</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.371951</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.363599</td>\n",
       "      <td>0.369612</td>\n",
       "      <td>0.371905</td>\n",
       "      <td>0.374090</td>\n",
       "      <td>0.381560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.472119</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.458133</td>\n",
       "      <td>0.469425</td>\n",
       "      <td>0.472350</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.480133</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.245859</td>\n",
       "      <td>0.250055</td>\n",
       "      <td>0.251484</td>\n",
       "      <td>0.252531</td>\n",
       "      <td>0.261158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.599452</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.594767</td>\n",
       "      <td>0.597750</td>\n",
       "      <td>0.599717</td>\n",
       "      <td>0.601067</td>\n",
       "      <td>0.605600</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.371320</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.365751</td>\n",
       "      <td>0.369749</td>\n",
       "      <td>0.371534</td>\n",
       "      <td>0.372979</td>\n",
       "      <td>0.377309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.461773</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.457867</td>\n",
       "      <td>0.462250</td>\n",
       "      <td>0.466433</td>\n",
       "      <td>0.470933</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.242711</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.237111</td>\n",
       "      <td>0.240438</td>\n",
       "      <td>0.242654</td>\n",
       "      <td>0.244967</td>\n",
       "      <td>0.248375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\\n            oob_score=False, random_state=None, verbose=0,\\n            warm_start=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.584691</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.575433</td>\n",
       "      <td>0.581358</td>\n",
       "      <td>0.584817</td>\n",
       "      <td>0.587908</td>\n",
       "      <td>0.593300</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.426802</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.410047</td>\n",
       "      <td>0.422750</td>\n",
       "      <td>0.426096</td>\n",
       "      <td>0.431173</td>\n",
       "      <td>0.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.592474</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>0.584667</td>\n",
       "      <td>0.590650</td>\n",
       "      <td>0.593217</td>\n",
       "      <td>0.594458</td>\n",
       "      <td>0.599233</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.434292</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.418811</td>\n",
       "      <td>0.432087</td>\n",
       "      <td>0.434852</td>\n",
       "      <td>0.436804</td>\n",
       "      <td>0.446010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.583114</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.570867</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>0.586208</td>\n",
       "      <td>0.595867</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.426330</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>0.413097</td>\n",
       "      <td>0.422877</td>\n",
       "      <td>0.426232</td>\n",
       "      <td>0.430057</td>\n",
       "      <td>0.436586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.589610</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.580700</td>\n",
       "      <td>0.586808</td>\n",
       "      <td>0.589267</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.601467</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.431415</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>0.417076</td>\n",
       "      <td>0.428068</td>\n",
       "      <td>0.430413</td>\n",
       "      <td>0.435522</td>\n",
       "      <td>0.442634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            accuracy  \\\n",
       "                                                                                               count   \n",
       "txt_field     classifier                                         tokenizer        vect_type            \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "                                                                 porter_tokenizer count         48.0   \n",
       "                                                                                  tfidf         48.0   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                 mean   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.699215   \n",
       "                                                                                  tfidf      0.697278   \n",
       "                                                                 porter_tokenizer count      0.695537   \n",
       "                                                                                  tfidf      0.692885   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.659657   \n",
       "                                                                                  tfidf      0.558638   \n",
       "                                                                 porter_tokenizer count      0.660235   \n",
       "                                                                                  tfidf      0.557409   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.640355   \n",
       "                                                                                  tfidf      0.643485   \n",
       "                                                                 porter_tokenizer count      0.642662   \n",
       "                                                                                  tfidf      0.645100   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.705263   \n",
       "                                                                                  tfidf      0.712499   \n",
       "                                                                 porter_tokenizer count      0.700760   \n",
       "                                                                                  tfidf      0.709045   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.658112   \n",
       "                                                                                  tfidf      0.499955   \n",
       "                                                                 porter_tokenizer count      0.657779   \n",
       "                                                                                  tfidf      0.488991   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.615981   \n",
       "                                                                                  tfidf      0.622331   \n",
       "                                                                 porter_tokenizer count      0.612688   \n",
       "                                                                                  tfidf      0.618903   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.655181   \n",
       "                                                                                  tfidf      0.672953   \n",
       "                                                                 porter_tokenizer count      0.650536   \n",
       "                                                                                  tfidf      0.669812   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.599706   \n",
       "                                                                                  tfidf      0.472119   \n",
       "                                                                 porter_tokenizer count      0.599452   \n",
       "                                                                                  tfidf      0.461773   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.584691   \n",
       "                                                                                  tfidf      0.592474   \n",
       "                                                                 porter_tokenizer count      0.583114   \n",
       "                                                                                  tfidf      0.589610   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  std   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002681   \n",
       "                                                                                  tfidf      0.002618   \n",
       "                                                                 porter_tokenizer count      0.002181   \n",
       "                                                                                  tfidf      0.002552   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002575   \n",
       "                                                                                  tfidf      0.002645   \n",
       "                                                                 porter_tokenizer count      0.002438   \n",
       "                                                                                  tfidf      0.003316   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.003555   \n",
       "                                                                                  tfidf      0.003804   \n",
       "                                                                 porter_tokenizer count      0.003229   \n",
       "                                                                                  tfidf      0.004148   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002722   \n",
       "                                                                                  tfidf      0.002465   \n",
       "                                                                 porter_tokenizer count      0.002371   \n",
       "                                                                                  tfidf      0.002653   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.003063   \n",
       "                                                                                  tfidf      0.004204   \n",
       "                                                                 porter_tokenizer count      0.002695   \n",
       "                                                                                  tfidf      0.004215   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.004411   \n",
       "                                                                                  tfidf      0.004307   \n",
       "                                                                 porter_tokenizer count      0.003729   \n",
       "                                                                                  tfidf      0.003161   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.003299   \n",
       "                                                                                  tfidf      0.002795   \n",
       "                                                                 porter_tokenizer count      0.003258   \n",
       "                                                                                  tfidf      0.002472   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002807   \n",
       "                                                                                  tfidf      0.004488   \n",
       "                                                                 porter_tokenizer count      0.002400   \n",
       "                                                                                  tfidf      0.005277   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.004374   \n",
       "                                                                                  tfidf      0.003216   \n",
       "                                                                 porter_tokenizer count      0.004181   \n",
       "                                                                                  tfidf      0.004371   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  min   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.693233   \n",
       "                                                                                  tfidf      0.692533   \n",
       "                                                                 porter_tokenizer count      0.690900   \n",
       "                                                                                  tfidf      0.688133   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.653700   \n",
       "                                                                                  tfidf      0.553233   \n",
       "                                                                 porter_tokenizer count      0.654567   \n",
       "                                                                                  tfidf      0.549567   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.632967   \n",
       "                                                                                  tfidf      0.635033   \n",
       "                                                                 porter_tokenizer count      0.634933   \n",
       "                                                                                  tfidf      0.634233   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.699000   \n",
       "                                                                                  tfidf      0.707633   \n",
       "                                                                 porter_tokenizer count      0.694800   \n",
       "                                                                                  tfidf      0.703967   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.653033   \n",
       "                                                                                  tfidf      0.490000   \n",
       "                                                                 porter_tokenizer count      0.652967   \n",
       "                                                                                  tfidf      0.480167   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.607267   \n",
       "                                                                                  tfidf      0.609367   \n",
       "                                                                 porter_tokenizer count      0.604833   \n",
       "                                                                                  tfidf      0.610467   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.649933   \n",
       "                                                                                  tfidf      0.667033   \n",
       "                                                                 porter_tokenizer count      0.641500   \n",
       "                                                                                  tfidf      0.663867   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.595000   \n",
       "                                                                                  tfidf      0.458133   \n",
       "                                                                 porter_tokenizer count      0.594767   \n",
       "                                                                                  tfidf      0.450900   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.575433   \n",
       "                                                                                  tfidf      0.584667   \n",
       "                                                                 porter_tokenizer count      0.570867   \n",
       "                                                                                  tfidf      0.580700   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  25%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.697108   \n",
       "                                                                                  tfidf      0.695600   \n",
       "                                                                 porter_tokenizer count      0.693758   \n",
       "                                                                                  tfidf      0.691025   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.657900   \n",
       "                                                                                  tfidf      0.556575   \n",
       "                                                                 porter_tokenizer count      0.658283   \n",
       "                                                                                  tfidf      0.555692   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.637517   \n",
       "                                                                                  tfidf      0.641025   \n",
       "                                                                 porter_tokenizer count      0.640717   \n",
       "                                                                                  tfidf      0.642367   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.703025   \n",
       "                                                                                  tfidf      0.710617   \n",
       "                                                                 porter_tokenizer count      0.698842   \n",
       "                                                                                  tfidf      0.707300   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.655650   \n",
       "                                                                                  tfidf      0.497017   \n",
       "                                                                 porter_tokenizer count      0.655417   \n",
       "                                                                                  tfidf      0.485908   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.612792   \n",
       "                                                                                  tfidf      0.619692   \n",
       "                                                                 porter_tokenizer count      0.610883   \n",
       "                                                                                  tfidf      0.616750   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.652875   \n",
       "                                                                                  tfidf      0.670783   \n",
       "                                                                 porter_tokenizer count      0.648233   \n",
       "                                                                                  tfidf      0.668492   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.597758   \n",
       "                                                                                  tfidf      0.469425   \n",
       "                                                                 porter_tokenizer count      0.597750   \n",
       "                                                                                  tfidf      0.457867   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.581358   \n",
       "                                                                                  tfidf      0.590650   \n",
       "                                                                 porter_tokenizer count      0.580600   \n",
       "                                                                                  tfidf      0.586808   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  50%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.699450   \n",
       "                                                                                  tfidf      0.696800   \n",
       "                                                                 porter_tokenizer count      0.695583   \n",
       "                                                                                  tfidf      0.692950   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.659517   \n",
       "                                                                                  tfidf      0.558317   \n",
       "                                                                 porter_tokenizer count      0.660283   \n",
       "                                                                                  tfidf      0.557233   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.640950   \n",
       "                                                                                  tfidf      0.644267   \n",
       "                                                                 porter_tokenizer count      0.642050   \n",
       "                                                                                  tfidf      0.645483   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.705483   \n",
       "                                                                                  tfidf      0.712417   \n",
       "                                                                 porter_tokenizer count      0.701333   \n",
       "                                                                                  tfidf      0.708400   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.658050   \n",
       "                                                                                  tfidf      0.499817   \n",
       "                                                                 porter_tokenizer count      0.658083   \n",
       "                                                                                  tfidf      0.488650   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.615983   \n",
       "                                                                                  tfidf      0.621967   \n",
       "                                                                 porter_tokenizer count      0.612367   \n",
       "                                                                                  tfidf      0.619133   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.654533   \n",
       "                                                                                  tfidf      0.673100   \n",
       "                                                                 porter_tokenizer count      0.650917   \n",
       "                                                                                  tfidf      0.669850   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.598900   \n",
       "                                                                                  tfidf      0.472350   \n",
       "                                                                 porter_tokenizer count      0.599717   \n",
       "                                                                                  tfidf      0.462250   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.584817   \n",
       "                                                                                  tfidf      0.593217   \n",
       "                                                                 porter_tokenizer count      0.583500   \n",
       "                                                                                  tfidf      0.589267   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  75%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.700842   \n",
       "                                                                                  tfidf      0.699258   \n",
       "                                                                 porter_tokenizer count      0.697275   \n",
       "                                                                                  tfidf      0.694183   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.661217   \n",
       "                                                                                  tfidf      0.560433   \n",
       "                                                                 porter_tokenizer count      0.662492   \n",
       "                                                                                  tfidf      0.559500   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.642383   \n",
       "                                                                                  tfidf      0.646342   \n",
       "                                                                 porter_tokenizer count      0.643925   \n",
       "                                                                                  tfidf      0.647750   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.707400   \n",
       "                                                                                  tfidf      0.714275   \n",
       "                                                                 porter_tokenizer count      0.702283   \n",
       "                                                                                  tfidf      0.711125   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.660217   \n",
       "                                                                                  tfidf      0.502650   \n",
       "                                                                 porter_tokenizer count      0.660100   \n",
       "                                                                                  tfidf      0.491900   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.618883   \n",
       "                                                                                  tfidf      0.625450   \n",
       "                                                                 porter_tokenizer count      0.615708   \n",
       "                                                                                  tfidf      0.621242   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.657333   \n",
       "                                                                                  tfidf      0.675017   \n",
       "                                                                 porter_tokenizer count      0.652575   \n",
       "                                                                                  tfidf      0.671483   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.601775   \n",
       "                                                                                  tfidf      0.475342   \n",
       "                                                                 porter_tokenizer count      0.601067   \n",
       "                                                                                  tfidf      0.466433   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.587908   \n",
       "                                                                                  tfidf      0.594458   \n",
       "                                                                 porter_tokenizer count      0.586208   \n",
       "                                                                                  tfidf      0.591700   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  max   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.705033   \n",
       "                                                                                  tfidf      0.702433   \n",
       "                                                                 porter_tokenizer count      0.700300   \n",
       "                                                                                  tfidf      0.699333   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.666500   \n",
       "                                                                                  tfidf      0.566233   \n",
       "                                                                 porter_tokenizer count      0.663967   \n",
       "                                                                                  tfidf      0.565500   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.646700   \n",
       "                                                                                  tfidf      0.650667   \n",
       "                                                                 porter_tokenizer count      0.653067   \n",
       "                                                                                  tfidf      0.654167   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.711200   \n",
       "                                                                                  tfidf      0.717267   \n",
       "                                                                 porter_tokenizer count      0.704867   \n",
       "                                                                                  tfidf      0.715467   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.664933   \n",
       "                                                                                  tfidf      0.510000   \n",
       "                                                                 porter_tokenizer count      0.663167   \n",
       "                                                                                  tfidf      0.498733   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.625800   \n",
       "                                                                                  tfidf      0.630700   \n",
       "                                                                 porter_tokenizer count      0.618700   \n",
       "                                                                                  tfidf      0.626900   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.663333   \n",
       "                                                                                  tfidf      0.679033   \n",
       "                                                                 porter_tokenizer count      0.655733   \n",
       "                                                                                  tfidf      0.675933   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.607000   \n",
       "                                                                                  tfidf      0.480133   \n",
       "                                                                 porter_tokenizer count      0.605600   \n",
       "                                                                                  tfidf      0.470933   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.593300   \n",
       "                                                                                  tfidf      0.599233   \n",
       "                                                                 porter_tokenizer count      0.595867   \n",
       "                                                                                  tfidf      0.601467   \n",
       "\n",
       "                                                                                               f1  \\\n",
       "                                                                                            count   \n",
       "txt_field     classifier                                         tokenizer        vect_type         \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "                                                                 porter_tokenizer count      48.0   \n",
       "                                                                                  tfidf      48.0   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                 mean   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.549106   \n",
       "                                                                                  tfidf      0.550577   \n",
       "                                                                 porter_tokenizer count      0.542215   \n",
       "                                                                                  tfidf      0.545736   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.448671   \n",
       "                                                                                  tfidf      0.330431   \n",
       "                                                                 porter_tokenizer count      0.450428   \n",
       "                                                                                  tfidf      0.329525   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.499362   \n",
       "                                                                                  tfidf      0.502417   \n",
       "                                                                 porter_tokenizer count      0.503231   \n",
       "                                                                                  tfidf      0.505267   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.551936   \n",
       "                                                                                  tfidf      0.545001   \n",
       "                                                                 porter_tokenizer count      0.545133   \n",
       "                                                                                  tfidf      0.540788   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.433701   \n",
       "                                                                                  tfidf      0.271215   \n",
       "                                                                 porter_tokenizer count      0.432929   \n",
       "                                                                                  tfidf      0.262150   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.453727   \n",
       "                                                                                  tfidf      0.460062   \n",
       "                                                                 porter_tokenizer count      0.451950   \n",
       "                                                                                  tfidf      0.458185   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.499922   \n",
       "                                                                                  tfidf      0.504880   \n",
       "                                                                 porter_tokenizer count      0.494407   \n",
       "                                                                                  tfidf      0.501595   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.371951   \n",
       "                                                                                  tfidf      0.251600   \n",
       "                                                                 porter_tokenizer count      0.371320   \n",
       "                                                                                  tfidf      0.242711   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.426802   \n",
       "                                                                                  tfidf      0.434292   \n",
       "                                                                 porter_tokenizer count      0.426330   \n",
       "                                                                                  tfidf      0.431415   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  std   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.003985   \n",
       "                                                                                  tfidf      0.004786   \n",
       "                                                                 porter_tokenizer count      0.004703   \n",
       "                                                                                  tfidf      0.003444   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.003129   \n",
       "                                                                                  tfidf      0.002431   \n",
       "                                                                 porter_tokenizer count      0.003606   \n",
       "                                                                                  tfidf      0.002330   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.005806   \n",
       "                                                                                  tfidf      0.006553   \n",
       "                                                                 porter_tokenizer count      0.005272   \n",
       "                                                                                  tfidf      0.006155   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.004547   \n",
       "                                                                                  tfidf      0.004446   \n",
       "                                                                 porter_tokenizer count      0.004174   \n",
       "                                                                                  tfidf      0.003785   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.003436   \n",
       "                                                                                  tfidf      0.002744   \n",
       "                                                                 porter_tokenizer count      0.003005   \n",
       "                                                                                  tfidf      0.002563   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.005141   \n",
       "                                                                                  tfidf      0.005734   \n",
       "                                                                 porter_tokenizer count      0.005534   \n",
       "                                                                                  tfidf      0.004308   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.004508   \n",
       "                                                                                  tfidf      0.003324   \n",
       "                                                                 porter_tokenizer count      0.004578   \n",
       "                                                                                  tfidf      0.003363   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.003794   \n",
       "                                                                                  tfidf      0.002791   \n",
       "                                                                 porter_tokenizer count      0.002668   \n",
       "                                                                                  tfidf      0.002976   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.006111   \n",
       "                                                                                  tfidf      0.004494   \n",
       "                                                                 porter_tokenizer count      0.005226   \n",
       "                                                                                  tfidf      0.005802   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  min   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.541369   \n",
       "                                                                                  tfidf      0.540493   \n",
       "                                                                 porter_tokenizer count      0.532686   \n",
       "                                                                                  tfidf      0.538274   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.442153   \n",
       "                                                                                  tfidf      0.325534   \n",
       "                                                                 porter_tokenizer count      0.442857   \n",
       "                                                                                  tfidf      0.324931   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.489057   \n",
       "                                                                                  tfidf      0.492290   \n",
       "                                                                 porter_tokenizer count      0.491848   \n",
       "                                                                                  tfidf      0.490982   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.540308   \n",
       "                                                                                  tfidf      0.534078   \n",
       "                                                                 porter_tokenizer count      0.536489   \n",
       "                                                                                  tfidf      0.533963   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.423814   \n",
       "                                                                                  tfidf      0.266073   \n",
       "                                                                 porter_tokenizer count      0.426950   \n",
       "                                                                                  tfidf      0.257057   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.444073   \n",
       "                                                                                  tfidf      0.448287   \n",
       "                                                                 porter_tokenizer count      0.435236   \n",
       "                                                                                  tfidf      0.448544   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.492773   \n",
       "                                                                                  tfidf      0.498290   \n",
       "                                                                 porter_tokenizer count      0.485106   \n",
       "                                                                                  tfidf      0.495570   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.363599   \n",
       "                                                                                  tfidf      0.245859   \n",
       "                                                                 porter_tokenizer count      0.365751   \n",
       "                                                                                  tfidf      0.237111   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.410047   \n",
       "                                                                                  tfidf      0.418811   \n",
       "                                                                 porter_tokenizer count      0.413097   \n",
       "                                                                                  tfidf      0.417076   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  25%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.546137   \n",
       "                                                                                  tfidf      0.547210   \n",
       "                                                                 porter_tokenizer count      0.538377   \n",
       "                                                                                  tfidf      0.543361   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.446622   \n",
       "                                                                                  tfidf      0.329370   \n",
       "                                                                 porter_tokenizer count      0.448206   \n",
       "                                                                                  tfidf      0.327550   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.494589   \n",
       "                                                                                  tfidf      0.497264   \n",
       "                                                                 porter_tokenizer count      0.500426   \n",
       "                                                                                  tfidf      0.501541   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.549120   \n",
       "                                                                                  tfidf      0.541805   \n",
       "                                                                 porter_tokenizer count      0.541949   \n",
       "                                                                                  tfidf      0.538523   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.431687   \n",
       "                                                                                  tfidf      0.269414   \n",
       "                                                                 porter_tokenizer count      0.430742   \n",
       "                                                                                  tfidf      0.259958   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.450424   \n",
       "                                                                                  tfidf      0.456206   \n",
       "                                                                 porter_tokenizer count      0.449188   \n",
       "                                                                                  tfidf      0.455987   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.496991   \n",
       "                                                                                  tfidf      0.502799   \n",
       "                                                                 porter_tokenizer count      0.491006   \n",
       "                                                                                  tfidf      0.498441   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.369612   \n",
       "                                                                                  tfidf      0.250055   \n",
       "                                                                 porter_tokenizer count      0.369749   \n",
       "                                                                                  tfidf      0.240438   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.422750   \n",
       "                                                                                  tfidf      0.432087   \n",
       "                                                                 porter_tokenizer count      0.422877   \n",
       "                                                                                  tfidf      0.428068   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  50%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.548990   \n",
       "                                                                                  tfidf      0.549587   \n",
       "                                                                 porter_tokenizer count      0.542811   \n",
       "                                                                                  tfidf      0.545791   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.448347   \n",
       "                                                                                  tfidf      0.330260   \n",
       "                                                                 porter_tokenizer count      0.450119   \n",
       "                                                                                  tfidf      0.329791   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.499688   \n",
       "                                                                                  tfidf      0.502105   \n",
       "                                                                 porter_tokenizer count      0.503675   \n",
       "                                                                                  tfidf      0.504355   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.552409   \n",
       "                                                                                  tfidf      0.545272   \n",
       "                                                                 porter_tokenizer count      0.545266   \n",
       "                                                                                  tfidf      0.540764   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.433676   \n",
       "                                                                                  tfidf      0.270807   \n",
       "                                                                 porter_tokenizer count      0.432629   \n",
       "                                                                                  tfidf      0.262426   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.454316   \n",
       "                                                                                  tfidf      0.459959   \n",
       "                                                                 porter_tokenizer count      0.452470   \n",
       "                                                                                  tfidf      0.458283   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.500149   \n",
       "                                                                                  tfidf      0.505008   \n",
       "                                                                 porter_tokenizer count      0.494850   \n",
       "                                                                                  tfidf      0.501681   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.371905   \n",
       "                                                                                  tfidf      0.251484   \n",
       "                                                                 porter_tokenizer count      0.371534   \n",
       "                                                                                  tfidf      0.242654   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.426096   \n",
       "                                                                                  tfidf      0.434852   \n",
       "                                                                 porter_tokenizer count      0.426232   \n",
       "                                                                                  tfidf      0.430413   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  75%   \n",
       "txt_field     classifier                                         tokenizer        vect_type             \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.552141   \n",
       "                                                                                  tfidf      0.554747   \n",
       "                                                                 porter_tokenizer count      0.545817   \n",
       "                                                                                  tfidf      0.548439   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.450588   \n",
       "                                                                                  tfidf      0.331961   \n",
       "                                                                 porter_tokenizer count      0.453286   \n",
       "                                                                                  tfidf      0.330996   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.503812   \n",
       "                                                                                  tfidf      0.507314   \n",
       "                                                                 porter_tokenizer count      0.505869   \n",
       "                                                                                  tfidf      0.508536   \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.554888   \n",
       "                                                                                  tfidf      0.548460   \n",
       "                                                                 porter_tokenizer count      0.548758   \n",
       "                                                                                  tfidf      0.543153   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.436707   \n",
       "                                                                                  tfidf      0.273052   \n",
       "                                                                 porter_tokenizer count      0.435277   \n",
       "                                                                                  tfidf      0.263803   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.457178   \n",
       "                                                                                  tfidf      0.464170   \n",
       "                                                                 porter_tokenizer count      0.455692   \n",
       "                                                                                  tfidf      0.460990   \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.502033   \n",
       "                                                                                  tfidf      0.507092   \n",
       "                                                                 porter_tokenizer count      0.497452   \n",
       "                                                                                  tfidf      0.503771   \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.374090   \n",
       "                                                                                  tfidf      0.252531   \n",
       "                                                                 porter_tokenizer count      0.372979   \n",
       "                                                                                  tfidf      0.244967   \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.431173   \n",
       "                                                                                  tfidf      0.436804   \n",
       "                                                                 porter_tokenizer count      0.430057   \n",
       "                                                                                  tfidf      0.435522   \n",
       "\n",
       "                                                                                                       \n",
       "                                                                                                  max  \n",
       "txt_field     classifier                                         tokenizer        vect_type            \n",
       "mission       ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.557126  \n",
       "                                                                                  tfidf      0.560542  \n",
       "                                                                 porter_tokenizer count      0.553073  \n",
       "                                                                                  tfidf      0.553280  \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.456381  \n",
       "                                                                                  tfidf      0.336480  \n",
       "                                                                 porter_tokenizer count      0.457872  \n",
       "                                                                                  tfidf      0.335323  \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.510094  \n",
       "                                                                                  tfidf      0.518759  \n",
       "                                                                 porter_tokenizer count      0.517862  \n",
       "                                                                                  tfidf      0.520476  \n",
       "mission_prgrm ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.562413  \n",
       "                                                                                  tfidf      0.551154  \n",
       "                                                                 porter_tokenizer count      0.554124  \n",
       "                                                                                  tfidf      0.550647  \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.440243  \n",
       "                                                                                  tfidf      0.279372  \n",
       "                                                                 porter_tokenizer count      0.438181  \n",
       "                                                                                  tfidf      0.268357  \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.463289  \n",
       "                                                                                  tfidf      0.471572  \n",
       "                                                                 porter_tokenizer count      0.461533  \n",
       "                                                                                  tfidf      0.468428  \n",
       "prgrm_dsc     ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.511826  \n",
       "                                                                                  tfidf      0.515543  \n",
       "                                                                 porter_tokenizer count      0.503668  \n",
       "                                                                                  tfidf      0.508528  \n",
       "              MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.381560  \n",
       "                                                                                  tfidf      0.261158  \n",
       "                                                                 porter_tokenizer count      0.377309  \n",
       "                                                                                  tfidf      0.248375  \n",
       "              RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.440200  \n",
       "                                                                                  tfidf      0.446010  \n",
       "                                                                 porter_tokenizer count      0.436586  \n",
       "                                                                                  tfidf      0.442634  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance[df_performance.average_mtd=='macro'].groupby(['txt_field', 'classifier', 'tokenizer', 'vect_type']).describe()[['accuracy','f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Draft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - MultinomialNB - LemmaTokenizer - TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_MNB_lemma_tfidf(trial):\n",
    "    global df_train, df_performance, txt_field, classifier, tokenizer, vect_type, average_mtd\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.MultinomialNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='tfidf' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>97</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.673533</td>\n",
       "      <td>0.479451</td>\n",
       "      <td>0.639056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>51</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.595300</td>\n",
       "      <td>0.353232</td>\n",
       "      <td>0.595305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>85</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.703933</td>\n",
       "      <td>0.533654</td>\n",
       "      <td>0.635387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>81</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.662233</td>\n",
       "      <td>0.426434</td>\n",
       "      <td>0.689126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>34</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.598700</td>\n",
       "      <td>0.355358</td>\n",
       "      <td>0.670974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>49</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.700633</td>\n",
       "      <td>0.540676</td>\n",
       "      <td>0.633220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>94</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.656733</td>\n",
       "      <td>0.475081</td>\n",
       "      <td>0.608428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>10</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.699167</td>\n",
       "      <td>0.529788</td>\n",
       "      <td>0.612299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>47</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.561700</td>\n",
       "      <td>0.300220</td>\n",
       "      <td>0.537231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>39</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>0.363073</td>\n",
       "      <td>0.620524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "681    97  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "353    51  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "595    85    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "561    81    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "240    34  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "347    49    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "662    94  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "79     10    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "335    47    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "269    39  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "681  LemmaTokenizer     tfidf       macro  0.673533   0.479451  0.639056  \n",
       "353  LemmaTokenizer     count       macro  0.595300   0.353232  0.595305  \n",
       "595  LemmaTokenizer     count       macro  0.703933   0.533654  0.635387  \n",
       "561  LemmaTokenizer     count       macro  0.662233   0.426434  0.689126  \n",
       "240  LemmaTokenizer     count       macro  0.598700   0.355358  0.670974  \n",
       "347  LemmaTokenizer     tfidf       macro  0.700633   0.540676  0.633220  \n",
       "662  LemmaTokenizer     count       macro  0.656733   0.475081  0.608428  \n",
       "79   LemmaTokenizer     tfidf       macro  0.699167   0.529788  0.612299  \n",
       "335  LemmaTokenizer     tfidf       macro  0.561700   0.300220  0.537231  \n",
       "269  LemmaTokenizer     count       macro  0.609900   0.363073  0.620524  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_mission_MNB_lemma_tfidf.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - MultinomialNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_MNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.MultinomialNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=func_mission_MNB_lemma_count.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - MultinomialNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_prgrm_MNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='prgrm_dsc' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.MultinomialNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.598833</td>\n",
       "      <td>0.349254</td>\n",
       "      <td>0.579754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.422397</td>\n",
       "      <td>0.644755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>26</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.656933</td>\n",
       "      <td>0.420701</td>\n",
       "      <td>0.683195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>88</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.426009</td>\n",
       "      <td>0.652042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>28</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.667333</td>\n",
       "      <td>0.431431</td>\n",
       "      <td>0.671606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>83</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.600867</td>\n",
       "      <td>0.357212</td>\n",
       "      <td>0.680094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.355520</td>\n",
       "      <td>0.627683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>96</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.603167</td>\n",
       "      <td>0.353313</td>\n",
       "      <td>0.623195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.660667</td>\n",
       "      <td>0.426780</td>\n",
       "      <td>0.647043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>94</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.662300</td>\n",
       "      <td>0.423771</td>\n",
       "      <td>0.637518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "4       1  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "13      7    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "52     26    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "176    88    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "56     28    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "167    83  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "11      5  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "194    96  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "28     14    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "188    94    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "4    LemmaTokenizer     count       macro  0.598833   0.349254  0.579754  \n",
       "13   LemmaTokenizer     count       macro  0.658333   0.422397  0.644755  \n",
       "52   LemmaTokenizer     count       macro  0.656933   0.420701  0.683195  \n",
       "176  LemmaTokenizer     count       macro  0.662600   0.426009  0.652042  \n",
       "56   LemmaTokenizer     count       macro  0.667333   0.431431  0.671606  \n",
       "167  LemmaTokenizer     count       macro  0.600867   0.357212  0.680094  \n",
       "11   LemmaTokenizer     count       macro  0.604600   0.355520  0.627683  \n",
       "194  LemmaTokenizer     count       macro  0.603167   0.353313  0.623195  \n",
       "28   LemmaTokenizer     count       macro  0.660667   0.426780  0.647043  \n",
       "188  LemmaTokenizer     count       macro  0.662300   0.423771  0.637518  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_prgrm_MNB_lemma_count.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_CNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.ComplementNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>29</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.606267</td>\n",
       "      <td>0.356488</td>\n",
       "      <td>0.618608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>53</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.663600</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>0.640428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>36</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.353021</td>\n",
       "      <td>0.649588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>30</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.699667</td>\n",
       "      <td>0.534671</td>\n",
       "      <td>0.630701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>39</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>0.363073</td>\n",
       "      <td>0.620524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>51</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.595300</td>\n",
       "      <td>0.353232</td>\n",
       "      <td>0.595305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>53</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>0.534318</td>\n",
       "      <td>0.625981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.353904</td>\n",
       "      <td>0.650824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.355520</td>\n",
       "      <td>0.627683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>60</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.428999</td>\n",
       "      <td>0.678820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "115    29  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "209    53    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "146    36  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "126    30    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "155    39  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "203    51  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "215    53    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "15      3  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "17      5  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "240    60    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "115  LemmaTokenizer     count       macro  0.606267   0.356488  0.618608  \n",
       "209  LemmaTokenizer     count       macro  0.663600   0.426699  0.640428  \n",
       "146  LemmaTokenizer     count       macro  0.606667   0.353021  0.649588  \n",
       "126  LemmaTokenizer     count       macro  0.699667   0.534671  0.630701  \n",
       "155  LemmaTokenizer     count       macro  0.609900   0.363073  0.620524  \n",
       "203  LemmaTokenizer     count       macro  0.595300   0.353232  0.595305  \n",
       "215  LemmaTokenizer     count       macro  0.700400   0.534318  0.625981  \n",
       "15   LemmaTokenizer     count       macro  0.601300   0.353904  0.650824  \n",
       "17   LemmaTokenizer     count       macro  0.604600   0.355520  0.627683  \n",
       "240  LemmaTokenizer     count       macro  0.663200   0.428999  0.678820  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_mission_CNB_lemma_count.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_prgrm_CNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='prgrm_dsc' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.ComplementNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.601267</td>\n",
       "      <td>0.351739</td>\n",
       "      <td>0.625842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>67</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.598733</td>\n",
       "      <td>0.353515</td>\n",
       "      <td>0.625180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>97</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.657133</td>\n",
       "      <td>0.422356</td>\n",
       "      <td>0.644037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>94</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.598733</td>\n",
       "      <td>0.352534</td>\n",
       "      <td>0.630569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>37</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.661867</td>\n",
       "      <td>0.428198</td>\n",
       "      <td>0.653606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>82</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.600867</td>\n",
       "      <td>0.351543</td>\n",
       "      <td>0.663272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>80</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.659667</td>\n",
       "      <td>0.426818</td>\n",
       "      <td>0.636164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>62</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.657400</td>\n",
       "      <td>0.478270</td>\n",
       "      <td>0.614129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>34</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.655733</td>\n",
       "      <td>0.479513</td>\n",
       "      <td>0.613932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>68</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.659033</td>\n",
       "      <td>0.424482</td>\n",
       "      <td>0.680102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "3       0  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "201    67  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "289    97    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "284    94  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "109    37    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "248    82  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "240    80    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "190    62  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "106    34  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "204    68    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "3    LemmaTokenizer     count       macro  0.601267   0.351739  0.625842  \n",
       "201  LemmaTokenizer     count       macro  0.598733   0.353515  0.625180  \n",
       "289  LemmaTokenizer     count       macro  0.657133   0.422356  0.644037  \n",
       "284  LemmaTokenizer     count       macro  0.598733   0.352534  0.630569  \n",
       "109  LemmaTokenizer     count       macro  0.661867   0.428198  0.653606  \n",
       "248  LemmaTokenizer     count       macro  0.600867   0.351543  0.663272  \n",
       "240  LemmaTokenizer     count       macro  0.659667   0.426818  0.636164  \n",
       "190  LemmaTokenizer     count       macro  0.657400   0.478270  0.614129  \n",
       "106  LemmaTokenizer     count       macro  0.655733   0.479513  0.613932  \n",
       "204  LemmaTokenizer     count       macro  0.659033   0.424482  0.680102  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_prgrm_CNB_lemma_count.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - ComplementNB - LemmaTokenizer - TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_prgrm_CNB_lemma_tfidf(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='prgrm_dsc' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.ComplementNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='tfidf' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>0.528067</td>\n",
       "      <td>0.616052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>51</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.661267</td>\n",
       "      <td>0.426321</td>\n",
       "      <td>0.683731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>46</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>0.531807</td>\n",
       "      <td>0.626519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>56</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.653367</td>\n",
       "      <td>0.477555</td>\n",
       "      <td>0.613562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>46</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.655467</td>\n",
       "      <td>0.474524</td>\n",
       "      <td>0.621911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>81</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.484696</td>\n",
       "      <td>0.616098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>73</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.656233</td>\n",
       "      <td>0.477075</td>\n",
       "      <td>0.610757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>93</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.657600</td>\n",
       "      <td>0.423429</td>\n",
       "      <td>0.641899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>11</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.661233</td>\n",
       "      <td>0.424397</td>\n",
       "      <td>0.657256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.353904</td>\n",
       "      <td>0.650824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "96     18    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "251    51    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "236    46    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "284    56  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "234    46  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "405    81  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "365    73  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "461    93    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "47     11    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "18      3  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "96   LemmaTokenizer     count       macro  0.700400   0.528067  0.616052  \n",
       "251  LemmaTokenizer     count       macro  0.661267   0.426321  0.683731  \n",
       "236  LemmaTokenizer     count       macro  0.701200   0.531807  0.626519  \n",
       "284  LemmaTokenizer     count       macro  0.653367   0.477555  0.613562  \n",
       "234  LemmaTokenizer     count       macro  0.655467   0.474524  0.621911  \n",
       "405  LemmaTokenizer     count       macro  0.660900   0.484696  0.616098  \n",
       "365  LemmaTokenizer     count       macro  0.656233   0.477075  0.610757  \n",
       "461  LemmaTokenizer     count       macro  0.657600   0.423429  0.641899  \n",
       "47   LemmaTokenizer     count       macro  0.661233   0.424397  0.657256  \n",
       "18   LemmaTokenizer     count       macro  0.601300   0.353904  0.650824  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_prgrm_CNB_lemma_tfidf.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - ComplementNB - LemmaTokenizer - TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_CNB_lemma_tfidf(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.ComplementNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='tfidf' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>40</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.673467</td>\n",
       "      <td>0.481593</td>\n",
       "      <td>0.663747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>69</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.603033</td>\n",
       "      <td>0.353719</td>\n",
       "      <td>0.629368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.693267</td>\n",
       "      <td>0.533007</td>\n",
       "      <td>0.631130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>37</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.608833</td>\n",
       "      <td>0.358827</td>\n",
       "      <td>0.599976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>63</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.665100</td>\n",
       "      <td>0.430047</td>\n",
       "      <td>0.652673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>88</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.604800</td>\n",
       "      <td>0.353762</td>\n",
       "      <td>0.596581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>16</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.659567</td>\n",
       "      <td>0.425299</td>\n",
       "      <td>0.648161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>63</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.655667</td>\n",
       "      <td>0.474790</td>\n",
       "      <td>0.606161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>33</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.657933</td>\n",
       "      <td>0.423631</td>\n",
       "      <td>0.641981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>14</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.662267</td>\n",
       "      <td>0.481588</td>\n",
       "      <td>0.611395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "248    40  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "411    69  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "51      6    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "219    37  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "373    63    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "530    88  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "96     16    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "377    63  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "193    33    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "88     14  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "248  LemmaTokenizer     tfidf       macro  0.673467   0.481593  0.663747  \n",
       "411  LemmaTokenizer     count       macro  0.603033   0.353719  0.629368  \n",
       "51   LemmaTokenizer     tfidf       macro  0.693267   0.533007  0.631130  \n",
       "219  LemmaTokenizer     count       macro  0.608833   0.358827  0.599976  \n",
       "373  LemmaTokenizer     count       macro  0.665100   0.430047  0.652673  \n",
       "530  LemmaTokenizer     count       macro  0.604800   0.353762  0.596581  \n",
       "96   LemmaTokenizer     count       macro  0.659567   0.425299  0.648161  \n",
       "377  LemmaTokenizer     count       macro  0.655667   0.474790  0.606161  \n",
       "193  LemmaTokenizer     count       macro  0.657933   0.423631  0.641981  \n",
       "88   LemmaTokenizer     count       macro  0.662267   0.481588  0.611395  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_mission_CNB_lemma_tfidf.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"5\" halign=\"left\">precision</th>\n",
       "      <th colspan=\"8\" halign=\"left\">recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>vect_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">mission</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.699907</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.693900</td>\n",
       "      <td>0.698317</td>\n",
       "      <td>0.700100</td>\n",
       "      <td>0.701692</td>\n",
       "      <td>0.706467</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.531594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533821</td>\n",
       "      <td>0.541320</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.627921</td>\n",
       "      <td>0.008283</td>\n",
       "      <td>0.608945</td>\n",
       "      <td>0.622043</td>\n",
       "      <td>0.627389</td>\n",
       "      <td>0.632986</td>\n",
       "      <td>0.647726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.697841</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.690767</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.698233</td>\n",
       "      <td>0.699642</td>\n",
       "      <td>0.703133</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.533794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536262</td>\n",
       "      <td>0.541629</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.622604</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.600930</td>\n",
       "      <td>0.617881</td>\n",
       "      <td>0.622611</td>\n",
       "      <td>0.626997</td>\n",
       "      <td>0.646784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.661342</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.659417</td>\n",
       "      <td>0.661550</td>\n",
       "      <td>0.663217</td>\n",
       "      <td>0.668867</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.425750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428148</td>\n",
       "      <td>0.433099</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.659809</td>\n",
       "      <td>0.021351</td>\n",
       "      <td>0.622731</td>\n",
       "      <td>0.643071</td>\n",
       "      <td>0.650996</td>\n",
       "      <td>0.682431</td>\n",
       "      <td>0.703958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">prgrm_dsc</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.657213</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.650667</td>\n",
       "      <td>0.655233</td>\n",
       "      <td>0.657233</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.663833</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.476934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479399</td>\n",
       "      <td>0.485138</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.614657</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.591100</td>\n",
       "      <td>0.609157</td>\n",
       "      <td>0.614943</td>\n",
       "      <td>0.619995</td>\n",
       "      <td>0.641197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.673393</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.667533</td>\n",
       "      <td>0.671317</td>\n",
       "      <td>0.673467</td>\n",
       "      <td>0.675242</td>\n",
       "      <td>0.680067</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.479188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481655</td>\n",
       "      <td>0.485097</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.644290</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>0.617138</td>\n",
       "      <td>0.635860</td>\n",
       "      <td>0.644252</td>\n",
       "      <td>0.650899</td>\n",
       "      <td>0.684130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.602443</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.595300</td>\n",
       "      <td>0.600658</td>\n",
       "      <td>0.602350</td>\n",
       "      <td>0.604150</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.355167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356803</td>\n",
       "      <td>0.364457</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.627163</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.577607</td>\n",
       "      <td>0.609106</td>\n",
       "      <td>0.625511</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.706715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       accuracy  \\\n",
       "                                                                          count   \n",
       "txt_field classifier                                         vect_type            \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count        100.0   \n",
       "                                                             tfidf        100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count        100.0   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count        100.0   \n",
       "                                                             tfidf        100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count        100.0   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                            mean   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.699907   \n",
       "                                                             tfidf      0.697841   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.661342   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.657213   \n",
       "                                                             tfidf      0.673393   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.602443   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             std   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.002527   \n",
       "                                                             tfidf      0.002500   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.002978   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.002682   \n",
       "                                                             tfidf      0.002592   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.002772   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             min   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.693900   \n",
       "                                                             tfidf      0.690767   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.651700   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.650667   \n",
       "                                                             tfidf      0.667533   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.595300   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             25%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.698317   \n",
       "                                                             tfidf      0.695900   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.659417   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.655233   \n",
       "                                                             tfidf      0.671317   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.600658   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             50%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.700100   \n",
       "                                                             tfidf      0.698233   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.661550   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.657233   \n",
       "                                                             tfidf      0.673467   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.602350   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             75%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.701692   \n",
       "                                                             tfidf      0.699642   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.663217   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.659000   \n",
       "                                                             tfidf      0.675242   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.604150   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             max   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.706467   \n",
       "                                                             tfidf      0.703133   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.668867   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.663833   \n",
       "                                                             tfidf      0.680067   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.609900   \n",
       "\n",
       "                                                                       precision  \\\n",
       "                                                                           count   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count         100.0   \n",
       "                                                             tfidf         100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count         100.0   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count         100.0   \n",
       "                                                             tfidf         100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count         100.0   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                            mean   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.531594   \n",
       "                                                             tfidf      0.533794   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.425750   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.476934   \n",
       "                                                             tfidf      0.479188   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.355167   \n",
       "\n",
       "                                                                          ...     \\\n",
       "                                                                          ...      \n",
       "txt_field classifier                                         vect_type    ...      \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count        ...      \n",
       "                                                             tfidf        ...      \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count        ...      \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count        ...      \n",
       "                                                             tfidf        ...      \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count        ...      \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             75%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.533821   \n",
       "                                                             tfidf      0.536262   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.428148   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.479399   \n",
       "                                                             tfidf      0.481655   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.356803   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             max   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.541320   \n",
       "                                                             tfidf      0.541629   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.433099   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.485138   \n",
       "                                                             tfidf      0.485097   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.364457   \n",
       "\n",
       "                                                                       recall  \\\n",
       "                                                                        count   \n",
       "txt_field classifier                                         vect_type          \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      100.0   \n",
       "                                                             tfidf      100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      100.0   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      100.0   \n",
       "                                                             tfidf      100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      100.0   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                            mean   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.627921   \n",
       "                                                             tfidf      0.622604   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.659809   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.614657   \n",
       "                                                             tfidf      0.644290   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.627163   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             std   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.008283   \n",
       "                                                             tfidf      0.007315   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.021351   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.008150   \n",
       "                                                             tfidf      0.012344   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.028436   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             min   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.608945   \n",
       "                                                             tfidf      0.600930   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.622731   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.591100   \n",
       "                                                             tfidf      0.617138   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.577607   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             25%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.622043   \n",
       "                                                             tfidf      0.617881   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.643071   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.609157   \n",
       "                                                             tfidf      0.635860   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.609106   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             50%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.627389   \n",
       "                                                             tfidf      0.622611   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.650996   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.614943   \n",
       "                                                             tfidf      0.644252   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.625511   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             75%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.632986   \n",
       "                                                             tfidf      0.626997   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.682431   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.619995   \n",
       "                                                             tfidf      0.650899   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.647579   \n",
       "\n",
       "                                                                                  \n",
       "                                                                             max  \n",
       "txt_field classifier                                         vect_type            \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.647726  \n",
       "                                                             tfidf      0.646784  \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.703958  \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.641197  \n",
       "                                                             tfidf      0.684130  \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.706715  \n",
       "\n",
       "[6 rows x 24 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance.groupby(['txt_field', 'classifier', 'vect_type']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.661060</td>\n",
       "      <td>0.425503</td>\n",
       "      <td>0.658984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.020294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.655533</td>\n",
       "      <td>0.419339</td>\n",
       "      <td>0.627483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.659017</td>\n",
       "      <td>0.423844</td>\n",
       "      <td>0.644080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.661350</td>\n",
       "      <td>0.425360</td>\n",
       "      <td>0.650408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.662775</td>\n",
       "      <td>0.427471</td>\n",
       "      <td>0.677598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.668267</td>\n",
       "      <td>0.432884</td>\n",
       "      <td>0.705346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy   precision      recall\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     0.661060    0.425503    0.658984\n",
       "std      0.002628    0.002539    0.020294\n",
       "min      0.655533    0.419339    0.627483\n",
       "25%      0.659017    0.423844    0.644080\n",
       "50%      0.661350    0.425360    0.650408\n",
       "75%      0.662775    0.427471    0.677598\n",
       "max      0.668267    0.432884    0.705346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mission_MNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [3:10:23<33:50, 135.39s/it]"
     ]
    }
   ],
   "source": [
    "df_mission_CNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['mission'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.ComplementNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_mission_CNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mission_CNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - MultinomialNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [11:35:52<00:00, 419.77s/it]\n"
     ]
    }
   ],
   "source": [
    "df_prgrm_MNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['prgrm_dsc'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.MultinomialNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_prgrm_MNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_MNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_CNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['prgrm_dsc'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.ComplementNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_prgrm_CNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_CNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, TF IDF Vectors:  [0.5583666666666667, 0.29982304712090974, 0.5961215483457072, datetime.timedelta(microseconds=498992)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(classifier=naive_bayes.MultinomialNB(), \n",
    "                       x_train=x_train_vect_tfidf,\n",
    "                       y_train= y_train, \n",
    "                       x_valid=x_valid_vect_tfidf,\n",
    "                       y_valid=y_valid\n",
    "                      )\n",
    "results.loc[len(results)] = [\"NB, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NB, TF IDF Vectors: \", accuracy)\n",
    "\n",
    "\n",
    "# Raw: MNB, Count Vectors:  [0.6732666666666667, 0.4503852200520778, 0.6661678975327818, datetime.timedelta(microseconds=489443)]\n",
    "# Raw: CNB, Count Vectors:  [0.7013333333333334, 0.5275971009306734, 0.6170202787685387, datetime.timedelta(microseconds=521657)]\n",
    "# Stemmed: MNB, Count Vectors:  [0.6601, 0.4243013182900721, 0.639930088219624, datetime.timedelta(microseconds=443571)]\n",
    "# Stemmed: CNB, Count Vectors:  [0.7, 0.5320034132229355, 0.6383184621220114, datetime.timedelta(microseconds=504788)]\n",
    "# Lemma: MNB, Count Vectors:  [0.6615, 0.42366552445612, 0.6783346552845068, datetime.timedelta(microseconds=572416)]\n",
    "# Lemma: CNB, Count Vectors:  [0.7021333333333334, 0.5341465196920103, 0.6318440519847688, datetime.timedelta(microseconds=586597)]\n",
    "\n",
    "# Raw: MNB, TF IDF Vectors:  [0.5859, 0.32327746269021723, 0.5485416630089535, datetime.timedelta(microseconds=536424)]\n",
    "# Raw: CNB, TF IDF Vectors:  [0.6992333333333334, 0.5306246321804787, 0.6136847132057796, datetime.timedelta(microseconds=580513)]\n",
    "# Stemmed: MNB, TF IDF Vectors:  [0.553, 0.2949552239378839, 0.536824161513825, datetime.timedelta(microseconds=453431)]\n",
    "# Stemmed: CNB, TF IDF Vectors:  [0.6967666666666666, 0.5349583083797411, 0.6223488506468897, datetime.timedelta(microseconds=520307)]\n",
    "# Lemma: MNB, TF IDF Vectors:  [0.5589666666666666, 0.297005157029138, 0.5293106710625075, datetime.timedelta(microseconds=529497)]\n",
    "# Lemma: CNB, TF IDF Vectors:  [0.7012333333333334, 0.536867845262306, 0.6273414839488708, datetime.timedelta(microseconds=546401)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like `Lemma-CNB-Count` produces best results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
