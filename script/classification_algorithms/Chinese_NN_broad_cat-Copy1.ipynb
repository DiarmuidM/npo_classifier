{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T15:38:40.471332Z",
     "start_time": "2019-09-21T15:38:30.048779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# obtain reproducible results\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ...\n",
    "\n",
    "# Check GPU device.\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T15:38:42.614356Z",
     "start_time": "2019-09-21T15:38:40.473496Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T15:38:42.737288Z",
     "start_time": "2019-09-21T15:38:42.615805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proj_name</th>\n",
       "      <th>service_area</th>\n",
       "      <th>proj_desc_zh</th>\n",
       "      <th>proj_desc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>购买导师、学位服</td>\n",
       "      <td>教育</td>\n",
       "      <td>支助武汉纺织大学教育事业发展</td>\n",
       "      <td>Support the development of education in Wuhan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>奖励教师</td>\n",
       "      <td>教育</td>\n",
       "      <td>奖励教师</td>\n",
       "      <td>Reward teacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>奖励资助学生</td>\n",
       "      <td>教育</td>\n",
       "      <td>奖励资助学生</td>\n",
       "      <td>Reward funded students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>教师及学生培训</td>\n",
       "      <td>教育</td>\n",
       "      <td>教师及学生培训费用</td>\n",
       "      <td>Teacher and student training fees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>资助义务教育学生</td>\n",
       "      <td>教育</td>\n",
       "      <td>资助义务教育阶段</td>\n",
       "      <td>Funding compulsory education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proj_name service_area    proj_desc_zh  \\\n",
       "0  购买导师、学位服           教育  支助武汉纺织大学教育事业发展   \n",
       "1      奖励教师           教育            奖励教师   \n",
       "2    奖励资助学生           教育          奖励资助学生   \n",
       "3   教师及学生培训           教育       教师及学生培训费用   \n",
       "4  资助义务教育学生           教育        资助义务教育阶段   \n",
       "\n",
       "                                        proj_desc_en  \n",
       "0  Support the development of education in Wuhan ...  \n",
       "1                                     Reward teacher  \n",
       "2                             Reward funded students  \n",
       "3                  Teacher and student training fees  \n",
       "4                       Funding compulsory education  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('/Users/yizhuoli/Downloads/WebCrawler/RICF_private/output_bin/df_proj.pkl.gz', compression='gzip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T15:38:42.774111Z",
     "start_time": "2019-09-21T15:38:42.739357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service_area\n",
      "体育         0.008939\n",
      "公益事业发展     0.113826\n",
      "医疗卫生       0.087505\n",
      "志愿服务       0.013707\n",
      "扶贫及社区发展    0.093564\n",
      "政策倡导       0.006555\n",
      "教育         0.539829\n",
      "文化艺术       0.038439\n",
      "法律与公民权力    0.005165\n",
      "灾害救助       0.009138\n",
      "生态环境       0.027910\n",
      "社会服务       0.039035\n",
      "科学研究       0.016389\n",
      "Name: proj_name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# See the composition by NTEE major groups.\n",
    "print(data.groupby('service_area')['proj_name'].count()/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T15:40:53.469393Z",
     "start_time": "2019-09-21T15:40:53.446611Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(list(data.service_area.unique()))\n",
    "\n",
    "data_y=lb.transform(data['service_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T15:40:55.335580Z",
     "start_time": "2019-09-21T15:40:55.330747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T15:41:03.596884Z",
     "start_time": "2019-09-21T15:41:03.591295Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save LabelBinarizer class for developing package.\n",
    "with open('../../output/chinese_lb_broad_cat.pkl', 'wb') as output:\n",
    "    pickle.dump(lb, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T15:41:07.369579Z",
     "start_time": "2019-09-21T15:41:07.366432Z"
    }
   },
   "outputs": [],
   "source": [
    "text_token_list=data['proj_desc_zh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:26:32.280946Z",
     "start_time": "2019-09-21T17:26:24.781113Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10068/10068 [00:07<00:00, 1364.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 \n",
    "\n",
    "token_sentence_list = []\n",
    "sentences_for_token = []\n",
    "\n",
    "for sentence in tqdm(text_token_list.to_list()):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False, HMM=True)\n",
    "    cur_seg_list = list(seg_list)\n",
    "    token_sentence_list.append(cur_seg_list)\n",
    "    sentences_for_token.extend(cur_seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:26:43.087393Z",
     "start_time": "2019-09-21T17:26:43.083353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['支助', '武汉', '纺织', '大学', '教育', '事业', '发展'],\n",
       " ['奖励', '教师'],\n",
       " ['奖励', '资助', '学生'],\n",
       " ['教师', '及', '学生', '培训', '费用'],\n",
       " ['资助', '义务教育', '阶段']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sentence_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:26:32.825959Z",
     "start_time": "2019-09-21T17:26:32.822035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699453"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_for_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:26:33.459320Z",
     "start_time": "2019-09-21T17:26:33.454483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10068"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:22:39.411078Z",
     "start_time": "2019-09-21T17:22:33.362855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('的', 1), ('、', 2), ('。', 3), ('项目', 4), ('年', 5), ('和', 6), ('为', 7), ('“', 8), ('”', 9), ('2017', 10), ('资助', 11), ('基金会', 12), ('在', 13), ('了', 14), ('元', 15), ('等', 16), ('学生', 17), ('月', 18), ('万元', 19), ('活动', 20), ('捐赠', 21), ('开展', 22), ('教育', 23), ('与', 24), ('用于', 25), ('发展', 26), ('公益', 27), ('学校', 28), ('及', 29), ('名', 30), ('是', 31), ('对', 32), ('中国', 33), ('社会', 34), ('进行', 35), ('人', 36), ('奖励', 37), ('建设', 38), ('由', 39), ('儿童', 40), ('支持', 41), ('通过', 42), ('文化', 43), ('该', 44), ('基金', 45), ('家庭', 46), ('工作', 47), ('设立', 48), ('日', 49), ('1', 50)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_for_token)\n",
    "print(list(tokenizer.word_index.items())[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:22:39.461631Z",
     "start_time": "2019-09-21T17:22:39.413523Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save tokenizer class for developing package.\n",
    "with open('../../output/chinese_tokenizer.pkl', 'wb') as output:\n",
    "    pickle.dump(tokenizer, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:27:06.042707Z",
     "start_time": "2019-09-21T17:27:05.704532Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_encoding_text = tokenizer.texts_to_sequences(token_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:27:14.601088Z",
     "start_time": "2019-09-21T17:27:14.596524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4154, 724, 1092, 79, 23, 113, 26],\n",
       " [37, 56],\n",
       " [37, 11, 17],\n",
       " [56, 29, 17, 81, 203],\n",
       " [11, 1128, 769]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_encoding_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:27:42.965213Z",
     "start_time": "2019-09-21T17:27:42.851261Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pads sequences to the same length (i.e., prepare matrix).\n",
    "data_sequences = pad_sequences(sequences=seq_encoding_text,\n",
    "                               # Max length of the sequence.\n",
    "                               maxlen=max([len(s) for s in seq_encoding_text]),\n",
    "                               dtype=\"int32\", padding=\"post\", truncating=\"post\",\n",
    "                               # Zero is used for representing None or Unknown.\n",
    "                               value=0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T17:28:37.724446Z",
     "start_time": "2019-09-21T17:28:37.718912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4154,  724, 1092, ...,    0,    0,    0],\n",
       "       [  37,   56,    0, ...,    0,    0,    0],\n",
       "       [  37,   11,   17, ...,    0,    0,    0],\n",
       "       [  56,   29,   17, ...,    0,    0,    0],\n",
       "       [  11, 1128,  769, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:23:38.155539Z",
     "start_time": "2019-09-21T19:21:03.742528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec load succeed\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv_from_text = KeyedVectors.load_word2vec_format('../classification_algorithms/WV/sgns.baidubaike.bigram-char',\n",
    "                                                 binary=False, encoding=\"utf8\",  unicode_errors='ignore')  # C text format\n",
    "print(\"word2vec load succeed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:32:42.154534Z",
     "start_time": "2019-09-21T19:32:42.150496Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=300\n",
    "\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM)) # Plus one: embedding matrix starts from 0, word index starts from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:32:50.225597Z",
     "start_time": "2019-09-21T19:32:50.221917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:32:50.828625Z",
     "start_time": "2019-09-21T19:32:50.823814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv_from_text.get_vector('的'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:32:57.761030Z",
     "start_time": "2019-09-21T19:32:57.488077Z"
    }
   },
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[index] = wv_from_text.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:32:59.544468Z",
     "start_time": "2019-09-21T19:32:59.535540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.38736004e-01,  3.99206012e-01,  4.68591988e-01,  2.27039997e-02,\n",
       "        1.02711998e-01,  4.20415014e-01,  4.57598001e-01,  2.16646999e-01,\n",
       "        7.90826023e-01,  8.36820006e-02, -1.47080002e-02, -4.44790989e-01,\n",
       "       -3.42970006e-02,  7.74936020e-01, -4.60329987e-02,  4.66491014e-01,\n",
       "       -2.89357007e-01,  5.02340019e-01,  3.28007996e-01, -6.75639987e-01,\n",
       "        5.65692008e-01, -3.43730986e-01, -1.89420003e-02, -7.98878014e-01,\n",
       "        4.05577004e-01,  1.72290001e-02,  4.81579006e-01,  2.92982012e-01,\n",
       "       -4.76305008e-01,  5.46975970e-01,  5.50563991e-01, -3.19233000e-01,\n",
       "        4.52850997e-01, -1.10495001e-01, -2.23923996e-01,  3.98712993e-01,\n",
       "       -8.58199969e-02,  9.53070000e-02,  5.71292996e-01,  1.10839996e-02,\n",
       "       -2.97479004e-01, -3.58787000e-01,  6.14437997e-01,  1.67586997e-01,\n",
       "       -8.35470036e-02,  3.57475996e-01, -2.91913986e-01,  4.54210997e-01,\n",
       "        6.67499006e-01,  6.94697022e-01, -5.39278984e-01,  4.34902012e-01,\n",
       "       -3.34470004e-01,  3.63689996e-02,  2.73842990e-01,  1.50370002e-02,\n",
       "        2.18005002e-01,  1.62416995e-01, -4.27902997e-01,  4.26957995e-01,\n",
       "       -2.37522006e-01, -3.63380015e-01, -1.38548002e-01,  6.78663015e-01,\n",
       "        4.89708006e-01,  4.30442989e-01,  2.27210000e-01,  2.91159991e-02,\n",
       "        8.64144981e-01, -1.01063997e-01,  1.48509994e-01, -7.13060975e-01,\n",
       "       -1.77102000e-01,  2.46560007e-01, -3.47618014e-01, -1.14494003e-01,\n",
       "        2.44317994e-01,  3.19119990e-01, -1.90467998e-01,  1.28597006e-01,\n",
       "        7.07076013e-01,  1.12281002e-01,  2.83587992e-01,  3.36273015e-01,\n",
       "        3.93027008e-01, -4.34875995e-01, -3.11500002e-02,  5.30914009e-01,\n",
       "       -1.02586001e-01, -2.25649998e-02, -1.99763998e-01,  8.51520002e-02,\n",
       "        3.24119002e-01, -6.09880015e-02, -1.91672996e-01,  1.01480000e-02,\n",
       "       -6.58710003e-02,  4.52973992e-01,  2.15353996e-01,  4.51010019e-02,\n",
       "       -6.85859993e-02, -2.23299995e-01, -2.96649992e-01, -9.42239985e-02,\n",
       "        2.08736002e-01, -1.04126997e-01,  5.98226011e-01, -2.13747993e-01,\n",
       "       -2.98487991e-01,  3.05063993e-01, -5.01627028e-01,  2.97363013e-01,\n",
       "        1.37895003e-01,  2.89009005e-01,  2.97991991e-01,  9.34910029e-02,\n",
       "       -2.91886985e-01, -5.82019985e-01, -7.72637010e-01,  3.37056011e-01,\n",
       "        1.96193993e-01, -3.97487998e-01, -1.51003003e-01, -5.34644008e-01,\n",
       "        3.87737989e-01,  4.57799993e-02,  4.69577014e-01,  3.09235007e-01,\n",
       "       -1.20912001e-01,  4.62624013e-01,  2.30316997e-01,  8.14543009e-01,\n",
       "       -1.81253999e-01, -5.68785012e-01,  4.42943990e-01,  5.23846984e-01,\n",
       "        1.73548996e-01, -1.57058999e-01, -6.10600002e-02, -6.30465984e-01,\n",
       "        2.84491986e-01, -9.64460000e-02,  1.79802001e-01, -3.29306990e-01,\n",
       "        1.96602002e-01,  5.50969988e-02, -1.85493007e-01,  1.30251005e-01,\n",
       "        4.80000017e-05, -2.43952006e-01, -9.82298970e-01, -1.64075002e-01,\n",
       "       -2.50151008e-01, -2.58935988e-01, -6.33791983e-01,  3.20248008e-01,\n",
       "       -3.88220012e-01,  5.32860011e-02,  6.81279004e-01,  5.52636981e-01,\n",
       "       -2.90116012e-01, -4.50670004e-01,  1.09466001e-01, -7.09273994e-01,\n",
       "       -1.82085007e-01,  2.07599998e-03, -1.13046996e-01,  2.26235002e-01,\n",
       "        2.82792985e-01,  1.71526998e-01,  6.68410003e-01,  4.56584990e-01,\n",
       "        6.12317979e-01,  2.93561995e-01, -5.87867022e-01,  2.19180007e-02,\n",
       "       -3.86189014e-01, -9.94950011e-02, -4.54773992e-01, -2.76338011e-01,\n",
       "        2.30774999e-01,  4.34695005e-01, -3.28469992e-01,  6.29562020e-01,\n",
       "       -1.58503994e-01, -5.62510014e-01,  1.11513102e+00,  4.18666005e-01,\n",
       "        6.45533025e-01, -1.91541001e-01,  3.67581993e-01, -4.21018988e-01,\n",
       "        4.12656993e-01, -8.28270018e-02, -5.17292976e-01,  1.13342002e-01,\n",
       "       -6.27093971e-01, -3.67224991e-01,  2.71198004e-01,  3.03185999e-01,\n",
       "       -5.30973971e-01,  7.26607025e-01,  3.14833999e-01,  6.49930015e-02,\n",
       "        4.66057986e-01, -2.29914993e-01,  4.19663996e-01,  2.19139997e-02,\n",
       "       -3.14100012e-02, -3.53715003e-01, -7.70950019e-02,  4.26021993e-01,\n",
       "        8.16569999e-02, -6.46170974e-01,  4.09601003e-01,  1.81206003e-01,\n",
       "        2.87905991e-01,  8.45274985e-01,  7.52973974e-01,  3.69910002e-02,\n",
       "       -1.30608007e-01, -3.97821009e-01,  2.90122002e-01, -5.81119001e-01,\n",
       "        2.01334998e-01, -2.13820003e-02,  1.34422004e-01,  1.10324003e-01,\n",
       "        5.41527987e-01,  4.31147993e-01, -1.05573997e-01, -2.13530008e-02,\n",
       "       -3.67350012e-01, -3.63171995e-01, -7.92024016e-01, -1.04000002e-01,\n",
       "        3.25120002e-01,  5.20090014e-02,  9.54000000e-03, -1.85736001e-01,\n",
       "        4.91571009e-01,  3.13380003e-01, -1.00070998e-01, -1.05972998e-01,\n",
       "        4.12705004e-01, -3.70779008e-01, -1.62940007e-02, -4.16370004e-01,\n",
       "       -3.11780989e-01,  3.31717998e-01,  4.68818992e-01,  2.36840006e-02,\n",
       "        3.55749987e-02,  2.29350001e-01, -2.78643012e-01, -1.75676003e-01,\n",
       "        5.52522004e-01, -3.96591008e-01,  1.89324006e-01, -1.92389991e-02,\n",
       "        1.70929998e-01,  1.36945993e-01,  3.35500017e-02,  2.74273992e-01,\n",
       "        2.64382005e-01, -9.90950018e-02, -1.16630001e-02, -3.09329003e-01,\n",
       "        2.63704002e-01, -1.74320005e-02, -5.28060012e-02,  4.28252995e-01,\n",
       "       -4.86840010e-01, -2.24531993e-01,  3.91983986e-01, -1.55422002e-01,\n",
       "       -2.03459993e-01, -1.47320004e-02,  2.51015991e-01,  1.26478001e-01,\n",
       "        4.54533994e-01, -2.18649991e-02,  7.99516022e-01, -2.60095000e-01,\n",
       "        4.09557015e-01, -2.70296007e-01,  6.35302007e-01,  4.70802993e-01,\n",
       "        6.65055990e-01,  2.57239014e-01,  2.89528012e-01,  2.26126999e-01,\n",
       "        9.54293013e-01,  2.33879000e-01, -1.23384997e-01,  1.89899996e-01,\n",
       "        4.15013999e-01,  2.75500007e-02,  5.35710990e-01,  8.23270008e-02])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:33:56.063975Z",
     "start_time": "2019-09-21T19:33:56.059338Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:49:48.992595Z",
     "start_time": "2019-09-21T19:49:48.987335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['教育', '医疗卫生', '志愿服务', '扶贫及社区发展', '社会服务', '公益事业发展', '文化艺术', '生态环境',\n",
       "       '法律与公民权力', '体育', '政策倡导', '科学研究', '灾害救助'], dtype=object)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.service_area.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:52:00.493189Z",
     "start_time": "2019-09-21T19:52:00.399814Z"
    }
   },
   "outputs": [],
   "source": [
    "# token y\n",
    "y_tk = Tokenizer()\n",
    "y_tk.fit_on_texts(data.service_area.unique())\n",
    "index_list = y_tk.texts_to_sequences(data.service_area.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:52:48.035345Z",
     "start_time": "2019-09-21T19:52:48.029697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['教育', '医疗卫生', '志愿服务', '扶贫及社区发展', '社会服务', '公益事业发展', '文化艺术', '生态环境',\n",
       "       '法律与公民权力', '体育', '政策倡导', '科学研究', '灾害救助'], dtype=object)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.service_area.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:52:43.588310Z",
     "start_time": "2019-09-21T19:52:43.582986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.service_area.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pad_sequences(index_list, maxlen=max([len(s) for s in index_list]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:44:33.860225Z",
     "start_time": "2019-09-21T19:44:33.840004Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data_sequences, data.service_area.to_list(), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T19:44:36.243192Z",
     "start_time": "2019-09-21T19:44:36.076883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 919, 300)          11999100  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 915, 128)          192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 12,197,246\n",
      "Trainable params: 198,146\n",
      "Non-trainable params: 11,999,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-17c41c0c4a3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "\n",
    "# with tf.device('/gpu:1'): # Specify which GPU to use.\n",
    "#     # define the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "# model.add(Flatten())\n",
    "model.add(Conv1D(128, 5, activation='softplus'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(units=32, activation='sigmoid'))\n",
    "model.add(Dense(units=32, activation='softplus'))\n",
    "model.add(Dense(units=16, activation='tanh'))\n",
    "model.add(Dense(units=16, activation='softplus'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \n",
    "#                                                                      precision, recall\n",
    "                                                                    ])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "history=model.fit(x_train, y_train, validation_split=0.2, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "df_history=pd.read_csv('../../output/chinese_grid_search_history_broad_cat.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [32, 64, 128]:\n",
    "    for kernel_size in [3,5,7]:\n",
    "        for conv_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:0'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_broad_cat.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
