{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "#Can't use keras on 129.114.32.33:8000\n",
    "#from keras import layers, models, optimizers\n",
    "import datetime, os\n",
    "import statistics, tqdm\n",
    "\n",
    "#to-do list\n",
    "#1. Record the amount of time a classifier takes: start(timestamp) - end(timestamp), put it in .pkl.gz\n",
    "\n",
    "#Observations:\n",
    "# 1. RF gives highest accuracy, but takes a lot of time to train: 25 minutes and 15 minutes\n",
    "# 2. Neural Network is weakest\n",
    "# 3. NB gives satisfactory results within a minute.\n",
    "\n",
    "trainDF = pd.concat([pd.read_pickle('../../data/2015/MasterData_2015.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2014/MasterData_2014.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2013/MasterData_2013.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/2012/MasterData_2012.pkl.gz')])\n",
    "                                   \n",
    "trainDF = trainDF[trainDF.TEXT.notna() & trainDF.NTEE.notna()]\n",
    "trainDF['text'] = trainDF['TEXT'].astype(str)\n",
    "trainDF['label'] = trainDF['NTEE'].astype(str)\n",
    "\n",
    "counts = trainDF['label'].value_counts().sort_index().to_frame()\n",
    "counts['category'] = counts.index\n",
    "counts['train_sample']=(counts['label']/2).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor rec in counts.values:\\n    print('class & samplesize: ', rec[1], rec[2]) \\n    \\n    train=trainDF.loc[trainDF['label']==rec[1]]\\n    accuracy, precision, recall = [[],[],[]], [[],[],[]], [[],[],[]]\\n    results = [[rec[1], rec[2]]]\\n    for iterate in range(0, 10):\\n        train = train.sample(n=rec[2])\\n        \\n        result=dataformodel(train)\\n        results.append(result)\\n        for j in range(0,3):\\n            accuracy[j].append(result[j])\\n            precision[j].append(result[j+3])\\n            recall[j].append(result[j+6])\\n            \\n    stats = [[], [], []]                                                                                                                                        \\n    for j in range(0,3):\\n        stats[0].append([statistics.mean(accuracy[j]), statistics.stdev(accuracy[j])])                                                                                                                                     \\n        stats[1].append([statistics.mean(precision[j]), statistics.stdev(precision[j])])                                                                                                                                             \\n        stats[2].append([statistics.mean(recall[j]), statistics.stdev(recall[j])])                                                                                                                                         \\n    \\n    stats = pd.DataFrame(stats)                                                                                                                                                             \\n    if(os.path.exists('../../data/results/classifier_stats_10.pkl.gz')):\\n        stats = pd.concat([pd.read_pickle('../../data/results/classifier_stats_10.pkl.gz'), stats]).drop_duplicates()\\n    \\n    stats.to_pickle('../../data/results/classifier_stats_10.pkl.gz')\\n\\n    results = pd.DataFrame(results, columns=['acNB','acRF', 'acNN', 'prNB', 'prRF', 'prNN', 'rcNB', 'rcRF', 'rcNN'])\\n    if(os.path.exists('../../data/results/classifier_results_10.pkl.gz')):\\n        results = pd.concat([pd.read_pickle('../../data/results/classifier_results_10.pkl.gz'), results]).drop_duplicates()\\n    \\n    results.to_pickle('../../data/results/classifier_results_10.pkl.gz')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataformodel(trainDF):\n",
    "    # split the dataset into training and validation datasets \n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    count_vect.fit(trainDF['text'])\n",
    "\n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)  \n",
    "    \n",
    "    # word level tf-idf\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "    \n",
    "    def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "        if is_neural_net:\n",
    "            predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "        return metrics.accuracy_score(predictions, valid_y), metrics.precision_score(predictions, valid_y, average='weighted'), metrics.recall_score(predictions, valid_y, average='weighted')\n",
    "\n",
    "\n",
    "    # Naive Bayes on Count Vectors\n",
    "    acNB, prNB, rcNB = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "    \n",
    "    # RF on Count Vectors\n",
    "    acRF, prRF, rcRF = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    \n",
    "    acNN, prNN, rcNN = 0, 0, 0\n",
    "    return [acNB, acRF, acNN, prNB, prRF, prNN, rcNB, rcRF, rcNN]\n",
    "    \n",
    "'''\n",
    "for rec in counts.values:\n",
    "    print('class & samplesize: ', rec[1], rec[2]) \n",
    "    \n",
    "    train=trainDF.loc[trainDF['label']==rec[1]]\n",
    "    accuracy, precision, recall = [[],[],[]], [[],[],[]], [[],[],[]]\n",
    "    results = [[rec[1], rec[2]]]\n",
    "    for iterate in range(0, 10):\n",
    "        train = train.sample(n=rec[2])\n",
    "        \n",
    "        result=dataformodel(train)\n",
    "        results.append(result)\n",
    "        for j in range(0,3):\n",
    "            accuracy[j].append(result[j])\n",
    "            precision[j].append(result[j+3])\n",
    "            recall[j].append(result[j+6])\n",
    "            \n",
    "    stats = [[], [], []]                                                                                                                                        \n",
    "    for j in range(0,3):\n",
    "        stats[0].append([statistics.mean(accuracy[j]), statistics.stdev(accuracy[j])])                                                                                                                                     \n",
    "        stats[1].append([statistics.mean(precision[j]), statistics.stdev(precision[j])])                                                                                                                                             \n",
    "        stats[2].append([statistics.mean(recall[j]), statistics.stdev(recall[j])])                                                                                                                                         \n",
    "    \n",
    "    stats = pd.DataFrame(stats)                                                                                                                                                             \n",
    "    if(os.path.exists('../../data/results/classifier_stats_10.pkl.gz')):\n",
    "        stats = pd.concat([pd.read_pickle('../../data/results/classifier_stats_10.pkl.gz'), stats]).drop_duplicates()\n",
    "    \n",
    "    stats.to_pickle('../../data/results/classifier_stats_10.pkl.gz')\n",
    "\n",
    "    results = pd.DataFrame(results, columns=['acNB','acRF', 'acNN', 'prNB', 'prRF', 'prNN', 'rcNB', 'rcRF', 'rcNN'])\n",
    "    if(os.path.exists('../../data/results/classifier_results_10.pkl.gz')):\n",
    "        results = pd.concat([pd.read_pickle('../../data/results/classifier_results_10.pkl.gz'), results]).drop_duplicates()\n",
    "    \n",
    "    results.to_pickle('../../data/results/classifier_results_10.pkl.gz')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6410256410256411, 0.7986880622863887, 0, 0.6828830315189934, 0.8123224353285377, 0, 0.6410256410256411, 0.7986880622863887, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6383895044982911, 0.799147853541159, 0, 0.6811789813199854, 0.8130791382853436, 0, 0.6383895044982911, 0.799147853541159, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6420525081612948, 0.7997455821723605, 0, 0.6821915921528454, 0.8123067075425511, 0, 0.6420525081612948, 0.7997455821723605, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6424969730409061, 0.7998222140481555, 0, 0.6842767805085773, 0.814511832396387, 0, 0.6424969730409061, 0.7998222140481555, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6394163716339448, 0.8003586371787209, 0, 0.6813122065737748, 0.8127681735142557, 0, 0.6394163716339448, 0.8003586371787209, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6413781476542982, 0.8001287415513357, 0, 0.683061102920407, 0.8140688623278014, 0, 0.6413781476542982, 0.8001287415513357, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.636887519732708, 0.8007571229328552, 0, 0.6794452143000647, 0.8133867564659337, 0, 0.636887519732708, 0.8007571229328552, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6438763468052171, 0.8012322405627845, 0, 0.6843477145260187, 0.8140909957328735, 0, 0.6438763468052171, 0.8012322405627845, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6416080432816834, 0.7960979048845158, 0, 0.6840611163836396, 0.8105175824395545, 0, 0.6416080432816834, 0.7960979048845158, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jupyter/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.638328198997655, 0.8010636504360353, 0, 0.6800651457513969, 0.814535077747628, 0, 0.638328198997655, 0.8010636504360353, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall = [[],[],[]], [[],[],[]], [[],[],[]]\n",
    "results = []\n",
    "for iterate in range(0,10): \n",
    "    \n",
    "    train = pd.DataFrame()\n",
    "    for rec in counts.values:\n",
    "        train = pd.concat([train, trainDF.loc[trainDF['label']==rec[1]].sample(n=rec[2])])\n",
    "        \n",
    "    result=dataformodel(train)\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "    for j in range(0,3):\n",
    "        accuracy[j].append(result[j])\n",
    "        precision[j].append(result[j+3])\n",
    "        recall[j].append(result[j+6])\n",
    "            \n",
    "stats = [[], [], []]                                                                                                                                        \n",
    "for j in range(0,3):\n",
    "    stats[0].append([statistics.mean(accuracy[j]), statistics.stdev(accuracy[j])])                                                                                                                                     \n",
    "    stats[1].append([statistics.mean(precision[j]), statistics.stdev(precision[j])])                                                                                                                                             \n",
    "    stats[2].append([statistics.mean(recall[j]), statistics.stdev(recall[j])])                                                                                                                                         \n",
    "    \n",
    "stats = pd.DataFrame(stats)                                                                                                                                                             \n",
    "if(os.path.exists('../../data/results/classifier_stats_10.pkl.gz')):\n",
    "    stats = pd.concat([pd.read_pickle('../../data/results/classifier_stats_10.pkl.gz'), stats]).drop_duplicates()\n",
    "    \n",
    "stats.to_pickle('../../data/results/classifier_stats_10.pkl.gz')\n",
    "\n",
    "results = pd.DataFrame(results, columns=['acNB','acRF', 'acNN', 'prNB', 'prRF', 'prNN', 'rcNB', 'rcRF', 'rcNN'])\n",
    "if(os.path.exists('../../data/results/classifier_results_10.pkl.gz')):\n",
    "    results = pd.concat([pd.read_pickle('../../data/results/classifier_results_10.pkl.gz'), results]).drop_duplicates()\n",
    "    \n",
    "results.to_pickle('../../data/results/classifier_results_10.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
