{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script purpose\n",
    "\n",
    "- Replicate and compare with: Anastasopoulos, L. Jason, and Andrew B. Whitford. 2019. “Machine Learning for Public Administration Research, With Application to Organizational Reputation.” Journal of Public Administration Research and Theory 29 (3): 491–510. https://doi.org/10.1093/jopart/muy060.\n",
    "- Use glove-twitter-100 as embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# obtain reproducible results\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from keras import backend as K\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "# Rest of code follows ...\n",
    "\n",
    "# Check GPU device.\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot using Plotly.\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HITID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Answer1</th>\n",
       "      <th>Answer2</th>\n",
       "      <th>Agreement</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Date</th>\n",
       "      <th>JasonCode</th>\n",
       "      <th>moral_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>3AXFSPQOYQYNNLFNVXHLO2FHNRHFJ3</td>\n",
       "      <td>RT NWSNewOrleans Our 6ft 6in MIC next to 11ft ...</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>Performative Reputation</td>\n",
       "      <td>No</td>\n",
       "      <td>no_agreement</td>\n",
       "      <td>2018-07-06 18:03:42 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>3N5YJ55YXG3OAKP0ZFNL38L7ZV9ANP</td>\n",
       "      <td>RT fema Storm surge can be one of the most dan...</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>Technical Reputation</td>\n",
       "      <td>No</td>\n",
       "      <td>no_agreement</td>\n",
       "      <td>2018-07-06 17:59:34 UTC</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35A1YQPVFEGZQD2S73JCQP94LIEI5B</td>\n",
       "      <td>RT statedeptspox Thank you  Dr Alison Mann fro...</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>2018-07-06 17:59:48 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3TX9T2ZCB91FYM6M38U7GKP71SJWZ2</td>\n",
       "      <td>CommerceGov announces preliminary countervaili...</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>Performative Reputation</td>\n",
       "      <td>No</td>\n",
       "      <td>no_agreement</td>\n",
       "      <td>2018-07-06 18:04:45 UTC</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>3GVPRXWRPHUEDHGBVWY9O9N0CO97IF</td>\n",
       "      <td>Whos a good boy?A USAirForce airman rewards Ni...</td>\n",
       "      <td>Moral Reputation</td>\n",
       "      <td>Technical Reputation</td>\n",
       "      <td>No</td>\n",
       "      <td>no_agreement</td>\n",
       "      <td>2018-07-06 18:08:55 UTC</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              HITID  \\\n",
       "65   3AXFSPQOYQYNNLFNVXHLO2FHNRHFJ3   \n",
       "114  3N5YJ55YXG3OAKP0ZFNL38L7ZV9ANP   \n",
       "16   35A1YQPVFEGZQD2S73JCQP94LIEI5B   \n",
       "141  3TX9T2ZCB91FYM6M38U7GKP71SJWZ2   \n",
       "156  3GVPRXWRPHUEDHGBVWY9O9N0CO97IF   \n",
       "\n",
       "                                                  Text             Answer1  \\\n",
       "65   RT NWSNewOrleans Our 6ft 6in MIC next to 11ft ...  None of the above.   \n",
       "114  RT fema Storm surge can be one of the most dan...  None of the above.   \n",
       "16   RT statedeptspox Thank you  Dr Alison Mann fro...  None of the above.   \n",
       "141  CommerceGov announces preliminary countervaili...  None of the above.   \n",
       "156  Whos a good boy?A USAirForce airman rewards Ni...    Moral Reputation   \n",
       "\n",
       "                     Answer2 Agreement              Answer  \\\n",
       "65   Performative Reputation        No        no_agreement   \n",
       "114     Technical Reputation        No        no_agreement   \n",
       "16        None of the above.       Yes  None of the above.   \n",
       "141  Performative Reputation        No        no_agreement   \n",
       "156     Technical Reputation        No        no_agreement   \n",
       "\n",
       "                        Date  JasonCode  moral_bin  \n",
       "65   2018-07-06 18:03:42 UTC          0          0  \n",
       "114  2018-07-06 17:59:34 UTC          2          1  \n",
       "16   2018-07-06 17:59:48 UTC          0          0  \n",
       "141  2018-07-06 18:04:45 UTC          1          0  \n",
       "156  2018-07-06 18:08:55 UTC          2          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coded_tweet=pd.read_csv('../../dataset/muy060_suppl_supplementary_appendix/muy060_Appendix_2.csv')\n",
    "df_coded_tweet.loc[df_coded_tweet.JasonCode==2, 'moral_bin']=1\n",
    "df_coded_tweet.moral_bin.fillna('0', inplace=True)\n",
    "df_coded_tweet['moral_bin']=[int(s) for s in df_coded_tweet.moral_bin]\n",
    "df_coded_tweet.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_favorites</th>\n",
       "      <th>tweet_retweets</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10430</th>\n",
       "      <td>Interior</td>\n",
       "      <td>RT @SecretaryZinke: HUGE!  Latest @USGS and @B...</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>1.513968e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11393</th>\n",
       "      <td>HUDgov</td>\n",
       "      <td>RT @HUDSoutheast: Celebrating #CollegeSigningD...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.461698e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17471</th>\n",
       "      <td>ENERGY</td>\n",
       "      <td>Former Energy Secretary @ErnestMoniz and @MIT ...</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>1.510787e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      agency_id                                         tweet_text  \\\n",
       "10430  Interior  RT @SecretaryZinke: HUGE!  Latest @USGS and @B...   \n",
       "11393    HUDgov  RT @HUDSoutheast: Celebrating #CollegeSigningD...   \n",
       "17471    ENERGY  Former Energy Secretary @ErnestMoniz and @MIT ...   \n",
       "\n",
       "       tweet_favorites  tweet_retweets  tweet_created  \n",
       "10430                0             103   1.513968e+09  \n",
       "11393                0               1   1.461698e+09  \n",
       "17471               18              10   1.510787e+09  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agency_tweets_database=pd.read_csv('../../dataset/muy060_suppl_supplementary_appendix/muy060_Appendix_1.csv', encoding='ISO-8859-1')\n",
    "df_agency_tweets_database.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=df_coded_tweet['Text']\n",
    "text_token_list_pred=df_agency_tweets_database['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "lb.fit(df_coded_tweet.moral_bin.unique())\n",
    "y_train=lb.transform(df_coded_tweet['moral_bin'])\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https', 1), ('t', 2), ('co', 3), ('the', 4), ('to', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_pred.to_list()) # Including uncoded tweets, vocabulary must be comprehensive.\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train dataset.\n",
    "# Text to sequences.\n",
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_pred=tokenizer.texts_to_sequences(text_token_list_pred)\n",
    "\n",
    "# Pads sequences to the same length (i.e., prepare matrix).\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train+seq_encoding_text_pred]), # Max length of the sequence. Must consider prediction dataset.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "x_pred=pad_sequences(sequences=seq_encoding_text_pred,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train+seq_encoding_text_pred]), # Max length of the sequence. Must consider prediction dataset.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "# glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))\n",
    "glove_word_vector=api.load('glove-twitter-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM)) # Plus one: embedding matrix starts from 0, word index starts from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_length=len(x_train[0])\n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=input_length, # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "num_filters=[32, 64, 128]\n",
    "kernel_size=[3,5,7]\n",
    "conv_act=['sigmoid', 'softplus', 'tanh', 'softmax']\n",
    "act_32=['sigmoid', 'softplus', 'tanh', 'softmax']\n",
    "act_16=['sigmoid', 'softplus', 'tanh', 'softmax']\n",
    "out_act=['sigmoid', 'softplus', 'tanh', 'softmax']\n",
    "param_list=list(itertools.product(num_filters, kernel_size, conv_act, act_32, act_16, out_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 332/2304 [17:08<1:41:43,  3.10s/it]"
     ]
    }
   ],
   "source": [
    "result_list=[]\n",
    "\n",
    "for num_filters, kernel_size, conv_act, act_32, act_16, out_act in tqdm(param_list):\n",
    "#     with tf.device('/gpu:0'): # Small dataset, may even increase time because of data transfer between C/GPU.\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation=act_32))\n",
    "    model.add(Dense(units=16, activation=act_16))\n",
    "    model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.3, epochs=100, verbose=0)\n",
    "    result_list+=[[max(history.history['val_acc']), [num_filters, kernel_size, conv_act, act_32, act_16, out_act]]]\n",
    "    ############### Clean up and rebuid objects; otherwise, tensor graph becomes larger and larger and requires more time. ##########\n",
    "    del model\n",
    "    K.clear_session() # Will cause error: Tensor must from the same graph. Cause: Embedding layers changed. Solution: rebuild embedding layer.\n",
    "    ##### Initialize session for reproducibility #####\n",
    "    np.random.seed(42)\n",
    "    rn.seed(12345)\n",
    "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                                  inter_op_parallelism_threads=1)\n",
    "    tf.set_random_seed(1234)\n",
    "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)\n",
    "    ##################################################\n",
    "    # Rebuid embedding layer.\n",
    "    embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                                input_length=input_length, # Length of input, i.e., length of padded sequence.\n",
    "                                output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "    #################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result_list).sort_values(0, ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result_list).sort_values(0, ascending=False).to_csv('../../output/muy060_grid_search_glove-tweet.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Clean up and rebuid objects; otherwise, tensor graph becomes larger and larger and requires more time. ##########\n",
    "K.clear_session() # Will cause error: Tensor must from the same graph. Cause: Embedding layers changed. Solution: rebuild embedding layer.\n",
    "##### Initialize session for reproducibility #####\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "##################################################\n",
    "# Rebuid embedding layer.\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=input_length, # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "#################################################################################################################################\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "# model.add(Flatten())\n",
    "model.add(Conv1D(64, 5, activation='softmax'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(units=32, activation='softplus'))\n",
    "model.add(Dense(units=16, activation='sigmoid'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "# fit the model\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=48, verbose=1)\n",
    "max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob=model.predict(x_pred)\n",
    "y_pred=lb.inverse_transform(np_utils.to_categorical(y_prob.argmax(axis=-1))).tolist()\n",
    "\n",
    "df_agency_tweets_database['moral_pred']=y_pred\n",
    "\n",
    "df_moral_stats=df_agency_tweets_database.groupby('agency_id').count()[['tweet_text']].merge(df_agency_tweets_database.groupby('agency_id').sum()[['moral_pred']], left_index=True, right_index=True, how='left')\n",
    "df_moral_stats['moral_ptg']=df_moral_stats['moral_pred']/df_moral_stats['tweet_text']\n",
    "df_moral_stats['ci_95']=np.sqrt(df_moral_stats.moral_ptg*(1-df_moral_stats.moral_ptg)/df_moral_stats.tweet_text)*1.96\n",
    "df_moral_stats.sort_values('moral_ptg', ascending=False, inplace=True)\n",
    "df_moral_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(df_moral_stats.reset_index().sort_values('moral_ptg'), x='moral_ptg', y='agency_id', error_x='ci_95',\n",
    "               labels=dict(agency_id='Agency', moral_ptg='% Moral Reputation Tweets')\n",
    "              )\n",
    "fig.update_layout(width=600, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traps.\n",
    "\n",
    "- Tried GridSearchCV, suffers from the \"tensor graph becomes larger and larger\" problem.\n",
    "\n",
    "<s>\n",
    "    \n",
    "```Python\n",
    "def create_model(num_filters, kernel_size, conv_act, act_first, act_second, out_act):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, activation=conv_act))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation=act_first))\n",
    "    model.add(Dense(units=16, activation=act_second))\n",
    "    model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "neural_network = KerasClassifier(build_fn=create_model, validation_split=0.3, verbose=0)\n",
    "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "grid_result = grid.fit(x_train, y_train, verbose=1)\n",
    "```\n",
    "    \n",
    "</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Give up trying multiprocessing: fail to send embedding layer (a pickle object) to threads.\n",
    "    - possible solution: buid embedding layer on remote engines/threads.\n",
    "        - Tried, looks like not working. Since it now only takes about 1.5 hours to complete the job, give up, move on.\n",
    "\n",
    "<s>\n",
    "\n",
    "```Python\n",
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "print(c.ids)\n",
    "dview = c[:]\n",
    "\n",
    "dview['Sequential']=Sequential\n",
    "dview['embedding_layer']=embedding_layer\n",
    "dview['Conv1D']=Conv1D\n",
    "dview['GlobalMaxPool1D']=GlobalMaxPool1D\n",
    "dview['Dense']=Dense\n",
    "\n",
    "@dview.parallel(block=True)\n",
    "def grid_search(params):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(params[0], params[1], activation=params[2]))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation=params[3]))\n",
    "    model.add(Dense(units=16, activation=params[4]))\n",
    "    model.add(Dense(units=len(y_train[0]), activation=params[5]))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.3, epochs=100, verbose=0)\n",
    "    return [[max(history.history['val_acc']), [num_filters, kernel_size, conv_act, act_32, act_16, out_act]]]\n",
    "\n",
    "grid_search.map(param_list)\n",
    "\n",
    "def grid_search(params):\n",
    "    with tf.device('/gpu:0'): # Specify which GPU to use.\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        # model.add(Flatten())\n",
    "        model.add(Conv1D(params[0], params[1], activation=params[2]))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dense(units=32, activation=params[3]))\n",
    "        model.add(Dense(units=16, activation=params[4]))\n",
    "        model.add(Dense(units=len(y_train[0]), activation=params[5]))\n",
    "        # compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "        # fit the model\n",
    "        history=model.fit(x_train, y_train, validation_split=0.3, epochs=100, verbose=0)\n",
    "    return [[max(history.history['val_acc']), [num_filters, kernel_size, conv_act, act_32, act_16, out_act]]]\n",
    "\n",
    "from multiprocessing import Pool\n",
    "# if __name__== \"__main__\":\n",
    "p=Pool(10)\n",
    "result_list=p.map(grid_search, param_list)\n",
    "```\n",
    "\n",
    "</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
