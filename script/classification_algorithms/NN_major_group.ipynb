{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check GPU device.\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "# Specify GPU to use. \n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"; \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and compile tranining and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229472, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc'] # Using raw.\n",
    "df_train['input_text']=df_train['mission_spellchk']+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_train['NTEE_M']=df_train['NTEE1'].apply(ntee2major)\n",
    "\n",
    "len(df_train['input_text']), len(df_train['NTEE1'].drop_duplicates()), len(df_train['NTEE_M'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training and testing data frame.\n",
    "small_num=0\n",
    "while small_num<500: # Make sure each category has at least 500 records.\n",
    "    sampleDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "    trainDF, valDF =train_test_split(sampleDF, test_size=.3)\n",
    "    small_num=trainDF.groupby('NTEE_M').count().sort_values('EIN').iloc[0]['EIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE1\n",
      "A    0.112142\n",
      "B    0.166717\n",
      "C    0.022450\n",
      "D    0.027600\n",
      "E    0.063658\n",
      "F    0.015350\n",
      "G    0.034408\n",
      "H    0.003142\n",
      "I    0.018700\n",
      "J    0.029150\n",
      "K    0.013042\n",
      "L    0.038308\n",
      "M    0.028850\n",
      "N    0.097517\n",
      "O    0.011075\n",
      "P    0.062925\n",
      "Q    0.013383\n",
      "R    0.006842\n",
      "S    0.091275\n",
      "T    0.014142\n",
      "U    0.005908\n",
      "V    0.002350\n",
      "W    0.053800\n",
      "X    0.028833\n",
      "Y    0.038433\n",
      "Name: EIN, dtype: float64 \n",
      "\n",
      " NTEE1\n",
      "A    0.111483\n",
      "B    0.167417\n",
      "C    0.022667\n",
      "D    0.028358\n",
      "E    0.063708\n",
      "F    0.014975\n",
      "G    0.034292\n",
      "H    0.003242\n",
      "I    0.019050\n",
      "J    0.029083\n",
      "K    0.012717\n",
      "L    0.038433\n",
      "M    0.029325\n",
      "N    0.097417\n",
      "O    0.010875\n",
      "P    0.063567\n",
      "Q    0.013225\n",
      "R    0.006967\n",
      "S    0.090208\n",
      "T    0.013867\n",
      "U    0.006000\n",
      "V    0.002342\n",
      "W    0.053867\n",
      "X    0.028325\n",
      "Y    0.038592\n",
      "Name: EIN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# See the composition by NTEE major groups.\n",
    "print(trainDF.groupby('NTEE1').count()['EIN']/len(trainDF), '\\n'*2, valDF.groupby('NTEE1').count()['EIN']/len(valDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label_list, class_list):\n",
    "    int_encoder=LabelEncoder().fit(class_list) # Build the encoder.\n",
    "    label_int_encoded=int_encoder.transform(label_list) # One-dimensional integer encoded.\n",
    "    return np_utils.to_categorical(label_int_encoded) # Multi-dimensional binary/one-hot encoded.\n",
    "\n",
    "y_train=one_hot(label_list=trainDF['NTEE1'], class_list=list(trainDF['NTEE1'].unique()))\n",
    "y_val=one_hot(label_list=valDF['NTEE1'], class_list=list(trainDF['NTEE1'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=stopwords.words('english')+list(string.punctuation)\n",
    "def tokenize_stopwords_remove(string):\n",
    "    global stop_list\n",
    "    return [s for s in nltk.word_tokenize(string) if s not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=trainDF['input_text'].apply(tokenize_stopwords_remove)\n",
    "text_token_list_val=valDF['input_text'].apply(tokenize_stopwords_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moved to preprocessing pipeline.**\n",
    "```Python\n",
    "# Spell check function. Return corrected word if unknown; return original word if known.\n",
    "def spellcheck(word_string_list):\n",
    "    return [SpellChecker().correction(word=s).upper() for s in word_string_list]\n",
    "\n",
    "# Parallel computing\n",
    "p = Pool(48)\n",
    "text_token_list_train=p.map(spellcheck, text_token_list_train)\n",
    "text_token_list_val=p.map(spellcheck, text_token_list_val)\n",
    "# Pool.map keep the original order of data passed to map.\n",
    "# https://stackoverflow.com/questions/41273960/python-3-does-pool-keep-the-original-order-of-data-passed-to-map\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1), ('the', 2), ('to', 3), ('of', 4), ('in', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_val.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_val=tokenizer.texts_to_sequences(text_token_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "x_val=pad_sequences(sequences=seq_encoding_text_val,\n",
    "                    maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                    dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                    value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@Petuum/embeddings-a-matrix-of-meaning-4de877c9aa27\n",
    "# Note that in the embedding matrix above, each row corresponds to a word and each column corresponds to a dimension (axis). \n",
    "# Typically, we store this in a dense fashion, where we have a list of words and row ID’s which map to the corresponding row of the matrix. \n",
    "# For the above example, we’d have the following list in addition to the matrix:\n",
    "# { hello: 0, there: 1, texas: 2, world: 3, … }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not using pre-trained embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index), # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=32, # Size of the vector space in which words will be embedded.\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(max([len(s) for s in seq_encoding_text_train]),), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=len(y_train[0]), activation='softmax')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics.\n",
    "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='sigmoid'))\n",
    "model.add(Dense(units=256, activation='sigmoid'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='relu'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', precision, recall])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# Batch size: https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pre-trained GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251685/251685 [00:00<00:00, 372840.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics.\n",
    "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n",
    "f1 = as_keras_metric(tf.contrib.metrics.f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth=True\n",
    "# sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 32070, 100)        25168600  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 32066, 128)        64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 25)                425       \n",
      "=================================================================\n",
      "Total params: 25,238,353\n",
      "Trainable params: 69,753\n",
      "Non-trainable params: 25,168,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 84000 samples, validate on 36000 samples\n",
      "Epoch 1/20\n",
      "84000/84000 [==============================] - 258s 3ms/step - loss: 0.1027 - acc: 0.9700 - val_loss: 0.0860 - val_acc: 0.9747\n",
      "Epoch 2/20\n",
      "84000/84000 [==============================] - 256s 3ms/step - loss: 0.0750 - acc: 0.9775 - val_loss: 0.0751 - val_acc: 0.9772\n",
      "Epoch 3/20\n",
      "84000/84000 [==============================] - 257s 3ms/step - loss: 0.0672 - acc: 0.9797 - val_loss: 0.0719 - val_acc: 0.9781\n",
      "Epoch 4/20\n",
      "84000/84000 [==============================] - 258s 3ms/step - loss: 0.0627 - acc: 0.9809 - val_loss: 0.0703 - val_acc: 0.9787\n",
      "Epoch 5/20\n",
      "84000/84000 [==============================] - 257s 3ms/step - loss: 0.0588 - acc: 0.9821 - val_loss: 0.0713 - val_acc: 0.9784\n",
      "Epoch 6/20\n",
      "84000/84000 [==============================] - 257s 3ms/step - loss: 0.0557 - acc: 0.9830 - val_loss: 0.0717 - val_acc: 0.9785\n",
      "Epoch 7/20\n",
      "25216/84000 [========>.....................] - ETA: 2:33 - loss: 0.0520 - acc: 0.9842"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "# model.add(Flatten())\n",
    "model.add(Conv1D(128, 5, activation='softplus'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(units=32, activation='sigmoid'))\n",
    "model.add(Dense(units=16, activation='softplus'))\n",
    "# model.add(PReLU()) # https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7\n",
    "model.add(Dense(units=16, activation='tanh'))\n",
    "model.add(Dense(units=16, activation='softplus'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \n",
    "#                                                                      precision, recall\n",
    "                                                                    ])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=20, verbose=1)\n",
    "\n",
    "'''\n",
    "(10, sigmoid; 9, relu): loss: 0.4414 - acc: 0.8896 - precision: 0.1604 - recall: 0.8439 - val_loss: 0.3964 - val_acc: 0.8943 - val_precision: 0.1632 - val_recall: 0.8974\n",
    "(10, softmax; 9, relu): loss: 0.5685 - acc: 0.8888 - precision: 0.1371 - recall: 0.8051 - val_loss: 0.5532 - val_acc: 0.8895 - val_precision: 0.1394 - val_recall: 0.8151\n",
    "(10, relu;    9, relu): loss: 0.5377 - acc: 0.8838 - precision: 0.1646 - recall: 0.8411 - val_loss: 0.4271 - val_acc: 0.8903 - val_precision: 0.1558 - val_recall: 0.8884\n",
    "(10, relu; 9, softmax): loss: 0.2596 - acc: 0.9037 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2303 - val_acc: 0.9135 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "(10, relu; 9, sigmoid): loss: 0.2681 - acc: 0.8975 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2272 - val_acc: 0.9121 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "(10, tanh; 9, sigmoid): loss: 0.2959 - acc: 0.8940 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2572 - val_acc: 0.9066 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "(10, relu;    9, tanh): loss: 0.4241 - acc: 0.8599 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.3609 - val_acc: 0.8885 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "\n",
    "(32, relu; 16, tanh; 9, sigmoid): loss: 0.2590 - acc: 0.9034 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2186 - val_acc: 0.9180 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "(32, relu; 16, tanh; 9,    relu): loss: 0.5613 - acc: 0.8654 - precision: 0.1360 - recall: 0.7180 - val_loss: 0.3850 - val_acc: 0.8889 - val_precision: 0.1394 - val_recall: 0.8721\n",
    "(32, relu; 16, relu; 9,    relu): loss: 0.3902 - acc: 0.8876 - precision: 0.1396 - recall: 0.9328 - val_loss: 0.3618 - val_acc: 0.8889 - val_precision: 0.1377 - val_recall: 0.9532\n",
    "(32, softmax; 16, relu; 9, relu): loss: 0.6418 - acc: 0.8916 - precision: 0.1212 - recall: 0.7585 - val_loss: 0.6097 - val_acc: 0.8942 - val_precision: 0.1276 - val_recall: 0.7584\n",
    "(32, sigmoid; 16, relu; 9, relu): loss: 0.5048 - acc: 0.8885 - precision: 0.1421 - recall: 0.7762 - val_loss: 0.3169 - val_acc: 0.8889 - val_precision: 0.1363 - val_recall: 0.8788\n",
    "\n",
    "(32, sigmoid; 32, sigmoid; 16, relu; 16, relu; 9, relu): loss: 0.3325 - acc: 0.8879 - precision: 0.1251 - recall: 0.9766 - val_loss: 0.4644 - val_acc: 0.7910 - val_precision: 0.1257 - val_recall: 0.9785\n",
    "(32, softmax; 32, softmax; 16, relu; 16, relu; 9, relu): loss: 1.1298 - acc: 0.8889 - precision: 0.0933 - recall: 0.4219 - val_loss: 1.1113 - val_acc: 0.8889 - val_precision: 0.0952 - val_recall: 0.4569\n",
    "\n",
    "1. Don't use sigmoid/softmax/tanh for output layer.\n",
    "2. Using relu near output layer increases loss but improve precision.\n",
    "3. sigmoid/tanh/softmax decreases loss, but also decreases precision.\n",
    "4. Possible strategy: use softmax near input layer, use relu near output layer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "history_dict = history.history\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000/120000 [==============================] - 126s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7410416666666667"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. \n",
    "The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 – probability) to give the probability for class 0.\n",
    "In the case of a multi-class classification problem, the softmax activation function \n",
    "is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n",
    "'''\n",
    "\n",
    "y_prob = model.predict(x_val, verbose=1)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)\n",
    "\n",
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 67s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.772"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. \n",
    "The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 – probability) to give the probability for class 0.\n",
    "In the case of a multi-class classification problem, the softmax activation function \n",
    "is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n",
    "'''\n",
    "\n",
    "y_prob = model.predict(x_val, verbose=1)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)\n",
    "\n",
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9346993456194288 0.4851916666666667\n"
     ]
    }
   ],
   "source": [
    "df_95=df_val[df_val.prob>.95]\n",
    "print(len(df_95[df_95.pred==df_95.true])/len(df_95), len(df_95)/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8247847099301924 0.8380166666666666\n"
     ]
    }
   ],
   "source": [
    "df_95=df_val[df_val.prob>.95]\n",
    "print(len(df_95[df_95.pred==df_95.true])/len(df_95), len(df_95)/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97857</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.491921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48309</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.445013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91307</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.998451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.858107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>0.429928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101330</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0.833755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21658</th>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>0.821795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35628</th>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>0.344335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.315280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred  true      prob\n",
       "97857      4     5  0.491921\n",
       "48309     10    16  0.445013\n",
       "91307     13    13  0.998451\n",
       "49997     23     0  0.392497\n",
       "4109      11    11  0.858107\n",
       "6911      23    16  0.429928\n",
       "101330    23    23  0.833755\n",
       "21658     22     8  0.821795\n",
       "35628     13    18  0.344335\n",
       "9273       9    12  0.315280"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13088.0</td>\n",
       "      <td>0.908388</td>\n",
       "      <td>0.177154</td>\n",
       "      <td>0.142027</td>\n",
       "      <td>0.939100</td>\n",
       "      <td>0.990128</td>\n",
       "      <td>0.995451</td>\n",
       "      <td>0.997781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18944.0</td>\n",
       "      <td>0.906150</td>\n",
       "      <td>0.169570</td>\n",
       "      <td>0.109747</td>\n",
       "      <td>0.923321</td>\n",
       "      <td>0.984253</td>\n",
       "      <td>0.993402</td>\n",
       "      <td>0.997820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3171.0</td>\n",
       "      <td>0.813541</td>\n",
       "      <td>0.216008</td>\n",
       "      <td>0.148135</td>\n",
       "      <td>0.674415</td>\n",
       "      <td>0.921770</td>\n",
       "      <td>0.982658</td>\n",
       "      <td>0.997292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3552.0</td>\n",
       "      <td>0.906009</td>\n",
       "      <td>0.165231</td>\n",
       "      <td>0.144918</td>\n",
       "      <td>0.925063</td>\n",
       "      <td>0.983874</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.996258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7536.0</td>\n",
       "      <td>0.871013</td>\n",
       "      <td>0.191857</td>\n",
       "      <td>0.155154</td>\n",
       "      <td>0.815629</td>\n",
       "      <td>0.978425</td>\n",
       "      <td>0.995479</td>\n",
       "      <td>0.998737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2036.0</td>\n",
       "      <td>0.629566</td>\n",
       "      <td>0.210578</td>\n",
       "      <td>0.134771</td>\n",
       "      <td>0.461040</td>\n",
       "      <td>0.634257</td>\n",
       "      <td>0.816017</td>\n",
       "      <td>0.970404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5651.0</td>\n",
       "      <td>0.699703</td>\n",
       "      <td>0.224970</td>\n",
       "      <td>0.136998</td>\n",
       "      <td>0.523264</td>\n",
       "      <td>0.768967</td>\n",
       "      <td>0.898311</td>\n",
       "      <td>0.977922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>106.0</td>\n",
       "      <td>0.448607</td>\n",
       "      <td>0.104133</td>\n",
       "      <td>0.203171</td>\n",
       "      <td>0.380574</td>\n",
       "      <td>0.458627</td>\n",
       "      <td>0.506585</td>\n",
       "      <td>0.690596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2401.0</td>\n",
       "      <td>0.645444</td>\n",
       "      <td>0.224147</td>\n",
       "      <td>0.128084</td>\n",
       "      <td>0.463254</td>\n",
       "      <td>0.682072</td>\n",
       "      <td>0.849529</td>\n",
       "      <td>0.966446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3222.0</td>\n",
       "      <td>0.789126</td>\n",
       "      <td>0.219737</td>\n",
       "      <td>0.145655</td>\n",
       "      <td>0.659721</td>\n",
       "      <td>0.900570</td>\n",
       "      <td>0.960045</td>\n",
       "      <td>0.980046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1939.0</td>\n",
       "      <td>0.728381</td>\n",
       "      <td>0.222038</td>\n",
       "      <td>0.149119</td>\n",
       "      <td>0.541646</td>\n",
       "      <td>0.781846</td>\n",
       "      <td>0.934978</td>\n",
       "      <td>0.986202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4135.0</td>\n",
       "      <td>0.790978</td>\n",
       "      <td>0.196248</td>\n",
       "      <td>0.162105</td>\n",
       "      <td>0.668983</td>\n",
       "      <td>0.868191</td>\n",
       "      <td>0.950709</td>\n",
       "      <td>0.991070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3557.0</td>\n",
       "      <td>0.900789</td>\n",
       "      <td>0.181403</td>\n",
       "      <td>0.146918</td>\n",
       "      <td>0.924903</td>\n",
       "      <td>0.988990</td>\n",
       "      <td>0.995205</td>\n",
       "      <td>0.998159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11227.0</td>\n",
       "      <td>0.923705</td>\n",
       "      <td>0.155608</td>\n",
       "      <td>0.121359</td>\n",
       "      <td>0.952169</td>\n",
       "      <td>0.992989</td>\n",
       "      <td>0.996704</td>\n",
       "      <td>0.998610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1098.0</td>\n",
       "      <td>0.619019</td>\n",
       "      <td>0.235930</td>\n",
       "      <td>0.151741</td>\n",
       "      <td>0.414210</td>\n",
       "      <td>0.637582</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>0.979060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6912.0</td>\n",
       "      <td>0.667097</td>\n",
       "      <td>0.221585</td>\n",
       "      <td>0.126351</td>\n",
       "      <td>0.483805</td>\n",
       "      <td>0.693039</td>\n",
       "      <td>0.869978</td>\n",
       "      <td>0.988523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1593.0</td>\n",
       "      <td>0.472803</td>\n",
       "      <td>0.190746</td>\n",
       "      <td>0.126787</td>\n",
       "      <td>0.315705</td>\n",
       "      <td>0.438267</td>\n",
       "      <td>0.619603</td>\n",
       "      <td>0.911286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>580.0</td>\n",
       "      <td>0.415268</td>\n",
       "      <td>0.151513</td>\n",
       "      <td>0.118035</td>\n",
       "      <td>0.300510</td>\n",
       "      <td>0.392174</td>\n",
       "      <td>0.518703</td>\n",
       "      <td>0.832576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11235.0</td>\n",
       "      <td>0.809223</td>\n",
       "      <td>0.232173</td>\n",
       "      <td>0.145952</td>\n",
       "      <td>0.659935</td>\n",
       "      <td>0.938022</td>\n",
       "      <td>0.990062</td>\n",
       "      <td>0.996336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>944.0</td>\n",
       "      <td>0.463488</td>\n",
       "      <td>0.184629</td>\n",
       "      <td>0.132051</td>\n",
       "      <td>0.315304</td>\n",
       "      <td>0.434593</td>\n",
       "      <td>0.594496</td>\n",
       "      <td>0.901521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>165.0</td>\n",
       "      <td>0.345842</td>\n",
       "      <td>0.100687</td>\n",
       "      <td>0.150297</td>\n",
       "      <td>0.276739</td>\n",
       "      <td>0.344555</td>\n",
       "      <td>0.402169</td>\n",
       "      <td>0.657184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7654.0</td>\n",
       "      <td>0.842533</td>\n",
       "      <td>0.233198</td>\n",
       "      <td>0.135629</td>\n",
       "      <td>0.744700</td>\n",
       "      <td>0.983917</td>\n",
       "      <td>0.995033</td>\n",
       "      <td>0.997171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4672.0</td>\n",
       "      <td>0.700134</td>\n",
       "      <td>0.241582</td>\n",
       "      <td>0.148612</td>\n",
       "      <td>0.486153</td>\n",
       "      <td>0.778218</td>\n",
       "      <td>0.924833</td>\n",
       "      <td>0.977141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4582.0</td>\n",
       "      <td>0.821450</td>\n",
       "      <td>0.203442</td>\n",
       "      <td>0.148625</td>\n",
       "      <td>0.718156</td>\n",
       "      <td>0.917286</td>\n",
       "      <td>0.977041</td>\n",
       "      <td>0.995323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0     13088.0  0.908388  0.177154  0.142027  0.939100  0.990128  0.995451   \n",
       "1     18944.0  0.906150  0.169570  0.109747  0.923321  0.984253  0.993402   \n",
       "2      3171.0  0.813541  0.216008  0.148135  0.674415  0.921770  0.982658   \n",
       "3      3552.0  0.906009  0.165231  0.144918  0.925063  0.983874  0.991304   \n",
       "4      7536.0  0.871013  0.191857  0.155154  0.815629  0.978425  0.995479   \n",
       "5      2036.0  0.629566  0.210578  0.134771  0.461040  0.634257  0.816017   \n",
       "6      5651.0  0.699703  0.224970  0.136998  0.523264  0.768967  0.898311   \n",
       "7       106.0  0.448607  0.104133  0.203171  0.380574  0.458627  0.506585   \n",
       "8      2401.0  0.645444  0.224147  0.128084  0.463254  0.682072  0.849529   \n",
       "9      3222.0  0.789126  0.219737  0.145655  0.659721  0.900570  0.960045   \n",
       "10     1939.0  0.728381  0.222038  0.149119  0.541646  0.781846  0.934978   \n",
       "11     4135.0  0.790978  0.196248  0.162105  0.668983  0.868191  0.950709   \n",
       "12     3557.0  0.900789  0.181403  0.146918  0.924903  0.988990  0.995205   \n",
       "13    11227.0  0.923705  0.155608  0.121359  0.952169  0.992989  0.996704   \n",
       "14     1098.0  0.619019  0.235930  0.151741  0.414210  0.637582  0.841121   \n",
       "15     6912.0  0.667097  0.221585  0.126351  0.483805  0.693039  0.869978   \n",
       "16     1593.0  0.472803  0.190746  0.126787  0.315705  0.438267  0.619603   \n",
       "17      580.0  0.415268  0.151513  0.118035  0.300510  0.392174  0.518703   \n",
       "18    11235.0  0.809223  0.232173  0.145952  0.659935  0.938022  0.990062   \n",
       "19      944.0  0.463488  0.184629  0.132051  0.315304  0.434593  0.594496   \n",
       "20      165.0  0.345842  0.100687  0.150297  0.276739  0.344555  0.402169   \n",
       "22     7654.0  0.842533  0.233198  0.135629  0.744700  0.983917  0.995033   \n",
       "23     4672.0  0.700134  0.241582  0.148612  0.486153  0.778218  0.924833   \n",
       "24     4582.0  0.821450  0.203442  0.148625  0.718156  0.917286  0.977041   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     0.997781  \n",
       "1     0.997820  \n",
       "2     0.997292  \n",
       "3     0.996258  \n",
       "4     0.998737  \n",
       "5     0.970404  \n",
       "6     0.977922  \n",
       "7     0.690596  \n",
       "8     0.966446  \n",
       "9     0.980046  \n",
       "10    0.986202  \n",
       "11    0.991070  \n",
       "12    0.998159  \n",
       "13    0.998610  \n",
       "14    0.979060  \n",
       "15    0.988523  \n",
       "16    0.911286  \n",
       "17    0.832576  \n",
       "18    0.996336  \n",
       "19    0.901521  \n",
       "20    0.657184  \n",
       "22    0.997171  \n",
       "23    0.977141  \n",
       "24    0.995323  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12602.0</td>\n",
       "      <td>0.848058</td>\n",
       "      <td>0.192906</td>\n",
       "      <td>0.099420</td>\n",
       "      <td>0.793265</td>\n",
       "      <td>0.942826</td>\n",
       "      <td>0.977441</td>\n",
       "      <td>0.995070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26000.0</td>\n",
       "      <td>0.809861</td>\n",
       "      <td>0.238692</td>\n",
       "      <td>0.087219</td>\n",
       "      <td>0.683788</td>\n",
       "      <td>0.946383</td>\n",
       "      <td>0.980377</td>\n",
       "      <td>0.993897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2297.0</td>\n",
       "      <td>0.605833</td>\n",
       "      <td>0.191890</td>\n",
       "      <td>0.094463</td>\n",
       "      <td>0.444121</td>\n",
       "      <td>0.639296</td>\n",
       "      <td>0.778246</td>\n",
       "      <td>0.894880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3366.0</td>\n",
       "      <td>0.874512</td>\n",
       "      <td>0.173787</td>\n",
       "      <td>0.157057</td>\n",
       "      <td>0.883541</td>\n",
       "      <td>0.956173</td>\n",
       "      <td>0.967872</td>\n",
       "      <td>0.974627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6441.0</td>\n",
       "      <td>0.741391</td>\n",
       "      <td>0.234529</td>\n",
       "      <td>0.126705</td>\n",
       "      <td>0.554008</td>\n",
       "      <td>0.835144</td>\n",
       "      <td>0.945148</td>\n",
       "      <td>0.982064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1366.0</td>\n",
       "      <td>0.386526</td>\n",
       "      <td>0.105981</td>\n",
       "      <td>0.138421</td>\n",
       "      <td>0.305284</td>\n",
       "      <td>0.385955</td>\n",
       "      <td>0.467187</td>\n",
       "      <td>0.617621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4108.0</td>\n",
       "      <td>0.498052</td>\n",
       "      <td>0.147095</td>\n",
       "      <td>0.093204</td>\n",
       "      <td>0.387617</td>\n",
       "      <td>0.518314</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.768572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2320.0</td>\n",
       "      <td>0.352670</td>\n",
       "      <td>0.128075</td>\n",
       "      <td>0.114436</td>\n",
       "      <td>0.243909</td>\n",
       "      <td>0.338815</td>\n",
       "      <td>0.453435</td>\n",
       "      <td>0.642961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4780.0</td>\n",
       "      <td>0.656115</td>\n",
       "      <td>0.247266</td>\n",
       "      <td>0.120240</td>\n",
       "      <td>0.423637</td>\n",
       "      <td>0.723281</td>\n",
       "      <td>0.890782</td>\n",
       "      <td>0.956166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1540.0</td>\n",
       "      <td>0.504510</td>\n",
       "      <td>0.168473</td>\n",
       "      <td>0.120028</td>\n",
       "      <td>0.371044</td>\n",
       "      <td>0.496594</td>\n",
       "      <td>0.645774</td>\n",
       "      <td>0.853527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4314.0</td>\n",
       "      <td>0.715330</td>\n",
       "      <td>0.189366</td>\n",
       "      <td>0.112852</td>\n",
       "      <td>0.583356</td>\n",
       "      <td>0.787513</td>\n",
       "      <td>0.871542</td>\n",
       "      <td>0.940572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3242.0</td>\n",
       "      <td>0.877686</td>\n",
       "      <td>0.178106</td>\n",
       "      <td>0.129092</td>\n",
       "      <td>0.897202</td>\n",
       "      <td>0.958898</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>0.981479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11921.0</td>\n",
       "      <td>0.839911</td>\n",
       "      <td>0.196575</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.780404</td>\n",
       "      <td>0.941785</td>\n",
       "      <td>0.971170</td>\n",
       "      <td>0.987029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1574.0</td>\n",
       "      <td>0.418690</td>\n",
       "      <td>0.133496</td>\n",
       "      <td>0.140829</td>\n",
       "      <td>0.314354</td>\n",
       "      <td>0.406837</td>\n",
       "      <td>0.524091</td>\n",
       "      <td>0.754473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10648.0</td>\n",
       "      <td>0.547153</td>\n",
       "      <td>0.193563</td>\n",
       "      <td>0.109558</td>\n",
       "      <td>0.393835</td>\n",
       "      <td>0.548929</td>\n",
       "      <td>0.714959</td>\n",
       "      <td>0.908429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>418.0</td>\n",
       "      <td>0.222347</td>\n",
       "      <td>0.045120</td>\n",
       "      <td>0.103251</td>\n",
       "      <td>0.193242</td>\n",
       "      <td>0.220646</td>\n",
       "      <td>0.252260</td>\n",
       "      <td>0.344682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.130555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9300.0</td>\n",
       "      <td>0.676713</td>\n",
       "      <td>0.271094</td>\n",
       "      <td>0.113672</td>\n",
       "      <td>0.425342</td>\n",
       "      <td>0.741195</td>\n",
       "      <td>0.942738</td>\n",
       "      <td>0.991517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>87.0</td>\n",
       "      <td>0.169295</td>\n",
       "      <td>0.028792</td>\n",
       "      <td>0.112522</td>\n",
       "      <td>0.147757</td>\n",
       "      <td>0.165717</td>\n",
       "      <td>0.188771</td>\n",
       "      <td>0.247617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5613.0</td>\n",
       "      <td>0.813688</td>\n",
       "      <td>0.217035</td>\n",
       "      <td>0.120058</td>\n",
       "      <td>0.738421</td>\n",
       "      <td>0.924710</td>\n",
       "      <td>0.964420</td>\n",
       "      <td>0.985283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4228.0</td>\n",
       "      <td>0.618473</td>\n",
       "      <td>0.193106</td>\n",
       "      <td>0.099853</td>\n",
       "      <td>0.471122</td>\n",
       "      <td>0.656026</td>\n",
       "      <td>0.788790</td>\n",
       "      <td>0.900154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3834.0</td>\n",
       "      <td>0.724429</td>\n",
       "      <td>0.196630</td>\n",
       "      <td>0.097521</td>\n",
       "      <td>0.595665</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.883907</td>\n",
       "      <td>0.959967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0     12602.0  0.848058  0.192906  0.099420  0.793265  0.942826  0.977441   \n",
       "1     26000.0  0.809861  0.238692  0.087219  0.683788  0.946383  0.980377   \n",
       "2      2297.0  0.605833  0.191890  0.094463  0.444121  0.639296  0.778246   \n",
       "3      3366.0  0.874512  0.173787  0.157057  0.883541  0.956173  0.967872   \n",
       "4      6441.0  0.741391  0.234529  0.126705  0.554008  0.835144  0.945148   \n",
       "5      1366.0  0.386526  0.105981  0.138421  0.305284  0.385955  0.467187   \n",
       "6      4108.0  0.498052  0.147095  0.093204  0.387617  0.518314  0.620197   \n",
       "8      2320.0  0.352670  0.128075  0.114436  0.243909  0.338815  0.453435   \n",
       "9      4780.0  0.656115  0.247266  0.120240  0.423637  0.723281  0.890782   \n",
       "10     1540.0  0.504510  0.168473  0.120028  0.371044  0.496594  0.645774   \n",
       "11     4314.0  0.715330  0.189366  0.112852  0.583356  0.787513  0.871542   \n",
       "12     3242.0  0.877686  0.178106  0.129092  0.897202  0.958898  0.972096   \n",
       "13    11921.0  0.839911  0.196575  0.118600  0.780404  0.941785  0.971170   \n",
       "14     1574.0  0.418690  0.133496  0.140829  0.314354  0.406837  0.524091   \n",
       "15    10648.0  0.547153  0.193563  0.109558  0.393835  0.548929  0.714959   \n",
       "16      418.0  0.222347  0.045120  0.103251  0.193242  0.220646  0.252260   \n",
       "17        1.0  0.130555       NaN  0.130555  0.130555  0.130555  0.130555   \n",
       "18     9300.0  0.676713  0.271094  0.113672  0.425342  0.741195  0.942738   \n",
       "19       87.0  0.169295  0.028792  0.112522  0.147757  0.165717  0.188771   \n",
       "22     5613.0  0.813688  0.217035  0.120058  0.738421  0.924710  0.964420   \n",
       "23     4228.0  0.618473  0.193106  0.099853  0.471122  0.656026  0.788790   \n",
       "24     3834.0  0.724429  0.196630  0.097521  0.595665  0.792248  0.883907   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     0.995070  \n",
       "1     0.993897  \n",
       "2     0.894880  \n",
       "3     0.974627  \n",
       "4     0.982064  \n",
       "5     0.617621  \n",
       "6     0.768572  \n",
       "8     0.642961  \n",
       "9     0.956166  \n",
       "10    0.853527  \n",
       "11    0.940572  \n",
       "12    0.981479  \n",
       "13    0.987029  \n",
       "14    0.754473  \n",
       "15    0.908429  \n",
       "16    0.344682  \n",
       "17    0.130555  \n",
       "18    0.991517  \n",
       "19    0.247617  \n",
       "22    0.985283  \n",
       "23    0.900154  \n",
       "24    0.959967  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 200)         6692600   \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               102400512 \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 109,159,937\n",
      "Trainable params: 102,467,337\n",
      "Non-trainable params: 6,692,600\n",
      "_________________________________________________________________\n",
      "Train on 8399 samples, validate on 33601 samples\n",
      "Epoch 1/2\n",
      "8399/8399 [==============================] - 125s 15ms/step - loss: 0.2489 - acc: 0.9057 - precision: 0.1104 - recall: 0.9940 - val_loss: 0.2185 - val_acc: 0.9155 - val_precision: 0.1111 - val_recall: 1.0000\n",
      "Epoch 2/2\n",
      "8399/8399 [==============================] - 121s 14ms/step - loss: 0.1594 - acc: 0.9394 - precision: 0.1111 - recall: 1.0000 - val_loss: 0.2231 - val_acc: 0.9167 - val_precision: 0.1111 - val_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "'''\n",
    "x = GRU(units=128, activation='tanh', return_sequences=True)(embedded_sequences)\n",
    "\n",
    "x = LSTM(units=256, activation='tanh', return_sequences=False)(embedded_sequences)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#x = LSTM(units=128, activation='tanh', return_sequences=True)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "'''\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=9, activation='softmax')(x) #softmax\n",
    "\n",
    "# x = Dense(units=512, activation='relu')(x)\n",
    "# x = Dense(units=128, activation='relu')(x)\n",
    "# preds = Dense(units=25, activation='sigmoid')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 25s 412us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19112927243113517, 0.9295742606123288, 0.1111111119389534, 1.0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=500, verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37694"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val[df_val.pred==df_val.true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 7, 5, 3, 4, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_95.pred.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
