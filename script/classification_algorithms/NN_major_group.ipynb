{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "# Check GPU device.\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and compile tranining and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154424, 38607)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path='../../dataset/df_ntee_universal/train/'\n",
    "file_list=os.listdir(train_file_path)\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, pd.read_pickle(train_file_path+file, compression='gzip')])\n",
    "\n",
    "test_file_path='../../dataset/df_ntee_universal/test/'\n",
    "file_list=os.listdir(test_file_path)\n",
    "df_test=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_test=pd.concat([df_test, pd.read_pickle(test_file_path+file, compression='gzip')])\n",
    "    \n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154424 25\n",
      "38607 25\n"
     ]
    }
   ],
   "source": [
    "df_train['mission_prgrm_spellchk']=df_train['TAXPAYER_NAME']+' '+df_train['mission_spellchk']+' '+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "print(len(df_train['mission_prgrm_spellchk']), len(df_train['NTEE1'].drop_duplicates()))\n",
    "\n",
    "df_test['mission_prgrm_spellchk']=df_test['TAXPAYER_NAME']+' '+df_test['mission_spellchk']+' '+df_test['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "print(len(df_test['mission_prgrm_spellchk']), len(df_test['NTEE1'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE1\n",
      "A    0.110151\n",
      "B    0.167247\n",
      "C    0.021519\n",
      "D    0.027450\n",
      "E    0.058378\n",
      "F    0.014901\n",
      "G    0.032722\n",
      "H    0.003024\n",
      "I    0.019084\n",
      "J    0.030902\n",
      "K    0.013010\n",
      "L    0.038478\n",
      "M    0.030390\n",
      "N    0.100114\n",
      "O    0.011209\n",
      "P    0.059447\n",
      "Q    0.012867\n",
      "R    0.006890\n",
      "S    0.093632\n",
      "T    0.013159\n",
      "U    0.006476\n",
      "V    0.002266\n",
      "W    0.054117\n",
      "X    0.029568\n",
      "Y    0.042998\n",
      "Name: EIN, dtype: float64 \n",
      "\n",
      " NTEE1\n",
      "A    0.111146\n",
      "B    0.166265\n",
      "C    0.021421\n",
      "D    0.026783\n",
      "E    0.059756\n",
      "F    0.014065\n",
      "G    0.035045\n",
      "H    0.003264\n",
      "I    0.019168\n",
      "J    0.029321\n",
      "K    0.013521\n",
      "L    0.039811\n",
      "M    0.029528\n",
      "N    0.101666\n",
      "O    0.010594\n",
      "P    0.060041\n",
      "Q    0.011293\n",
      "R    0.006657\n",
      "S    0.093325\n",
      "T    0.014013\n",
      "U    0.005828\n",
      "V    0.002202\n",
      "W    0.052788\n",
      "X    0.028440\n",
      "Y    0.044059\n",
      "Name: EIN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# # Build training and testing data frame.\n",
    "# small_num=0\n",
    "# while small_num<500: # Make sure each category has at least 500 records.\n",
    "#     sampleDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "#     trainDF, valDF =train_test_split(sampleDF, test_size=.3)\n",
    "#     small_num=trainDF.groupby('NTEE_M').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "# See the composition by NTEE major groups.\n",
    "print(df_train.groupby('NTEE1').count()['EIN']/len(df_train), '\\n'*2, df_test.groupby('NTEE1').count()['EIN']/len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(list(df_train.NTEE1.unique()))\n",
    "\n",
    "y_train=lb.transform(df_train['NTEE1'])\n",
    "# y_test=lb.transform(df_test['NTEE1']) # No need to transform Y for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=stopwords.words('english')+list(string.punctuation)\n",
    "def stopwords_remove(string):\n",
    "    global stop_list\n",
    "    tokens=word_tokenize(string)\n",
    "    return [s for s in tokens if s not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=df_train['mission_prgrm_spellchk'].apply(stopwords_remove)\n",
    "text_token_list_test=df_test['mission_prgrm_spellchk'].apply(stopwords_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_test.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_test=tokenizer.texts_to_sequences(text_token_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "x_test=pad_sequences(sequences=seq_encoding_text_test,\n",
    "                    maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                    dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                    value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embedding layer.\n",
    "Use pre-trained GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic tuning of training params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/gpu:1'): # Specify which GPU to use.\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(128, 5, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    # model.add(PReLU()) # https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \n",
    "    #                                                                      precision, recall\n",
    "                                                                        ])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.3, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "df_history=pd.read_csv('../../output/grid_search_history_major_group.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [32, 64, 128]:\n",
    "    for kernel_size in [3,5,7]:\n",
    "        for conv_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:1'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_major_group.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision making: Optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "try:\n",
    "    df_history=pd.read_csv('../../output/grid_search_history_major_group_optimizing.tsv', sep='\\t', index_col=0)\n",
    "except:\n",
    "    df_history=pd.DataFrame(columns=pd.read_csv('../../output/grid_search_history_major_group.tsv', sep='\\t', index_col=0).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [128, 256, 512, 1024]:\n",
    "    for kernel_size in [3]:\n",
    "        for conv_act in ['softplus']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:1'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_major_group_optimizing.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model finalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best configuration**\n",
    "\n",
    "_Broad Category_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.820386905 | 0.776488095 | -- | 0.613613255 | 0.738004138 | 4 | 512 | 3 | softplus | softplus|\n",
    "\n",
    "_Major Group_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.764369048 | 0.710428571 | -- | 0.888776311 | 1.120441045 | 6 | 256 | 3 | softplus | softmax|\n",
    "|0.779 | 0.71 | -- | 0.834419103 | 1.140895644 | 7 | 256 | 3 | softplus | softplus|\n",
    "\n",
    "Use `softplus`+`softplus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(512, 3, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=32, activation='softplus'))\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softplus'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.2, epochs=4, verbose=1)\n",
    "    y_prob = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate on validation dataset.\n",
    "# From probability --> serial coding, e.g., [4, 3, 2, 55] --> categorical, e.g., [[00010...], [001000..]...]\n",
    "y_train = lb.inverse_transform(np_utils.to_categorical(y_prob.argmax(axis=-1)))\n",
    "y_train_prob=[s.max() for s in y_prob]\n",
    "df_val=pd.DataFrame({'pred':y_train, \n",
    "                     'true':y_test, \n",
    "                     'prob':y_train_prob})\n",
    "print('Overall ACC:', len(df_val[df_val.pred==df_val.true])/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=y_train, y_pred=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft codes saved in `NN_broad_cat.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
