{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "# Check GPU device.\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and compile tranining and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234027, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc'] # Using raw.\n",
    "df_train['input_text']=df_train['mission_spellchk']+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "len(df_train['input_text']), len(df_train['NTEE1'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training and testing data frame.\n",
    "small_num=0\n",
    "while small_num<250: # Make sure each category has at least 500 records.\n",
    "    sampleDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(150000)\n",
    "    trainDF, valDF =train_test_split(sampleDF, test_size=.3)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE1\n",
      "A    11693\n",
      "B    17342\n",
      "C     2315\n",
      "D     3025\n",
      "E     6599\n",
      "F     1612\n",
      "G     3641\n",
      "H      349\n",
      "I     2008\n",
      "J     3003\n",
      "K     1347\n",
      "L     4014\n",
      "M     2977\n",
      "N    10098\n",
      "O     1169\n",
      "P     6636\n",
      "Q     1462\n",
      "R      766\n",
      "S     9604\n",
      "T     1461\n",
      "U      695\n",
      "V      255\n",
      "W     5618\n",
      "X     3022\n",
      "Y     4289\n",
      "Name: EIN, dtype: int64 \n",
      "\n",
      " NTEE1\n",
      "A    4948\n",
      "B    7395\n",
      "C    1066\n",
      "D    1262\n",
      "E    2865\n",
      "F     737\n",
      "G    1486\n",
      "H     129\n",
      "I     900\n",
      "J    1389\n",
      "K     600\n",
      "L    1716\n",
      "M    1343\n",
      "N    4308\n",
      "O     505\n",
      "P    2863\n",
      "Q     591\n",
      "R     318\n",
      "S    4095\n",
      "T     647\n",
      "U     285\n",
      "V     107\n",
      "W    2339\n",
      "X    1319\n",
      "Y    1787\n",
      "Name: EIN, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# See the composition by NTEE major groups.\n",
    "print(trainDF.groupby('NTEE1').count()['EIN'], '\\n'*2, valDF.groupby('NTEE1').count()['EIN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label_list, class_list):\n",
    "    int_encoder=LabelEncoder().fit(class_list) # Build the encoder.\n",
    "    label_int_encoded=int_encoder.transform(label_list) # One-dimensional integer encoded.\n",
    "    return np_utils.to_categorical(label_int_encoded) # Multi-dimensional binary/one-hot encoded.\n",
    "\n",
    "y_train=one_hot(label_list=trainDF['NTEE1'], class_list=list(trainDF['NTEE1'].unique()))\n",
    "y_val=one_hot(label_list=valDF['NTEE1'], class_list=list(trainDF['NTEE1'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=stopwords.words('english')+list(string.punctuation)\n",
    "def stopwords_remove(token_list):\n",
    "    global stop_list\n",
    "    return [s for s in token_list if s not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=trainDF['input_text'].apply(stopwords_remove)\n",
    "text_token_list_val=valDF['input_text'].apply(stopwords_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moved to preprocessing pipeline.**\n",
    "```Python\n",
    "# Spell check function. Return corrected word if unknown; return original word if known.\n",
    "def spellcheck(word_string_list):\n",
    "    return [SpellChecker().correction(word=s).upper() for s in word_string_list]\n",
    "\n",
    "# Parallel computing\n",
    "p = Pool(48)\n",
    "text_token_list_train=p.map(spellcheck, text_token_list_train)\n",
    "text_token_list_val=p.map(spellcheck, text_token_list_val)\n",
    "# Pool.map keep the original order of data passed to map.\n",
    "# https://stackoverflow.com/questions/41273960/python-3-does-pool-keep-the-original-order-of-data-passed-to-map\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1), ('the', 2), ('to', 3), ('of', 4), ('in', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_val.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_val=tokenizer.texts_to_sequences(text_token_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "x_val=pad_sequences(sequences=seq_encoding_text_val,\n",
    "                    maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                    dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                    value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not using pre-trained embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index), # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=32, # Size of the vector space in which words will be embedded.\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(max([len(s) for s in seq_encoding_text_train]),), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=len(y_train[0]), activation='softmax')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics.\n",
    "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='sigmoid'))\n",
    "model.add(Dense(units=256, activation='sigmoid'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='relu'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', precision, recall])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# Batch size: https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pre-trained GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158688/158688 [00:00<00:00, 390283.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic tuning of training params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/gpu:1'): # Specify which GPU to use.\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(128, 5, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    # model.add(PReLU()) # https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \n",
    "    #                                                                      precision, recall\n",
    "                                                                        ])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.3, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "df_history=pd.read_csv('../../output/grid_search_history_major_group.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [32, 64, 128]:\n",
    "    for kernel_size in [3,5,7]:\n",
    "        for conv_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:1'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_major_group.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmYFNXZ9/Hvj10EAQdEBZlB5Ykim+MIKii4RMEYUDAKwcQlSjQSkxiSaPSJXrxR4m6MxgS36CMRjUbFRMVIcIvroIAiYRFRRxABEcRBBbnfP041FE3PTM9S3TPM/bmuuqaWU1WniqbvPkudkpnhnHPO1bUm+c6Ac865HZMHGOecc4nwAOOccy4RHmCcc84lwgOMc865RHiAcc45lwgPMC5RkppKWi+pW12mzSdJ+0qq8/79ko6RtDS2vEDS4dmkrcG5bpf065ru71w2muU7A65+kbQ+ttga+BL4Olr+oZlNqc7xzOxroE1dp20MzOwbdXEcSWcDp5nZkNixz66LYztXGQ8wbhtmtuULPvqFfLaZPV1ReknNzGxTLvLmXFX881i/eBWZqxZJv5V0v6T7JH0GnCbpUEkvS/pU0nJJN0lqHqVvJskkFUXL90bbn5D0maSXJHWvbtpo+zBJCyWtlfQHSf+RdEYF+c4mjz+UtFjSGkk3xfZtKukGSaslvQMMreT+XCppatq6WyRdH82fLWl+dD3vRKWLio5VJmlINN9a0v9FeZsHHJThvEui486TNDxa3xu4GTg8qn5cFbu3l8f2Pze69tWSHpG0Rzb3pjr3OZUfSU9L+kTSR5J+GTvP/0b3ZJ2kUkl7ZqqOlPRC6t85up/PRef5BLhUUg9JM6NrWRXdt3ax/Quja1wZbf+9pFZRnvePpdtDUrmkgoqu11XBzHzyKeMELAWOSVv3W+Ar4NuEHyg7AQcDAwgl4r2BhcD4KH0zwICiaPleYBVQAjQH7gfurUHa3YDPgBHRtguBjcAZFVxLNnl8FGgHFAGfpK4dGA/MA7oCBcBz4b9OxvPsDawHdo4d+2OgJFr+dpRGwFHABqBPtO0YYGnsWGXAkGj+WuAZoANQCLydlvYUYI/o3+S7UR46R9vOBp5Jy+e9wOXR/LFRHvsBrYA/Av/O5t5U8z63A1YAPwFaArsA/aNtFwNzgB7RNfQDdgX2Tb/XwAupf+fo2jYB5wFNCZ/H/wGOBlpEn5P/ANfGruet6H7uHKUfGG2bDFwRO8/PgYfz/f+wIU95z4BP9Xei4gDz7yr2mwD8LZrPFDT+FEs7HHirBmnPAp6PbROwnAoCTJZ5PCS2/e/AhGj+OUJVYWrb8elfemnHfhn4bjQ/DFhYSdp/AOdH85UFmPfj/xbAj+JpMxz3LeBb0XxVAeZu4MrYtl0I7W5dq7o31bzP3wNKK0j3Tiq/aeuzCTBLqsjDycBr0fzhwEdA0wzpBgLvAoqWZwMj6/r/VWOavIrM1cQH8QVJ+0n6Z1TlsQ6YCHSsZP+PYvPlVN6wX1HaPeP5sPCNUFbRQbLMY1bnAt6rJL8AfwXGRPPfBbZ0jJB0gqRXoiqiTwmlh8ruVcoeleVB0hmS5kTVPJ8C+2V5XAjXt+V4ZrYOWAN0iaXJ6t+sivu8F7C4gjzsRQgyNZH+edxd0gOSPozy8Je0PCy10KFkG2b2H0JpaJCkXkA34J81zJPD22BczaR30f0z4Rfzvma2C/AbQokiScsJv7ABkCS2/UJMV5s8Lid8MaVU1Y36fuAYSV0JVXh/jfK4E/AgMIlQfdUeeCrLfHxUUR4k7Q3cSqgmKoiO+9/YcavqUr2MUO2WOl5bQlXch1nkK11l9/kDYJ8K9qto2+dRnlrH1u2elib9+q4i9H7sHeXhjLQ8FEpqWkE+7gFOI5S2HjCzLytI57LgAcbVhbbAWuDzqJH0hzk45z+AYknfltSMUK/fKaE8PgD8VFKXqMH3V5UlNrMVhGqcu4AFZrYo2tSS0C6wEvha0gmEtoJs8/BrSe0VnhMaH9vWhvAlu5IQa88mlGBSVgBd443tae4DfiCpj6SWhAD4vJlVWCKsRGX3eRrQTdJ4SS0k7SKpf7TtduC3kvZR0E/SroTA+hGhM0lTSeOIBcNK8vA5sFbSXoRqupSXgNXAlQodJ3aSNDC2/f8IVWrfJQQbVwseYFxd+DlwOqHR/c+EX/CJir7ETwWuJ3xh7AO8QfjlWtd5vBWYAbwJvEYohVTlr4Q2lb/G8vwp8DPgYUJD+cmEQJmNywglqaXAE8S+/MxsLnAT8GqUZj/gldi+/wIWASskxau6Uvs/SajKejjavxswNst8pavwPpvZWuCbwChCp4KFwOBo8zXAI4T7vI7Q4N4qqvo8B/g1ocPHvmnXlsllQH9CoJsGPBTLwybgBGB/QmnmfcK/Q2r7UsK/81dm9mI1r92lSTVmOdegRVUey4CTzez5fOfHNVyS7iF0HLg833lp6PxBS9dgSRpKqPL4gtDNdRPhV7xzNRK1Z40Aeuc7LzsCryJzDdkgYAmh6mQocKI3yrqakjSJ8CzOlWb2fr7zsyPwKjLnnHOJ8BKMc865RDTqNpiOHTtaUVFRvrPhnHMNyqxZs1aZWWWPBQCNPMAUFRVRWlqa72w451yDIqmq0SwAryJzzjmXEA8wzjnnEuEBxjnnXCIadRtMJhs3bqSsrIwvvvgi31lxlWjVqhVdu3alefOKhtdyzuWbB5g0ZWVltG3blqKiIsIAva6+MTNWr15NWVkZ3bt3r3oH51xeeBVZmi+++IKCggIPLvWYJAoKCryU6VwNTJkCRUXQpEn4O2VKVXvUnJdgMvDgUv/5v5Fz1TdlCowbB+XlYfm998IywNiajp9dCS/BOOdcA1KbEsgll2wNLinl5WF9EjzA1DOrV6+mX79+9OvXj913350uXbpsWf7qq6+yOsaZZ57JggULKk1zyy23MCXJsrFzrs6lSiDvvQdmW0sg2f5Xfr+CITwrWl9bjXqwy5KSEkt/kn/+/Pnsv//+WR9jypQQ/d9/H7p1gyuuqLui5uWXX06bNm2YMGHCNuvNDDOjSZPG/fuguv9WzjV0RUUhqKQrLISlS5PfP0XSLDMrqSpd4/6GqqXa/pqojsWLF9OrVy/OPfdciouLWb58OePGjaOkpIQDDjiAiRMnbkk7aNAgZs+ezaZNm2jfvj0XXXQRffv25dBDD+Xjjz8G4NJLL+XGG2/ckv6iiy6if//+fOMb3+DFF8OL/D7//HNGjRpF3759GTNmDCUlJcyePXu7vF122WUcfPDBW/KX+tGycOFCjjrqKPr27UtxcTFLo0/wlVdeSe/evenbty+XJFU2d66eqk0VV21LIFdcAa1bb7uudeuwPhGpX8ONcTrooIMs3dtvv73duooUFpqF0LLtVFiY9SEqddlll9k111xjZmaLFi0ySfbqq69u2b569WozM9u4caMNGjTI5s2bZ2ZmAwcOtDfeeMM2btxogD3++ONmZvazn/3MJk2aZGZml1xyid1www1b0v/yl780M7NHH33UjjvuODMzmzRpkv3oRz8yM7PZs2dbkyZN7I033tgun6l8bN682UaPHr3lfMXFxTZt2jQzM9uwYYN9/vnnNm3aNBs0aJCVl5dvs29NVOffyrn64N57zVq33vb7onXrsD4bdfGdc++9Ib0U/mZ77jig1LL4jk20BCNpqKQFkhZLuijD9kJJMyTNlfSMpK6xbVdLmidpvqSbFLSVNDs2rZJ0Y5T+DEkrY9vOTvLaIPf1mfvssw8HH3zwluX77ruP4uJiiouLmT9/Pm+//fZ2++y0004MGzYMgIMOOmhLKSLdyJEjt0vzwgsvMHr0aAD69u3LAQcckHHfGTNm0L9/f/r27cuzzz7LvHnzWLNmDatWreLb3/42EB6MbN26NU8//TRnnXUWO+20EwC77rpr9W+Ecw1UbRvZ66IEMnZsqA7bvDn8TaL3WEpi3ZSjd6TfAnwTKANekzTNzOLfgtcC95jZ3ZKOAiYB35N0GDAQ6BOlewEYbGbPAP1i55gF/D12vPvNbHxS15SuW7fM9ZnduiVzvp133nnL/KJFi/j973/Pq6++Svv27TnttNMyPhfSokWLLfNNmzZl06ZNGY/dsmXL7dJYFu1z5eXljB8/ntdff50uXbpw6aWXbslHpq7EZuZdjF2jVdsfpalgkFS7b11LsgTTH1hsZkvM7CtgKuFd13E9gRnR/MzYdgNaAS2AlkBzYEV8R0k9gN2A5xPJfRZyXp8Zs27dOtq2bcsuu+zC8uXLmT59ep2fY9CgQTzwwAMAvPnmmxlLSBs2bKBJkyZ07NiRzz77jIceegiADh060LFjRx577DEgPMBaXl7Oscceyx133MGGDRsA+OSTT+o8384lqTZtKBX9+KzOj9JclkBqK8kA0wX4ILZcFq2LmwOMiuZPAtpKKjCzlwgBZ3k0TTez+Wn7jiGUWOI/s0dF1W0PStqrri6kImPHwuTJoQeGFP5Onpybf/Di4mJ69uxJr169OOeccxg4cGCdn+PHP/4xH374IX369OG6666jV69etGvXbps0BQUFnH766fTq1YuTTjqJAQMGbNk2ZcoUrrvuOvr06cOgQYNYuXIlJ5xwAkOHDqWkpIR+/fpxww031Hm+nUtKbTv25PNHaV5k01BTkwn4DnB7bPl7wB/S0uxJqOJ6A/g9IQi1A/YF/gm0iaaXgCPS9n0bOCi2XAC0jObPBf5dQb7GAaVAabdu3bZrvPKG4602btxoGzZsMDOzhQsXWlFRkW3cuDHPudrK/61cTdSmkbu+NLLnG1k28ic5VEwZEC9FdAWWxROY2TJgJICkNsAoM1sraRzwspmtj7Y9ARwCPBct9wWamdms2LFWxw59G3BVpkyZ2WRgMoTnYGpzgTu69evXc/TRR7Np0ybMjD//+c80a+ajC7mGq7ZDpdRFx56xY+t3tVZdSrKK7DWgh6TukloAo4Fp8QSSOkpK5eFi4M5o/n1gsKRmkpoDg4F4FdkY4L60Y+0RWxyelt7VQPv27Zk1axZz5sxh7ty5HHvssfnOknO1UtteXHXRhtKYJBZgzGwTMB6YTviyf8DM5kmaKGl4lGwIsEDSQqAzkKqJfBB4B3iT0E4zx8weix3+FNICDHBB1K15DnABcEbdX5VzriFrcA8qNnCJ1neY2ePA42nrfhObf5AQTNL3+xr4YSXH3TvDuosJpSDn3A6sNsMz1fbRgobWTTjffKgY51yDUR96cTWkbsL55gHGOddg1LYNJZ+PFjRGHmDqmSFDhmz30OSNN97Ij370o0r3a9OmDQDLli3j5JNPrvDY6aNHp7vxxhspj/0PPv744/n000+zybpzWcnnYI/gJZBc8gBTz4wZM4apU6dus27q1KmMGTMmq/333HNPHnxwu2atrKUHmMcff5z27dvX+HjOxdW2ist7cTUsHmDqmZNPPpl//OMffPnllwAsXbqUZcuWMWjQoC3PpRQXF9O7d28effTR7fZfunQpvXr1AsIwLqNHj6ZPnz6ceuqpW4ZnATjvvPO2DPV/2WWXAXDTTTexbNkyjjzySI488kgAioqKWLVqFQDXX389vXr1olevXluG+l+6dCn7778/55xzDgcccADHHnvsNudJeeyxxxgwYAAHHnggxxxzDCtWhJF/1q9fz5lnnknv3r3p06fPlqFmnnzySYqLi+nbty9HH310ndxbl3/1YbBHlzv+1FwlfvpTyPD6k1rp1w+i7+aMCgoK6N+/P08++SQjRoxg6tSpnHrqqUiiVatWPPzww+yyyy6sWrWKQw45hOHDh1c4eOStt95K69atmTt3LnPnzqW4uHjLtiuuuIJdd92Vr7/+mqOPPpq5c+dywQUXcP311zNz5kw6duy4zbFmzZrFXXfdxSuvvIKZMWDAAAYPHkyHDh1YtGgR9913H7fddhunnHIKDz30EKeddto2+w8aNIiXX34ZSdx+++1cffXVXHfddfy///f/aNeuHW+++SYAa9asYeXKlZxzzjk899xzdO/e3ccr24E0tsEeGzsvwdRD8WqyePWYmfHrX/+aPn36cMwxx/Dhhx9uKQlk8txzz235ou/Tpw99+vTZsu2BBx6guLiYAw88kHnz5mUcyDLuhRde4KSTTmLnnXemTZs2jBw5kuefD+OMdu/enX79wiDXFb0SoKysjOOOO47evXtzzTXXMG/ePACefvppzj///C3pOnTowMsvv8wRRxxB9+7dAR/Sv77xwR5dtrwEU4nKShpJOvHEE7nwwgt5/fXX2bBhw5aSx5QpU1i5ciWzZs2iefPmFBUVZRyiPy5T6ebdd9/l2muv5bXXXqNDhw6cccYZVR7HKhm6PzXUP4Th/jNVkf34xz/mwgsvZPjw4TzzzDNcfvnlW46bnsdM61z9UNuhVq64Ytv9wau4dmRegqmH2rRpw5AhQzjrrLO2adxfu3Ytu+22G82bN2fmzJm8l+mJsZgjjjiCKdHPy7feeou5c+cCYaj/nXfemXbt2rFixQqeeOKJLfu0bduWzz77LOOxHnnkEcrLy/n88895+OGHOfzww7O+prVr19KlSxhM++67796y/thjj+Xmm2/esrxmzRoOPfRQnn32Wd59913Ah/SvT7ybsKsODzD11JgxY5gzZ86WN0oCjB07ltLSUkpKSpgyZQr77bdfpcc477zzWL9+PX369OHqq6+mf//+QHg75YEHHsgBBxzAWWedtc1Q/+PGjWPYsGFbGvlTiouLOeOMM+jfvz8DBgzg7LPP5sADD8z6ei6//HK+853vcPjhh2/TvnPppZeyZs0aevXqRd++fZk5cyadOnVi8uTJjBw5kr59+3LqqadmfR5XNe8m7HJFlVV97OhKSkos/bmQ+fPns//+++cpR646/N+q+tKruCBUUWVbiigqyjzUSmFhCBaucZA0y8xKqkrnJRjnGhHvJuxyyQOMc41IXXQT9jYUly3vRZaB92Kq/xpz1W5t1HY0YWhcL8xyteMlmDStWrVi9erV/gVWj5kZq1evplWrVvnOSoPjVVwul7wEk6Zr166UlZWxcuXKfGfFVaJVq1Z07do139nIi9q8D8WfhHe55L3Iqhhd2Ln6pLa9wJyrC96LzLkdUG17gTmXSx5gnGtA6uJBR+dyxQOMczmW78EincuVRAOMpKGSFkhaLOmiDNsLJc2QNFfSM5K6xrZdLWmepPmSblLUbzhKt0DS7GjaLVrfUtL90blekVSU5LU5VxP14Z3yzuVKYgFGUlPgFmAY0BMYI6lnWrJrgXvMrA8wEZgU7XsYMBDoA/QCDgYGx/Yba2b9ounjaN0PgDVmti9wA3BVMlfmXM35YJGuMUmyBNMfWGxmS8zsK2AqMCItTU9gRjQ/M7bdgFZAC6Al0Byo+MUnwQggNUzvg8DR8qclXT3jg0W6xiTJANMF+CC2XBati5sDjIrmTwLaSiows5cIAWd5NE03s/mx/e6Kqsf+NxZEtpzPzDYBa4GC9ExJGiepVFKpP+vics3bUFxjkmSAyVR6SH/oZgIwWNIbhCqwD4FNkvYF9ge6EgLHUZKOiPYZa2a9gcOj6XvVOB9mNtnMSsyspFOnTtW9JudqxdtQXGOSZIApA/aKLXcFlsUTmNkyMxtpZgcCl0Tr1hJKMy+b2XozWw88ARwSbf8w+vsZ8FdCVdw255PUDGgH+JuqXJ2rTS8wb0NxjUmSAeY1oIek7pJaAKOBafEEkjpKSuXhYuDOaP59QsmmmaTmhNLN/Gi5Y7Rvc+AE4K1on2nA6dH8ycC/rTEPU+ASUdteYOBtKK7xSCzARO0g44HpwHzgATObJ2mipOFRsiHAAkkLgc5AqqLgQeAd4E1CO80cM3uM0OA/XdJcYDahSu22aJ87gAJJi4ELge26RTtXW/4kvXPZ87HIfCwyVw1NmoSSSzoplEicawx8LDLnEuC9wJzLngcY1+jUppHee4E5lz0PMK5RqW0jvfcCcy573gbjbTCNSlFR5lcGFxaGHl3Ouap5G4xzGfhw987ljgcY16h4I71zueMBxjUq3kjvXO54gHGNijfSO5c7zfKdAedybexYDyjO5YKXYFyDU5vnWJxzueMlGNegpJ5jSY0HlnqOBbxU4lx94yUY16D4YJPONRweYFyD4s+xONdweIBxDYo/x+Jcw+EBxjUo/hyLcw2HBxjXoPhzLM41HB5gXM7Vtpuxv3LYuYbBuym7nPJuxs41Hl6CcTnl3Yydazw8wLic8m7GzjUeiQYYSUMlLZC0WNJFGbYXSpohaa6kZyR1jW27WtI8SfMl3aSgtaR/SvpvtO13sfRnSFopaXY0nZ3ktbma8W7GzjUeiQUYSU2BW4BhQE9gjKSeacmuBe4xsz7ARGBStO9hwECgD9ALOBgYnNrHzPYDDgQGShoWO979ZtYvmm5P6NJcLXg3Y+cajyRLMP2BxWa2xMy+AqYCI9LS9ARmRPMzY9sNaAW0AFoCzYEVZlZuZjMBomO+DnTFNRjezdi5xiPJANMF+CC2XBati5sDjIrmTwLaSiows5cIAWd5NE03s/nxHSW1B77N1gAFMCqqbntQ0l6ZMiVpnKRSSaUrV66s6bW5WvBuxs41DkkGGGVYZ2nLE4DBkt4gVIF9CGyStC+wP6F00gU4StIRWw4sNQPuA24ysyXR6seAoqi67Wng7kyZMrPJZlZiZiWdOnWq+dU555yrVJIBpgyIlyK6AsviCcxsmZmNNLMDgUuidWsJpZmXzWy9ma0HngAOie06GVhkZjfGjrXazL6MFm8DDqrrC3LOOZe9JAPMa0APSd0ltQBGA9PiCSR1lJTKw8XAndH8+4SSTTNJzQmlm/nRPr8F2gE/TTvWHrHF4an0rm75y76cc9lKLMCY2SZgPDCd8GX/gJnNkzRR0vAo2RBggaSFQGcg1ZfoQeAd4E1CO80cM3ss6sZ8CaFzwOtp3ZEviLouzwEuAM5I6toaq9RT+O+9B2Zbn8L3IOOcy0Rm6c0ijUdJSYmVlpbmOxsNRlFRCCrpCgtDY71zrnGQNMvMSqpK50/yu6z5U/jOuerwAOOy5k/hO+eqwwOMy5o/he+cqw4PMC5r/hS+c646/H0wrlrGjvWA4pzLjpdgnHPOJcIDjHPOuUR4gHHOOZeIKgOMpPGSOuQiM84553Yc2ZRgdgdek/RA9IbKTKMkuwbCxxJzzuVKlQHGzC4FegB3EMb3WiTpSkn7JJw3V8d8LDHnXC5l1QZjYcCyj6JpE9ABeFDS1QnmzdWxSy6B8vJt15WXh/XOOVfXqnwORtIFwOnAKuB24BdmtjEaZn8R8Mtks+jqio8l5pzLpWwetOwIjDSzbcbRNbPNkk5IJlsuCd26ZR4N2ccSc84lIZsqsseBT1ILktpKGgBgZv5SrwbExxJzzuVSNgHmVmB9bPnzaJ1rYHwsMedcLmVTRSaLvZUsqhrzMcwaKB9LzDmXK9mUYJZIukBS82j6CbAk6Yw555xr2LIJMOcChwEfAmXAAGBckplyzjnX8GXzoOXHZjbazHYzs85m9l0z+zibg0dP/i+QtFjSRRm2F0qaIWmupGckdY1tu1rSPEnzJd2UGkFA0kGS3oyOGV+/q6R/SVoU/fXhbZxzLo+yGYuslaTzJf1R0p2pKYv9mgK3AMOAnsAYST3Tkl0L3GNmfYCJwKRo38OAgUAfoBdwMDA42udWQgmqRzQNjdZfBMwwsx7AjGjZOedcnmRTRfZ/hPHIjgOeBboCn2WxX39gsZktMbOvgKnAiLQ0PQnBAGBmbLsBrYAWQEugObBC0h7ALmb2UtTx4B7gxGifEcDd0fzdsfXOOefyIJsAs6+Z/S/wuZndDXwL6J3Ffl2AD2LLZdG6uDnAqGj+JKCtpAIze4kQcJZH0/TomZsu0XEyHbOzmS0HiP7ulilTksZJKpVUunLlyiwuwznnXE1kE2A2Rn8/ldQLaAcUZbFfplGXLW15AjBY0huEKrAPgU2S9gX2J5SWugBHSToiy2NWyswmm1mJmZV06tSpOrvWCz4asnOuocjmeZbJUYP5pcA0oA3wv1nsVwbsFVvuCiyLJzCzZcBIAEltgFFmtlbSOOBlM1sfbXsCOIRQXde1gmOukLSHmS2PqtKy6ojQkKRGQ04NWJkaDRn82RbnXP1TaQkmGtBynZmtMbPnzGzvqDfZn7M49mtAD0ndJbUARhMCVPz4HaNzAFwMpDoPvE8o2TST1JxQupkfVX19JumQqPfY94FHo32mEQblJPqbWr/D8NGQnXMNSaUBxsw2A+NrcmAz2xTtOx2YDzxgZvMkTZQ0PEo2BFggaSHQGUiNivUg8A7wJqGdZo6ZPRZtO48wqvPiKM0T0frfAd+UtAj4ZrS8Q/HRkJ1zDYlio8BkTiD9L7ABuJ8wDhkAZvZJhTs1ECUlJVZaWprvbGStqCjzaMiFhbB0aa5z45xrrCTNMrOSqtJl08h/FnA+8BwwK5oazrfyDsRHQ3bONSRVNvKbWfdcZMRVLdWQf8kloVqsW7cQXLyB3zlXH2XzRsvvZ1pvZvfUfXZcVXw0ZOdcQ5FNN+WDY/OtgKOB1wlP0TvnnHMZZVNF9uP4sqR2hOdRnHPOuQpl08ifrpwwyKRrgL7+Gm69FQYMgB/+EB55BD7LZmS5HcjHH8Ptt8PMmds/V+ScqzvZtME8xtbhWJoQBqh8IMlMuWQ8+yxccAHMnQu9esF994VXJjdvDoMGwbBhMHRo2KZMg/LU0tq1UFoKr74KixaF840YAS1a1P25MlmwAK6/Hu6+G778Mqxr1gwOOihc/6BBMHAgNMARhJyrl7J5DmZwbHET8J6ZlVWUviFpaM/B1NR778EvfgF/+1voeXbddTBqFGzcCC++CE88EaY33wzpu3YNgWbYMDjmGNhll+qf88svYc6cEExefRVeew3++9+t29u1CwGnc2f4wQ/gnHPCcz51zQxeeAGuvRamTYOWLeH000Pp7aOPwrbnnw95/OqrsM9++20NOIMGwd57JxNwnWuosn0OJpsA0x1YbmZfRMs7EUYuXloXGc2nHT3AlJfD1VfDVVeFL8iLLgqBZqedMqcvK4MnnwzB5umnYd268At/4MAQbIb4OwTlAAAZIklEQVQNg969t/+y3bwZFi7cGkxefRVmzw4BDEIQGTAA+vcPU0lJCFrTp8Of/gT//GcIBEOHwrnnwvHHh/PWxtdfw8MPh8DyyitQUADnnx+m3TKMs/3FFzBrVgg4qenTT8O23XcPgebww8PfPn1qnz/nGrK6DDClwGHRO12IxhX7j5kdXOmODcCOGmDM4MEHYcKE8LzMKafANdeE0ku2Nm6El17aWrqZMyes79IlBILDDoPFi7eWTtatC9vbtAkBJBVM+vcPJaLKSgAffBDaRG67DZYvD+nPPjtMXdJf8FCFzz+Hu+6CG26AJUtgn33gwgvhjDO2f0i1Mps3w9tvbw02zz+/dUieNm3g0EPDdMghIXjuumv18ulcQ1aXAWa2mfVLWzfHzPrWMo95tyMGmLlzQzvLs89C377w+9/D4MFV71eVZcu2lm7+9a9QvdWsWfg1Hw8m++0HTZvW7BwbN8I//gF//nMo3TRtCt/+dqjOOvbY8IqCiqxYATffDH/8I3zySfji/8UvQhtPTfOT7v334T//2Rp03norBCKA//mfcM7U1Lu3l3LcjivbAIOZVToB/wKGx5ZHEF5NXOW+9X066KCDbEexapXZeeeZNWliVlBgduutZps2JXOujRvN3n7brLw8meObmS1ebParX5l16mQGZt27m02aZPbRR9ummz/f7OyzzVq2NJPMTjzR7IUXkstX3Lp1Zv/+t9mVV5oNH262224hr2DWurXZEUeY/fKXZn//u9myZbnJU7Y2b853DlxDBpRaFt+x2ZRg9gGmAHtGq8qA75vZ4hqHv3piRyjBbNoU2jF+85tQTfWjH8Hll+84VTZffhm6Uv/pT/DMM6HH20knwfDhcP/98Nhj0KpVqAL72c9CSSJfzMKgoy+/vHV6442tbVHdum1byikuDp0OcmHz5lDNOX16KIm+8gocdRT86lehbck7MbjqqLMSTGoivGisbbbpG8KUjxLMvfeaFRaGX9uFhWG5pmbMMOvVK/xiPvposzffrKtc1k/z55v97GdmHTqEay4oMLvsMrMVK/Kds4pt2GD24otm119vdsopZt26bS3ltGhhdthhZhMmhFLO8uV1e+6PPzabMsXs+98369x563n79g2lvlTpcMAAs4ceSq7E63Y81GEJ5krgajP7NFruAPzczC6tbRTMt1yXYNLfSAmh4Xny5MrHFzMLPZrefz90OX7/fZgxI/yyLyoKz3aceGLj+RW6YUPoWFBSUr2G+/pi2bJQunnppdBNvLR0axfpvfcOHShSU69e2bchbdoUjpsqpcyaFT47BQXwzW+GzhnHHgt77BHSb9gAf/lL6Gm3ZEko/U2YAN/7XigVOleRumzkf8PMDkxb97qZFdcyj3mX6wBT0ftcunULjcap4BEPJKm/6U/bt20bqjd+/nP/MmjovvwSXn89BJuXXgodCT76KGxr0yZUpx16aAg4hxwC7dtv3ff997cGlBkzQueLJk1CuqFD4bjjwoOklQWpr7+Ghx4KXdpnzQrdsn/yk9BlPH6uuvTVV1uDarpMP5TS1zVp4p/7fKrLADMXONjMvoyWdyIUjw6ok5zmUa4DTJMm4RdlNgoKQuDp1i28UCz9b6dOlfeqcg2XWfhh8eKLW6c5c0I7igQ9e4Yegm+8AfPnh31SD8cedxwcfTR06FCz886cGQLN9OkhuP3wh/DTn4bj19TXX4cu36++Gtp+Xnll2x54NdWzZ3hm6vjjw7NauRoRwtVtgPklMBy4K1p1JjDNzK6udS7zrL6UYHbZZetzKoWFsNde4T+3cynr14cv6FTAmT07dIU+7rgQWPbfv26rSGfPDp/J++8PP2TGjg3dvnv2rHrfsrIQRFIBpbQ0PJ8EIfD17w8HHxxGc0iX6eso07ovvgjPJj37bOhE0bZtqAY8/vjwQPCee26/j6s7dRZgooMNBY4BBKwB9jCz82udyzzLdYC5/fbwizD+yy2bNhjn8mXp0tDGd/vtoc3mhBNC1ezAgSGgrVu3dXy5VFBZtizs26IF9Ou3dRSHAQNg333rNhCuXx+qBh9/PExl0SBW/fqFQHP88aG6sK6eSVq/PlRLfvBBCGzNmoXqx4qmirY3awYdOzbcar66DjD9gO8CpwDvAg+Z2c21zmWe5TLAfPklfOtboQqioABWrfI3UrqGY9UquOUW+MMfYPXq0K6zYUOookt9hfTosTWQDBgQqvFy1Q0bQj7eemtrsPnPf0L1XPv2oaR3/PGhtJdpqCAIP/w++mjbts/0NtE1a+o2z507h1qLTNXghYUh7/Wx806tA4yk/wFGA2OA1cD9wAQzK6xGJoYCvweaAreb2e/SthcCdwKdgE+A08ysTNKRwA2xpPsBo83sEUnPA22j9bsBr5rZiZKGAI8SAiDA381sYmX5y1WA+fprGDMmDDZ5993w/YzvCHWu/isvD0Px3HXX9mPM1bdnrz79NIyplwo4K1aE9QcfHAIObBtAysq2PrOU0q7d9m2h3bqFauyWLcP/7U2bwt+KpkzbN24M+XnvvW2D2RdfbHv+Nm22DzqpPOy8cwg+VU1NmmRe3759zdrqoG4CzGbgeeAHFj1UKWmJme2dZQaaAguBbxIeznwNGGNmb8fS/A34h5ndLeko4Ewz+17acXYFFgNdzaw8bdtDwKNmdk8UYCaY2QnZ5A9yE2DMwsOPf/pT6A76858nejrnXAabN4d2pVSwefnl8CXbpcv2wSM+ZWonSopZeFdRei/S+Pwnn9Td+X71K/jd76pOl0m2AaaymslRhBLMTElPAlMJbTDZ6g8sNrMlUYamEoaZeTuWpifws2h+JvBIhuOcDDyRIbi0BY4idDqoty6/PASXVJdi51zuNWkSRk4oLoZLLw1tKa1a1a/x4qRQKuzcOZSyMkm1AaVKO1sfn6182rx5+3W9eyd/TRXeXjN7GHhY0s7AiYRA0FnSrcDDZvZUFcfuAnwQWy4DBqSlmUMIZL8HTgLaSiows9WxNKOB6zMc/yTCmGjrYusOlTQHWEYozcxL30nSOGAcQLfqDC9cAzffDBMnwllnwaRJiZ7KOVcNDbWXZps2oSdfNr356oMqn6Qws8/NbEpU9dQVmA1clMWxM5V20uvjJgCDJb0BDAY+JLzULBxA2gPoDUzPcKwxwH2x5deBQgujPP+BzKUhzGyymZWYWUmnBF9dOHVqGNV4xIgwOnB9bKhzzrkkVetRPTP7xMz+bGZHZZG8DNgrttyVULKIH2+ZmY2MRgq4JFq3NpbkFEJpaZumN0kFhCq4f8aOtc7M1kfzjwPNJXXM/urqzlNPhYb8ww8PryWuT8Vw55zLlSSfBX8N6CGpe/SSstHAtHgCSR0lpfJwMaFHWVx6KSXlO4TOAVv6XEjaXQrlBEn9Cde2OsO+iXrlFRg5MhRhp02r+O2Rzjm3o0sswJjZJmA8oXprPvCAmc2TNFHS8CjZEGCBpIVAZ+CK1P6SiggloGczHH402week4G3ojaYmwjdmrMcmKVuzJ8f+tp37hzGhsplDxTnnKtvsnrQckdVl92UP/ggDEa4cWN4wGufferksM45V+/URTdll6XVq8Mw6OvWhbGRPLg455wHmFpbvz5Ui737bhiBtl+/fOfIOefqBw8wtfDVVzBqVBjs7+9/h8GD850j55yrPzzA1NDmzXD66aFL8h13hOddnHPObeWvrKoBs/DGv6lT4aqrwpP6zjnntuUBpgb+8pcwDMzPfx5ewuScc257XkVWA6eeCp99BuPH+xAwzjlXEQ8wNdC6dRhnzDnnXMW8isw551wiPMA455xLhAcY55xzifAA45xzLhEeYJxzziXCA4xzzrlEeIBxzjmXCA8wzjnnEuEBxjnnXCI8wDjnnEuEBxjnnHOJSDTASBoqaYGkxZIuyrC9UNIMSXMlPSOpa7T+SEmzY9MXkk6Mtv1F0ruxbf2i9ZJ0U3SuuZKKk7w255xzlUtssEtJTYFbgG8CZcBrkqaZ2duxZNcC95jZ3ZKOAiYB3zOzmUAqcOwKLAaeiu33CzN7MO2Uw4Ae0TQAuDX665xzLg+SLMH0Bxab2RIz+wqYCqS/97EnMCOan5lhO8DJwBNmVl7F+UYQgpWZ2ctAe0l71Dz7zjnnaiPJANMF+CC2XBati5sDjIrmTwLaSipISzMauC9t3RVRNdgNklpW43zOOedyJMkAk+lVXJa2PAEYLOkNYDDwIbBpywFCCaQ3MD22z8XAfsDBwK7Ar6pxPiSNk1QqqXTlypVZXopzzrnqSjLAlAF7xZa7AsviCcxsmZmNNLMDgUuidWtjSU4BHjazjbF9lkfVYF8CdxGq4rI6X7T/ZDMrMbOSTp061fzqnHPOVSrJAPMa0ENSd0ktCFVd0+IJJHWUlMrDxcCdaccYQ1r1WKpdRZKAE4G3ok3TgO9HvckOAdaa2fK6vCDnnHPZS6wXmZltkjSeUL3VFLjTzOZJmgiUmtk0YAgwSZIBzwHnp/aXVEQokTybdugpkjoRqsRmA+dG6x8Hjif0OCsHzkzmypxzzmVDZts1UzQaJSUlVlpamu9sOOdcgyJplpmVVJXOn+R3zjmXCA8wzjnnEuEBxjnnXCI8wDjnnEuEBxjnnHOJ8ADjnHMuER5gnHPOJcIDjHPOuUR4gHHOOZcIDzDOOecS4QHGOedcIjzAOOecS4QHGOecc4nwAOOccy4RHmCcc84lwgOMc865RHiAcc45lwgPMM455xLhAcY551wiPMA455xLRKIBRtJQSQskLZZ0UYbthZJmSJor6RlJXaP1R0qaHZu+kHRitG1KdMy3JN0pqXm0foiktbF9fpPktTnnnKtcYgFGUlPgFmAY0BMYI6lnWrJrgXvMrA8wEZgEYGYzzayfmfUDjgLKgaeifaYA+wG9gZ2As2PHez61n5lNTOjSnHPOZSHJEkx/YLGZLTGzr4CpwIi0ND2BGdH8zAzbAU4GnjCzcgAze9wiwKtA10Ry75xzrlaSDDBdgA9iy2XRurg5wKho/iSgraSCtDSjgfvSDx5VjX0PeDK2+lBJcyQ9IemATJmSNE5SqaTSlStXZn81zjnnqiXJAKMM6yxteQIwWNIbwGDgQ2DTlgNIexCqwqZnONYfgefM7Plo+XWg0Mz6An8AHsmUKTObbGYlZlbSqVOn6lyPc865akgywJQBe8WWuwLL4gnMbJmZjTSzA4FLonVrY0lOAR42s43x/SRdBnQCLowda52ZrY/mHweaS+pYh9fjnHOuGpIMMK8BPSR1l9SCUNU1LZ5AUkdJqTxcDNyZdowxpFWPSTobOA4YY2abY+t3l6Rovj/h2lbX4fU455yrhsQCjJltAsYTqrfmAw+Y2TxJEyUNj5INARZIWgh0Bq5I7S+piFACejbt0H+K0r6U1h35ZOAtSXOAm4DRUUcA55xzeaDG/B1cUlJipaWl+c6Gc841KJJmmVlJVen8SX7nnHOJ8ADjnHMuER5gnHPOJcIDjHPOuUR4gHHOOZcIDzDOOecS4QHGOedcIjzAOOecS4QHGOecc4nwAOOccy4RHmCqacoUKCqCJk3C3ylT8p0j55yrn5rlOwMNyZQpMG4clJeH5ffeC8sAY8fmL1/OOVcfeQmmGi65ZGtwSSkvD+udc85tywNMNbz/fvXWO+dcY+YBphq6daveeueca8w8wFTDFVdA69bbrmvdOqx3zjm3LQ8w1TB2LEyeDIWFIIW/kyd7A79zzmXivciqaexYDyjOOZcNL8E455xLRKIBRtJQSQskLZZ0UYbthZJmSJor6RlJXaP1R0qaHZu+kHRitK27pFckLZJ0v6QW0fqW0fLiaHtRktfmnHOucokFGElNgVuAYUBPYIyknmnJrgXuMbM+wERgEoCZzTSzfmbWDzgKKAeeiva5CrjBzHoAa4AfROt/AKwxs32BG6J0zjnn8iTJEkx/YLGZLTGzr4CpwIi0ND2BGdH8zAzbAU4GnjCzckkiBJwHo213AydG8yOiZaLtR0fpnXPO5UGSAaYL8EFsuSxaFzcHGBXNnwS0lVSQlmY0cF80XwB8amabMhxzy/mi7Wuj9M455/IgyV5kmUoPlrY8AbhZ0hnAc8CHQCp4IGkPoDcwPYtjZnM+JI0DohHEWC9pQQX5z7eOwKp8Z6IS9T1/UP/z6PmrHc9f7dQmf4XZJEoywJQBe8WWuwLL4gnMbBkwEkBSG2CUma2NJTkFeNjMNkbLq4D2kppFpZT4MVPnK5PUDGgHfJKeKTObDEyu5bUlTlKpmZXkOx8Vqe/5g/qfR89f7Xj+aicX+Uuyiuw1oEfU66sFoaprWjyBpI6SUnm4GLgz7Rhj2Fo9hpkZoa3m5GjV6cCj0fy0aJlo+7+j9M455/IgsQATlTDGE6q35gMPmNk8SRMlDY+SDQEWSFoIdAa2DLoSdTPeC3g27dC/Ai6UtJjQxnJHtP4OoCBafyGwXbdo55xzuZPok/xm9jjweNq638TmH2Rrj7D0fZeyfacAzGwJoYda+vovgO/ULsf1Sn2vxqvv+YP6n0fPX+14/mon8fzJa5Gcc84lwYeKcc45lwgPMM455xLhASaPJO0laaak+ZLmSfpJhjRDJK2Njcv2m0zHSjCPSyW9GZ27NMN2SbopGgNurqTiHObtG2lj1q2T9NO0NDm/f5LulPSxpLdi63aV9K9oDL1/SepQwb6nR2kWSTo9U5qE8neNpP9G/4YPS2pfwb6Vfh4SzN/lkj6M/TseX8G+lY5/mGD+7o/lbamk2RXsm+j9q+g7JW+fPzPzKU8TsAdQHM23BRYCPdPSDAH+kcc8LgU6VrL9eOAJwoOuhwCv5CmfTYGPgMJ83z/gCKAYeCu27mrgomj+IuCqDPvtCiyJ/naI5jvkKH/HAs2i+asy5S+bz0OC+bscmJDFZ+AdYG+gBWGkkJ65yF/a9uuA3+Tj/lX0nZKvz5+XYPLIzJab2evR/GeE7tzb9Zyr50YQBiw1M3uZ8CDsHnnIx9HAO2b2Xh7OvQ0ze47tH/KNj5UXH0Mv7jjgX2b2iZmtAf4FDM1F/szsKds6BNPLhIeY86KC+5eNbMY/rLXK8heNf3gKsef3cqmS75S8fP48wNQT0XM/BwKvZNh8qKQ5kp6QdEBOMxaG23lK0qxomJ102Yw5lwvxMevS5fP+pXQ2s+UQvgSA3TKkqS/38ixCqTSTqj4PSRofVeHdWUEVT324f4cDK8xsUQXbc3b/0r5T8vL58wBTDygMk/MQ8FMzW5e2+XVCtU9f4A/AIznO3kAzKya8duF8SUekbc9qDLgkRSNFDAf+lmFzvu9fddSHe3kJYTzAKRUkqerzkJRbgX2AfsByQjVUurzfP9JGH8kgJ/eviu+UCnfLsK5W988DTJ5Jak74IEwxs7+nbzezdWa2Ppp/HGguqWOu8mdhvDjM7GPgYbZ/yLXKMedyYBjwupmtSN+Q7/sXsyJVdRj9/ThDmrzey6hR9wRgrEWV8umy+DwkwsxWmNnXZrYZuK2C8+b7/jUjjK14f0VpcnH/KvhOycvnzwNMHkX1tXcA883s+grS7B6lQ1J/wr/Z6hzlb2dJbVPzhIbgt9KSTQO+H/UmOwRYmyqK51CFvxrzef/SxMfKi4+hFzcdOFZSh6gK6Fi2jiSeKElDCcMwDTez8grSZPN5SCp/8Xa9kyo4b5XjHybsGOC/ZlaWaWMu7l8l3yn5+fwl1ZvBp6x6fAwiFEHnArOj6XjgXODcKM14YB6hR8zLwGE5zN/e0XnnRHm4JFofz58Iby59B3gTKMnxPWxNCBjtYuvyev8IwW45sJHwq/AHhHHzZgCLor+7RmlLgNtj+54FLI6mM3OYv8WE+vfU5/BPUdo9gccr+zzkKH//F32+5hK+LPdIz1+0fDyh59Q7ucxftP4vqc9dLG1O718l3yl5+fz5UDHOOecS4VVkzjnnEuEBxjnnXCI8wDjnnEuEBxjnnHOJ8ADjnHMuER5gnEuApK+17UjPdTayr6Si+Ei+ztVXib4y2blGbIOZ9ct3JpzLJy/BOJdD0ftArpL0ajTtG60vlDQjGsxxhqRu0frOCu9nmRNNh0WHairptuidH09J2ilKf4Gkt6PjTM3TZToHeIBxLik7pVWRnRrbts7M+gM3AzdG624mvPagD2GgyZui9TcBz1oYrLOY8AQ4QA/gFjM7APgUGBWtvwg4MDrOuUldnHPZ8Cf5nUuApPVm1ibD+qXAUWa2JBqU8CMzK5C0ijD8ycZo/XIz6yhpJdDVzL6MHaOI8N6OHtHyr4DmZvZbSU8C6wmjRj9i0UCfzuWDl2Ccyz2rYL6iNJl8GZv/mq3tqd8ijA13EDArGuHXubzwAONc7p0a+/tSNP8iYfRfgLHAC9H8DOA8AElNJe1S0UElNQH2MrOZwC+B9sB2pSjncsV/3TiXjJ0kzY4tP2lmqa7KLSW9QviBNyZadwFwp6RfACuBM6P1PwEmS/oBoaRyHmEk30yaAvdKakcY5foGM/u0zq7IuWryNhjncihqgykxs1X5zotzSfMqMuecc4nwEoxzzrlEeAnGOedcIjzAOOecS4QHGOecc4nwAOOccy4RHmCcc84l4v8Dys7yhSuYpCUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "history_dict = history.history\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 57s 947us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6837"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. \n",
    "The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 – probability) to give the probability for class 0.\n",
    "In the case of a multi-class classification problem, the softmax activation function \n",
    "is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n",
    "'''\n",
    "\n",
    "y_prob = model.predict(x_val, verbose=1)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)\n",
    "\n",
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 67s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.772"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. \n",
    "The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 – probability) to give the probability for class 0.\n",
    "In the case of a multi-class classification problem, the softmax activation function \n",
    "is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n",
    "'''\n",
    "\n",
    "y_prob = model.predict(x_val, verbose=1)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)\n",
    "\n",
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9043997827267789 0.46025\n"
     ]
    }
   ],
   "source": [
    "df_95=df_val[df_val.prob>.95]\n",
    "print(len(df_95[df_95.pred==df_95.true])/len(df_95), len(df_95)/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8247847099301924 0.8380166666666666\n"
     ]
    }
   ],
   "source": [
    "df_95=df_val[df_val.prob>.95]\n",
    "print(len(df_95[df_95.pred==df_95.true])/len(df_95), len(df_95)/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24508</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.588295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38411</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.362110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36405</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24741</th>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.900885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19093</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.978523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31912</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.795643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27189</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.594007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25468</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.955487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  true      prob\n",
       "24508     1     1  0.982052\n",
       "5806      8     8  0.588295\n",
       "38411     1     1  0.362110\n",
       "36405     1     1  0.987800\n",
       "24741    11    18  0.900885\n",
       "19093    13    13  0.978523\n",
       "31912     1     1  0.795643\n",
       "27189     4     4  0.594007\n",
       "25468     1     1  0.990625\n",
       "893      15     1  0.955487"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6548.0</td>\n",
       "      <td>0.895009</td>\n",
       "      <td>0.179004</td>\n",
       "      <td>0.132235</td>\n",
       "      <td>0.895889</td>\n",
       "      <td>0.984995</td>\n",
       "      <td>0.993459</td>\n",
       "      <td>0.996476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10703.0</td>\n",
       "      <td>0.888338</td>\n",
       "      <td>0.180936</td>\n",
       "      <td>0.147765</td>\n",
       "      <td>0.894780</td>\n",
       "      <td>0.978533</td>\n",
       "      <td>0.988133</td>\n",
       "      <td>0.992789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.787910</td>\n",
       "      <td>0.213254</td>\n",
       "      <td>0.143312</td>\n",
       "      <td>0.645671</td>\n",
       "      <td>0.885924</td>\n",
       "      <td>0.964104</td>\n",
       "      <td>0.988802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1815.0</td>\n",
       "      <td>0.883840</td>\n",
       "      <td>0.189944</td>\n",
       "      <td>0.144596</td>\n",
       "      <td>0.874203</td>\n",
       "      <td>0.977297</td>\n",
       "      <td>0.994552</td>\n",
       "      <td>0.997026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4308.0</td>\n",
       "      <td>0.855663</td>\n",
       "      <td>0.202555</td>\n",
       "      <td>0.164770</td>\n",
       "      <td>0.787208</td>\n",
       "      <td>0.971184</td>\n",
       "      <td>0.989906</td>\n",
       "      <td>0.996556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>803.0</td>\n",
       "      <td>0.696321</td>\n",
       "      <td>0.214642</td>\n",
       "      <td>0.173410</td>\n",
       "      <td>0.516907</td>\n",
       "      <td>0.728996</td>\n",
       "      <td>0.905332</td>\n",
       "      <td>0.967258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1774.0</td>\n",
       "      <td>0.641360</td>\n",
       "      <td>0.204593</td>\n",
       "      <td>0.135869</td>\n",
       "      <td>0.486512</td>\n",
       "      <td>0.668911</td>\n",
       "      <td>0.821526</td>\n",
       "      <td>0.939298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.403332</td>\n",
       "      <td>0.078925</td>\n",
       "      <td>0.238325</td>\n",
       "      <td>0.326465</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.468575</td>\n",
       "      <td>0.520184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1295.0</td>\n",
       "      <td>0.659798</td>\n",
       "      <td>0.234151</td>\n",
       "      <td>0.139096</td>\n",
       "      <td>0.456157</td>\n",
       "      <td>0.696444</td>\n",
       "      <td>0.883880</td>\n",
       "      <td>0.973886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1661.0</td>\n",
       "      <td>0.775016</td>\n",
       "      <td>0.225044</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>0.596288</td>\n",
       "      <td>0.877797</td>\n",
       "      <td>0.965056</td>\n",
       "      <td>0.990586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>801.0</td>\n",
       "      <td>0.673194</td>\n",
       "      <td>0.222252</td>\n",
       "      <td>0.174963</td>\n",
       "      <td>0.484500</td>\n",
       "      <td>0.710785</td>\n",
       "      <td>0.887294</td>\n",
       "      <td>0.962535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2736.0</td>\n",
       "      <td>0.774450</td>\n",
       "      <td>0.207941</td>\n",
       "      <td>0.158012</td>\n",
       "      <td>0.624839</td>\n",
       "      <td>0.879040</td>\n",
       "      <td>0.942594</td>\n",
       "      <td>0.970933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1678.0</td>\n",
       "      <td>0.873589</td>\n",
       "      <td>0.198027</td>\n",
       "      <td>0.151522</td>\n",
       "      <td>0.887529</td>\n",
       "      <td>0.971037</td>\n",
       "      <td>0.984812</td>\n",
       "      <td>0.995105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6183.0</td>\n",
       "      <td>0.899887</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.147650</td>\n",
       "      <td>0.909504</td>\n",
       "      <td>0.982217</td>\n",
       "      <td>0.992061</td>\n",
       "      <td>0.996537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>434.0</td>\n",
       "      <td>0.584106</td>\n",
       "      <td>0.198653</td>\n",
       "      <td>0.176205</td>\n",
       "      <td>0.410142</td>\n",
       "      <td>0.578708</td>\n",
       "      <td>0.757370</td>\n",
       "      <td>0.908809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4227.0</td>\n",
       "      <td>0.692852</td>\n",
       "      <td>0.228619</td>\n",
       "      <td>0.126495</td>\n",
       "      <td>0.499693</td>\n",
       "      <td>0.744948</td>\n",
       "      <td>0.905682</td>\n",
       "      <td>0.966472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>771.0</td>\n",
       "      <td>0.510157</td>\n",
       "      <td>0.195483</td>\n",
       "      <td>0.140117</td>\n",
       "      <td>0.345483</td>\n",
       "      <td>0.501816</td>\n",
       "      <td>0.669224</td>\n",
       "      <td>0.885243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>307.0</td>\n",
       "      <td>0.433083</td>\n",
       "      <td>0.192359</td>\n",
       "      <td>0.147781</td>\n",
       "      <td>0.285978</td>\n",
       "      <td>0.391007</td>\n",
       "      <td>0.553162</td>\n",
       "      <td>0.938684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5037.0</td>\n",
       "      <td>0.767665</td>\n",
       "      <td>0.253668</td>\n",
       "      <td>0.138688</td>\n",
       "      <td>0.560904</td>\n",
       "      <td>0.895692</td>\n",
       "      <td>0.988557</td>\n",
       "      <td>0.995088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>139.0</td>\n",
       "      <td>0.228219</td>\n",
       "      <td>0.048581</td>\n",
       "      <td>0.124310</td>\n",
       "      <td>0.192914</td>\n",
       "      <td>0.223080</td>\n",
       "      <td>0.264735</td>\n",
       "      <td>0.349515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>239.0</td>\n",
       "      <td>0.434444</td>\n",
       "      <td>0.131468</td>\n",
       "      <td>0.162479</td>\n",
       "      <td>0.329023</td>\n",
       "      <td>0.420549</td>\n",
       "      <td>0.541756</td>\n",
       "      <td>0.769116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.303136</td>\n",
       "      <td>0.066490</td>\n",
       "      <td>0.179689</td>\n",
       "      <td>0.265337</td>\n",
       "      <td>0.292203</td>\n",
       "      <td>0.351449</td>\n",
       "      <td>0.439437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2846.0</td>\n",
       "      <td>0.882077</td>\n",
       "      <td>0.199181</td>\n",
       "      <td>0.145349</td>\n",
       "      <td>0.883274</td>\n",
       "      <td>0.985817</td>\n",
       "      <td>0.994503</td>\n",
       "      <td>0.998688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.699827</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>0.152228</td>\n",
       "      <td>0.513699</td>\n",
       "      <td>0.761022</td>\n",
       "      <td>0.911728</td>\n",
       "      <td>0.973591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2346.0</td>\n",
       "      <td>0.801178</td>\n",
       "      <td>0.225391</td>\n",
       "      <td>0.140684</td>\n",
       "      <td>0.663363</td>\n",
       "      <td>0.914628</td>\n",
       "      <td>0.980358</td>\n",
       "      <td>0.994963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0      6548.0  0.895009  0.179004  0.132235  0.895889  0.984995  0.993459   \n",
       "1     10703.0  0.888338  0.180936  0.147765  0.894780  0.978533  0.988133   \n",
       "2      1265.0  0.787910  0.213254  0.143312  0.645671  0.885924  0.964104   \n",
       "3      1815.0  0.883840  0.189944  0.144596  0.874203  0.977297  0.994552   \n",
       "4      4308.0  0.855663  0.202555  0.164770  0.787208  0.971184  0.989906   \n",
       "5       803.0  0.696321  0.214642  0.173410  0.516907  0.728996  0.905332   \n",
       "6      1774.0  0.641360  0.204593  0.135869  0.486512  0.668911  0.821526   \n",
       "7        52.0  0.403332  0.078925  0.238325  0.326465  0.415584  0.468575   \n",
       "8      1295.0  0.659798  0.234151  0.139096  0.456157  0.696444  0.883880   \n",
       "9      1661.0  0.775016  0.225044  0.157899  0.596288  0.877797  0.965056   \n",
       "10      801.0  0.673194  0.222252  0.174963  0.484500  0.710785  0.887294   \n",
       "11     2736.0  0.774450  0.207941  0.158012  0.624839  0.879040  0.942594   \n",
       "12     1678.0  0.873589  0.198027  0.151522  0.887529  0.971037  0.984812   \n",
       "13     6183.0  0.899887  0.171034  0.147650  0.909504  0.982217  0.992061   \n",
       "14      434.0  0.584106  0.198653  0.176205  0.410142  0.578708  0.757370   \n",
       "15     4227.0  0.692852  0.228619  0.126495  0.499693  0.744948  0.905682   \n",
       "16      771.0  0.510157  0.195483  0.140117  0.345483  0.501816  0.669224   \n",
       "17      307.0  0.433083  0.192359  0.147781  0.285978  0.391007  0.553162   \n",
       "18     5037.0  0.767665  0.253668  0.138688  0.560904  0.895692  0.988557   \n",
       "19      139.0  0.228219  0.048581  0.124310  0.192914  0.223080  0.264735   \n",
       "20      239.0  0.434444  0.131468  0.162479  0.329023  0.420549  0.541756   \n",
       "21       25.0  0.303136  0.066490  0.179689  0.265337  0.292203  0.351449   \n",
       "22     2846.0  0.882077  0.199181  0.145349  0.883274  0.985817  0.994503   \n",
       "23     2007.0  0.699827  0.229800  0.152228  0.513699  0.761022  0.911728   \n",
       "24     2346.0  0.801178  0.225391  0.140684  0.663363  0.914628  0.980358   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     0.996476  \n",
       "1     0.992789  \n",
       "2     0.988802  \n",
       "3     0.997026  \n",
       "4     0.996556  \n",
       "5     0.967258  \n",
       "6     0.939298  \n",
       "7     0.520184  \n",
       "8     0.973886  \n",
       "9     0.990586  \n",
       "10    0.962535  \n",
       "11    0.970933  \n",
       "12    0.995105  \n",
       "13    0.996537  \n",
       "14    0.908809  \n",
       "15    0.966472  \n",
       "16    0.885243  \n",
       "17    0.938684  \n",
       "18    0.995088  \n",
       "19    0.349515  \n",
       "20    0.769116  \n",
       "21    0.439437  \n",
       "22    0.998688  \n",
       "23    0.973591  \n",
       "24    0.994963  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12602.0</td>\n",
       "      <td>0.848058</td>\n",
       "      <td>0.192906</td>\n",
       "      <td>0.099420</td>\n",
       "      <td>0.793265</td>\n",
       "      <td>0.942826</td>\n",
       "      <td>0.977441</td>\n",
       "      <td>0.995070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26000.0</td>\n",
       "      <td>0.809861</td>\n",
       "      <td>0.238692</td>\n",
       "      <td>0.087219</td>\n",
       "      <td>0.683788</td>\n",
       "      <td>0.946383</td>\n",
       "      <td>0.980377</td>\n",
       "      <td>0.993897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2297.0</td>\n",
       "      <td>0.605833</td>\n",
       "      <td>0.191890</td>\n",
       "      <td>0.094463</td>\n",
       "      <td>0.444121</td>\n",
       "      <td>0.639296</td>\n",
       "      <td>0.778246</td>\n",
       "      <td>0.894880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3366.0</td>\n",
       "      <td>0.874512</td>\n",
       "      <td>0.173787</td>\n",
       "      <td>0.157057</td>\n",
       "      <td>0.883541</td>\n",
       "      <td>0.956173</td>\n",
       "      <td>0.967872</td>\n",
       "      <td>0.974627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6441.0</td>\n",
       "      <td>0.741391</td>\n",
       "      <td>0.234529</td>\n",
       "      <td>0.126705</td>\n",
       "      <td>0.554008</td>\n",
       "      <td>0.835144</td>\n",
       "      <td>0.945148</td>\n",
       "      <td>0.982064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1366.0</td>\n",
       "      <td>0.386526</td>\n",
       "      <td>0.105981</td>\n",
       "      <td>0.138421</td>\n",
       "      <td>0.305284</td>\n",
       "      <td>0.385955</td>\n",
       "      <td>0.467187</td>\n",
       "      <td>0.617621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4108.0</td>\n",
       "      <td>0.498052</td>\n",
       "      <td>0.147095</td>\n",
       "      <td>0.093204</td>\n",
       "      <td>0.387617</td>\n",
       "      <td>0.518314</td>\n",
       "      <td>0.620197</td>\n",
       "      <td>0.768572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2320.0</td>\n",
       "      <td>0.352670</td>\n",
       "      <td>0.128075</td>\n",
       "      <td>0.114436</td>\n",
       "      <td>0.243909</td>\n",
       "      <td>0.338815</td>\n",
       "      <td>0.453435</td>\n",
       "      <td>0.642961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4780.0</td>\n",
       "      <td>0.656115</td>\n",
       "      <td>0.247266</td>\n",
       "      <td>0.120240</td>\n",
       "      <td>0.423637</td>\n",
       "      <td>0.723281</td>\n",
       "      <td>0.890782</td>\n",
       "      <td>0.956166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1540.0</td>\n",
       "      <td>0.504510</td>\n",
       "      <td>0.168473</td>\n",
       "      <td>0.120028</td>\n",
       "      <td>0.371044</td>\n",
       "      <td>0.496594</td>\n",
       "      <td>0.645774</td>\n",
       "      <td>0.853527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4314.0</td>\n",
       "      <td>0.715330</td>\n",
       "      <td>0.189366</td>\n",
       "      <td>0.112852</td>\n",
       "      <td>0.583356</td>\n",
       "      <td>0.787513</td>\n",
       "      <td>0.871542</td>\n",
       "      <td>0.940572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3242.0</td>\n",
       "      <td>0.877686</td>\n",
       "      <td>0.178106</td>\n",
       "      <td>0.129092</td>\n",
       "      <td>0.897202</td>\n",
       "      <td>0.958898</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>0.981479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11921.0</td>\n",
       "      <td>0.839911</td>\n",
       "      <td>0.196575</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.780404</td>\n",
       "      <td>0.941785</td>\n",
       "      <td>0.971170</td>\n",
       "      <td>0.987029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1574.0</td>\n",
       "      <td>0.418690</td>\n",
       "      <td>0.133496</td>\n",
       "      <td>0.140829</td>\n",
       "      <td>0.314354</td>\n",
       "      <td>0.406837</td>\n",
       "      <td>0.524091</td>\n",
       "      <td>0.754473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10648.0</td>\n",
       "      <td>0.547153</td>\n",
       "      <td>0.193563</td>\n",
       "      <td>0.109558</td>\n",
       "      <td>0.393835</td>\n",
       "      <td>0.548929</td>\n",
       "      <td>0.714959</td>\n",
       "      <td>0.908429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>418.0</td>\n",
       "      <td>0.222347</td>\n",
       "      <td>0.045120</td>\n",
       "      <td>0.103251</td>\n",
       "      <td>0.193242</td>\n",
       "      <td>0.220646</td>\n",
       "      <td>0.252260</td>\n",
       "      <td>0.344682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.130555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9300.0</td>\n",
       "      <td>0.676713</td>\n",
       "      <td>0.271094</td>\n",
       "      <td>0.113672</td>\n",
       "      <td>0.425342</td>\n",
       "      <td>0.741195</td>\n",
       "      <td>0.942738</td>\n",
       "      <td>0.991517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>87.0</td>\n",
       "      <td>0.169295</td>\n",
       "      <td>0.028792</td>\n",
       "      <td>0.112522</td>\n",
       "      <td>0.147757</td>\n",
       "      <td>0.165717</td>\n",
       "      <td>0.188771</td>\n",
       "      <td>0.247617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5613.0</td>\n",
       "      <td>0.813688</td>\n",
       "      <td>0.217035</td>\n",
       "      <td>0.120058</td>\n",
       "      <td>0.738421</td>\n",
       "      <td>0.924710</td>\n",
       "      <td>0.964420</td>\n",
       "      <td>0.985283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4228.0</td>\n",
       "      <td>0.618473</td>\n",
       "      <td>0.193106</td>\n",
       "      <td>0.099853</td>\n",
       "      <td>0.471122</td>\n",
       "      <td>0.656026</td>\n",
       "      <td>0.788790</td>\n",
       "      <td>0.900154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3834.0</td>\n",
       "      <td>0.724429</td>\n",
       "      <td>0.196630</td>\n",
       "      <td>0.097521</td>\n",
       "      <td>0.595665</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.883907</td>\n",
       "      <td>0.959967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0     12602.0  0.848058  0.192906  0.099420  0.793265  0.942826  0.977441   \n",
       "1     26000.0  0.809861  0.238692  0.087219  0.683788  0.946383  0.980377   \n",
       "2      2297.0  0.605833  0.191890  0.094463  0.444121  0.639296  0.778246   \n",
       "3      3366.0  0.874512  0.173787  0.157057  0.883541  0.956173  0.967872   \n",
       "4      6441.0  0.741391  0.234529  0.126705  0.554008  0.835144  0.945148   \n",
       "5      1366.0  0.386526  0.105981  0.138421  0.305284  0.385955  0.467187   \n",
       "6      4108.0  0.498052  0.147095  0.093204  0.387617  0.518314  0.620197   \n",
       "8      2320.0  0.352670  0.128075  0.114436  0.243909  0.338815  0.453435   \n",
       "9      4780.0  0.656115  0.247266  0.120240  0.423637  0.723281  0.890782   \n",
       "10     1540.0  0.504510  0.168473  0.120028  0.371044  0.496594  0.645774   \n",
       "11     4314.0  0.715330  0.189366  0.112852  0.583356  0.787513  0.871542   \n",
       "12     3242.0  0.877686  0.178106  0.129092  0.897202  0.958898  0.972096   \n",
       "13    11921.0  0.839911  0.196575  0.118600  0.780404  0.941785  0.971170   \n",
       "14     1574.0  0.418690  0.133496  0.140829  0.314354  0.406837  0.524091   \n",
       "15    10648.0  0.547153  0.193563  0.109558  0.393835  0.548929  0.714959   \n",
       "16      418.0  0.222347  0.045120  0.103251  0.193242  0.220646  0.252260   \n",
       "17        1.0  0.130555       NaN  0.130555  0.130555  0.130555  0.130555   \n",
       "18     9300.0  0.676713  0.271094  0.113672  0.425342  0.741195  0.942738   \n",
       "19       87.0  0.169295  0.028792  0.112522  0.147757  0.165717  0.188771   \n",
       "22     5613.0  0.813688  0.217035  0.120058  0.738421  0.924710  0.964420   \n",
       "23     4228.0  0.618473  0.193106  0.099853  0.471122  0.656026  0.788790   \n",
       "24     3834.0  0.724429  0.196630  0.097521  0.595665  0.792248  0.883907   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     0.995070  \n",
       "1     0.993897  \n",
       "2     0.894880  \n",
       "3     0.974627  \n",
       "4     0.982064  \n",
       "5     0.617621  \n",
       "6     0.768572  \n",
       "8     0.642961  \n",
       "9     0.956166  \n",
       "10    0.853527  \n",
       "11    0.940572  \n",
       "12    0.981479  \n",
       "13    0.987029  \n",
       "14    0.754473  \n",
       "15    0.908429  \n",
       "16    0.344682  \n",
       "17    0.130555  \n",
       "18    0.991517  \n",
       "19    0.247617  \n",
       "22    0.985283  \n",
       "23    0.900154  \n",
       "24    0.959967  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 200)         6692600   \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               102400512 \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 109,159,937\n",
      "Trainable params: 102,467,337\n",
      "Non-trainable params: 6,692,600\n",
      "_________________________________________________________________\n",
      "Train on 8399 samples, validate on 33601 samples\n",
      "Epoch 1/2\n",
      "8399/8399 [==============================] - 125s 15ms/step - loss: 0.2489 - acc: 0.9057 - precision: 0.1104 - recall: 0.9940 - val_loss: 0.2185 - val_acc: 0.9155 - val_precision: 0.1111 - val_recall: 1.0000\n",
      "Epoch 2/2\n",
      "8399/8399 [==============================] - 121s 14ms/step - loss: 0.1594 - acc: 0.9394 - precision: 0.1111 - recall: 1.0000 - val_loss: 0.2231 - val_acc: 0.9167 - val_precision: 0.1111 - val_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "'''\n",
    "x = GRU(units=128, activation='tanh', return_sequences=True)(embedded_sequences)\n",
    "\n",
    "x = LSTM(units=256, activation='tanh', return_sequences=False)(embedded_sequences)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#x = LSTM(units=128, activation='tanh', return_sequences=True)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "'''\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=9, activation='softmax')(x) #softmax\n",
    "\n",
    "# x = Dense(units=512, activation='relu')(x)\n",
    "# x = Dense(units=128, activation='relu')(x)\n",
    "# preds = Dense(units=25, activation='sigmoid')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 25s 412us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19112927243113517, 0.9295742606123288, 0.1111111119389534, 1.0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=500, verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37694"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val[df_val.pred==df_val.true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 7, 5, 3, 4, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_95.pred.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
