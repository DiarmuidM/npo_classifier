{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os, pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229472"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "\n",
    "len(df_train['mission_prgrm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_num=0\n",
    "while small_num<100: # Make sure each category has at least 100 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(60000)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "small_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"    \n",
    "    return string.strip().lower()\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(len(trainDF)):\n",
    "    text = trainDF['mission_prgrm'].values[idx]\n",
    "    texts.append(clean_str(text))\n",
    "    labels.append(ord(trainDF['NTEE1'].values[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/root/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85180 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "MAX_NB_WORDS = 2000\n",
    "MAX_SEQUENCE_LENGTH=1000\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (60000, 1000)\n",
      "Shape of label tensor: (60000, 26)\n"
     ]
    }
   ],
   "source": [
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "labels1 = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    pos = np.zeros((26,), dtype=int)\n",
    "    pos[labels[i]-65] = 1\n",
    "    labels1.append(np.asarray(pos))\n",
    "\n",
    "labels = np.asarray(labels1)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "VALIDATION_SPLIT = 0.7\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:nb_validation_samples]\n",
    "y_train = labels[:nb_validation_samples]\n",
    "x_val = data[nb_validation_samples:]\n",
    "y_val = labels[nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "EMBEDDING_DIM=50\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          4259050   \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 256)               235776    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 4,531,076\n",
      "Trainable params: 272,026\n",
      "Non-trainable params: 4,259,050\n",
      "_________________________________________________________________\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 539s 13ms/step - loss: 2.2178 - acc: 0.3788 - val_loss: 1.7512 - val_acc: 0.5171\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 536s 13ms/step - loss: 1.5608 - acc: 0.5719 - val_loss: 1.4280 - val_acc: 0.6092\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 538s 13ms/step - loss: 1.3421 - acc: 0.6318 - val_loss: 1.3378 - val_acc: 0.6301\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 535s 13ms/step - loss: 1.2319 - acc: 0.6612 - val_loss: 1.2482 - val_acc: 0.6596\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 542s 13ms/step - loss: 1.1608 - acc: 0.6795 - val_loss: 1.1689 - val_acc: 0.6801\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 541s 13ms/step - loss: 1.1067 - acc: 0.6935 - val_loss: 1.1638 - val_acc: 0.6816\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 539s 13ms/step - loss: 1.0652 - acc: 0.7050 - val_loss: 1.1291 - val_acc: 0.6929\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 537s 13ms/step - loss: 1.0242 - acc: 0.7159 - val_loss: 1.1210 - val_acc: 0.6966\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 540s 13ms/step - loss: 0.9921 - acc: 0.7239 - val_loss: 1.1131 - val_acc: 0.6986\n",
      "Epoch 10/20\n",
      " 6000/42000 [===>..........................] - ETA: 6:57 - loss: 0.9497 - acc: 0.7340"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = GRU(256, activation='tanh')(embedded_sequences)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=26, activation='softmax')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=500)\n",
    "\n",
    "#128,5 ; 5; 128,5; 5; 128; 5; 35; 128; 26 : 67.64; 256 instead of 128: 60%\n",
    "#128, 3,,,25,: 29.61\n",
    "#128-5, 5, dense 128, 26: 64.47\n",
    "# with activation function sigmlid: Accuracy doesn't increase above 17\n",
    "#selu: Overfits\n",
    "# elu: 58: Overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 14s 754us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.565594153271781, 0.6007777783605788]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, \n",
    "                   batch_size=500, verbose=1)\n",
    "\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
