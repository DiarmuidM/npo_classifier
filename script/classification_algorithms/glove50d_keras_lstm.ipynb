{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "import os, pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "\n",
    "len(df_train['mission_prgrm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_num=0\n",
    "while small_num<100: # Make sure each category has at least 100 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(60000)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "small_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"    \n",
    "    return string.strip().lower()\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(len(trainDF)):\n",
    "    text = trainDF['mission_prgrm'].values[idx]\n",
    "    texts.append(clean_str(text))\n",
    "    labels.append(ord(trainDF['NTEE1'].values[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "MAX_NB_WORDS = 2000\n",
    "MAX_SEQUENCE_LENGTH=1000\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (60000, 1000)\n",
      "Shape of label tensor: (60000, 26)\n"
     ]
    }
   ],
   "source": [
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "labels1 = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    pos = np.zeros((26,), dtype=int)\n",
    "    pos[labels[i]-65] = 1\n",
    "    labels1.append(np.asarray(pos))\n",
    "\n",
    "labels = np.asarray(labels1)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "VALIDATION_SPLIT = 0.7\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:nb_validation_samples]\n",
    "y_train = labels[:nb_validation_samples]\n",
    "x_val = data[nb_validation_samples:]\n",
    "y_val = labels[nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 255202 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "EMBEDDING_DIM=50\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, SimpleRNN, LSTM\n",
    "from keras.models import Model\n",
    "'''\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(filters=128, kernel_size=5, activation='tanh')(embedded_sequences)\n",
    "#x = GlobalMaxPooling1D()\n",
    "x = MaxPooling1D(pool_size=5)(x)\n",
    "x = Conv1D(filters=128, kernel_size=5, activation='tanh')(x)\n",
    "x = MaxPooling1D(pool_size=5)(x)\n",
    "x = Conv1D(filters=128, kernel_size=5, activation='tanh')(x)\n",
    "x = MaxPooling1D(pool_size=35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(units=256, activation='tanh')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=26, activation='softmax')(x) #softmax\n",
    "'''\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "#x = SimpleRNN(units=128, activation='tanh')(embedded_sequences)\n",
    "x = LSTM(units = 128, return_sequences=True, activation='tanh')(embedded_sequences) #retun_sequences=True\n",
    "x = LSTM(units = 256, activation='tanh')(x) #return_sequences=False\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=26, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=500)\n",
    "\n",
    "#128,5 ; 5; 128,5; 5; 128; 5; 35; 128; 26 : 67.64; 256 instead of 128: 60%\n",
    "#128, 3,,,25,: 29.61\n",
    "#128-5, 5, dense 128, 26: 64.47\n",
    "# with activation function sigmlid: Accuracy doesn't increase above 17\n",
    "#selu: Overfits\n",
    "# elu: 58: Overfits\n",
    "# tanh: 64.77\n",
    "#simpleRNN: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 14s 754us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.565594153271781, 0.6007777783605788]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, \n",
    "                   batch_size=500, verbose=1)\n",
    "\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
