{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154424"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path='../../dataset/df_ntee_universal/train/'\n",
    "file_list=os.listdir(train_file_path)\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, pd.read_pickle(train_file_path+file, compression='gzip')])\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154424, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['mission_prgrm']=df_train['mission']+' '+df_train['prgrm_dsc']\n",
    "df_train['mission_prgrm_spellchk']=df_train['mission_spellchk']+' '+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "len(df_train['mission_prgrm_spellchk']), len(df_train['NTEE1'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE1\n",
      "A    13657\n",
      "B    20728\n",
      "C     2655\n",
      "D     3379\n",
      "E     7152\n",
      "F     1832\n",
      "G     4010\n",
      "H      364\n",
      "I     2372\n",
      "J     3841\n",
      "K     1610\n",
      "L     4763\n",
      "M     3704\n",
      "N    12266\n",
      "O     1377\n",
      "P     7292\n",
      "Q     1592\n",
      "R      858\n",
      "S    11659\n",
      "T     1645\n",
      "U      795\n",
      "V      282\n",
      "W     6687\n",
      "X     3650\n",
      "Y     5369\n",
      "Name: EIN, dtype: int64 \n",
      "\n",
      " NTEE1\n",
      "A    3353\n",
      "B    5099\n",
      "C     668\n",
      "D     860\n",
      "E    1863\n",
      "F     469\n",
      "G    1043\n",
      "H     103\n",
      "I     575\n",
      "J     931\n",
      "K     399\n",
      "L    1179\n",
      "M     989\n",
      "N    3194\n",
      "O     354\n",
      "P    1888\n",
      "Q     395\n",
      "R     206\n",
      "S    2800\n",
      "T     387\n",
      "U     205\n",
      "V      68\n",
      "W    1670\n",
      "X     916\n",
      "Y    1271\n",
      "Name: EIN, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the sampling criteria can be satisfied.\n",
    "small_num=0\n",
    "while small_num<100: # Make sure each category in training dataset has at least 100 records.\n",
    "    trainDF, valDF = model_selection.train_test_split(df_train, test_size=.2)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "# See the composition by broad category.\n",
    "print(trainDF.groupby('NTEE1').count()['EIN'], '\\n'*2, valDF.groupby('NTEE1').count()['EIN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Prepare parrallel envionment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_naive_bayes(param_list):\n",
    "    global trainDF, valDF\n",
    "    input_text, classifier, tokenizer, vect_type, average_mtd = param_list\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    # Build training and testing data frame.\n",
    "    x_train=trainDF[input_text]\n",
    "    y_train=trainDF['NTEE1']\n",
    "    x_valid=valDF[input_text]\n",
    "    y_valid=valDF['NTEE1']\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "\n",
    "    def porter_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatize using POS tags, assume to improve accuracy.\n",
    "    # Ref: \n",
    "    #   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    #   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemma_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "        tokens=word_tokenize(str_input)\n",
    "        return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=lemma_tokenizer\n",
    "    elif tokenizer=='porter':\n",
    "        tokenizer=porter_tokenizer\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab.\n",
    "        vectorizer.fit(x_train)\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(x_train)\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    return {'input_text':input_text,\n",
    "            'classifier':str(classifier), \n",
    "            'tokenizer':tokenizer.__name__, \n",
    "            'vect_type':vect_type, \n",
    "            'average_mtd':average_mtd,\n",
    "            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "            'precision':metrics.precision_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "            'recall':metrics.recall_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "            'f1':metrics.f1_score(y_pred=predictions, y_true=y_valid, average=average_mtd)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of parameters.\n",
    "param_llist=[]\n",
    "for input_text in ['mission', 'prgrm_dsc', 'mission_prgrm', 'mission_spellchk', 'prgrm_dsc_spellchk', 'mission_prgrm_spellchk']:\n",
    "    for classifier in [naive_bayes.MultinomialNB(), naive_bayes.ComplementNB()]:\n",
    "        for tokenizer in ['lemma', 'porter']:\n",
    "            for vect_type in ['count', 'tfidf']:\n",
    "                for average_mtd in ['macro', 'weighted']:\n",
    "                    param_llist+=[[input_text, classifier, tokenizer, vect_type, average_mtd]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=Pool(24)\n",
    "df_performance=pd.DataFrame(p.map(func_naive_bayes, param_llist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"8\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.241269</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.235833</td>\n",
       "      <td>0.240069</td>\n",
       "      <td>0.241569</td>\n",
       "      <td>0.242604</td>\n",
       "      <td>0.244778</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.093399</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.089127</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>0.093258</td>\n",
       "      <td>0.095179</td>\n",
       "      <td>0.097384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.238564</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.232667</td>\n",
       "      <td>0.236396</td>\n",
       "      <td>0.237958</td>\n",
       "      <td>0.240625</td>\n",
       "      <td>0.245556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.108314</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.104967</td>\n",
       "      <td>0.107377</td>\n",
       "      <td>0.108080</td>\n",
       "      <td>0.109486</td>\n",
       "      <td>0.112052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.240762</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.235306</td>\n",
       "      <td>0.239028</td>\n",
       "      <td>0.240431</td>\n",
       "      <td>0.242806</td>\n",
       "      <td>0.245556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.092774</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.087243</td>\n",
       "      <td>0.091331</td>\n",
       "      <td>0.093029</td>\n",
       "      <td>0.094016</td>\n",
       "      <td>0.098436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.238947</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.236813</td>\n",
       "      <td>0.238861</td>\n",
       "      <td>0.240194</td>\n",
       "      <td>0.245972</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.108256</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.107222</td>\n",
       "      <td>0.108431</td>\n",
       "      <td>0.108919</td>\n",
       "      <td>0.112142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.263319</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.257361</td>\n",
       "      <td>0.261604</td>\n",
       "      <td>0.263056</td>\n",
       "      <td>0.265132</td>\n",
       "      <td>0.269222</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.163160</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.157605</td>\n",
       "      <td>0.161699</td>\n",
       "      <td>0.163116</td>\n",
       "      <td>0.164495</td>\n",
       "      <td>0.167805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.165811</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.164382</td>\n",
       "      <td>0.165625</td>\n",
       "      <td>0.167153</td>\n",
       "      <td>0.169306</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.011620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.263534</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.257250</td>\n",
       "      <td>0.261354</td>\n",
       "      <td>0.263750</td>\n",
       "      <td>0.265222</td>\n",
       "      <td>0.269250</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.163105</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.158064</td>\n",
       "      <td>0.161268</td>\n",
       "      <td>0.163216</td>\n",
       "      <td>0.164914</td>\n",
       "      <td>0.168329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.166251</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.161611</td>\n",
       "      <td>0.165111</td>\n",
       "      <td>0.166361</td>\n",
       "      <td>0.167458</td>\n",
       "      <td>0.168722</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.011459</td>\n",
       "      <td>0.011527</td>\n",
       "      <td>0.011642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              accuracy  \\\n",
       "                                                                                 count   \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "                                                   porter_tokenizer count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "                                                   porter_tokenizer count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                   mean   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.241269   \n",
       "                                                                    tfidf      0.238564   \n",
       "                                                   porter_tokenizer count      0.240762   \n",
       "                                                                    tfidf      0.238947   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.263319   \n",
       "                                                                    tfidf      0.165811   \n",
       "                                                   porter_tokenizer count      0.263534   \n",
       "                                                                    tfidf      0.166251   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    std   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002266   \n",
       "                                                                    tfidf      0.002922   \n",
       "                                                   porter_tokenizer count      0.002503   \n",
       "                                                                    tfidf      0.002754   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002883   \n",
       "                                                                    tfidf      0.001591   \n",
       "                                                   porter_tokenizer count      0.002993   \n",
       "                                                                    tfidf      0.001728   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    min   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.235833   \n",
       "                                                                    tfidf      0.232667   \n",
       "                                                   porter_tokenizer count      0.235306   \n",
       "                                                                    tfidf      0.233333   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.257361   \n",
       "                                                                    tfidf      0.163000   \n",
       "                                                   porter_tokenizer count      0.257250   \n",
       "                                                                    tfidf      0.161611   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    25%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.240069   \n",
       "                                                                    tfidf      0.236396   \n",
       "                                                   porter_tokenizer count      0.239028   \n",
       "                                                                    tfidf      0.236813   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.261604   \n",
       "                                                                    tfidf      0.164382   \n",
       "                                                   porter_tokenizer count      0.261354   \n",
       "                                                                    tfidf      0.165111   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    50%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.241569   \n",
       "                                                                    tfidf      0.237958   \n",
       "                                                   porter_tokenizer count      0.240431   \n",
       "                                                                    tfidf      0.238861   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.263056   \n",
       "                                                                    tfidf      0.165625   \n",
       "                                                   porter_tokenizer count      0.263750   \n",
       "                                                                    tfidf      0.166361   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    75%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.242604   \n",
       "                                                                    tfidf      0.240625   \n",
       "                                                   porter_tokenizer count      0.242806   \n",
       "                                                                    tfidf      0.240194   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.265132   \n",
       "                                                                    tfidf      0.167153   \n",
       "                                                   porter_tokenizer count      0.265222   \n",
       "                                                                    tfidf      0.167458   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    max   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.244778   \n",
       "                                                                    tfidf      0.245556   \n",
       "                                                   porter_tokenizer count      0.245556   \n",
       "                                                                    tfidf      0.245972   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.269222   \n",
       "                                                                    tfidf      0.169306   \n",
       "                                                   porter_tokenizer count      0.269250   \n",
       "                                                                    tfidf      0.168722   \n",
       "\n",
       "                                                                                 f1  \\\n",
       "                                                                              count   \n",
       "classifier                                         tokenizer        vect_type         \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "                                                   porter_tokenizer count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "                                                   porter_tokenizer count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                   mean   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.093399   \n",
       "                                                                    tfidf      0.108314   \n",
       "                                                   porter_tokenizer count      0.092774   \n",
       "                                                                    tfidf      0.108256   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.163160   \n",
       "                                                                    tfidf      0.011404   \n",
       "                                                   porter_tokenizer count      0.163105   \n",
       "                                                                    tfidf      0.011448   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    std   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002126   \n",
       "                                                                    tfidf      0.001699   \n",
       "                                                   porter_tokenizer count      0.002300   \n",
       "                                                                    tfidf      0.001617   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002632   \n",
       "                                                                    tfidf      0.000103   \n",
       "                                                   porter_tokenizer count      0.002602   \n",
       "                                                                    tfidf      0.000120   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    min   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.089127   \n",
       "                                                                    tfidf      0.104967   \n",
       "                                                   porter_tokenizer count      0.087243   \n",
       "                                                                    tfidf      0.105023   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.157605   \n",
       "                                                                    tfidf      0.011213   \n",
       "                                                   porter_tokenizer count      0.158064   \n",
       "                                                                    tfidf      0.011165   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    25%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.092017   \n",
       "                                                                    tfidf      0.107377   \n",
       "                                                   porter_tokenizer count      0.091331   \n",
       "                                                                    tfidf      0.107222   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.161699   \n",
       "                                                                    tfidf      0.011322   \n",
       "                                                   porter_tokenizer count      0.161268   \n",
       "                                                                    tfidf      0.011385   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    50%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.093258   \n",
       "                                                                    tfidf      0.108080   \n",
       "                                                   porter_tokenizer count      0.093029   \n",
       "                                                                    tfidf      0.108431   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.163116   \n",
       "                                                                    tfidf      0.011400   \n",
       "                                                   porter_tokenizer count      0.163216   \n",
       "                                                                    tfidf      0.011459   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    75%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.095179   \n",
       "                                                                    tfidf      0.109486   \n",
       "                                                   porter_tokenizer count      0.094016   \n",
       "                                                                    tfidf      0.108919   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.164495   \n",
       "                                                                    tfidf      0.011481   \n",
       "                                                   porter_tokenizer count      0.164914   \n",
       "                                                                    tfidf      0.011527   \n",
       "\n",
       "                                                                                         \n",
       "                                                                                    max  \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.097384  \n",
       "                                                                    tfidf      0.112052  \n",
       "                                                   porter_tokenizer count      0.098436  \n",
       "                                                                    tfidf      0.112142  \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.167805  \n",
       "                                                                    tfidf      0.011620  \n",
       "                                                   porter_tokenizer count      0.168329  \n",
       "                                                                    tfidf      0.011642  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance[df_performance.average_mtd=='macro'].groupby(['classifier', 'tokenizer', 'vect_type']).describe()[['accuracy','f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [ensemble.RandomForestClassifier()]:\n",
    "    for tokenizer in ['lemma', 'porter']:\n",
    "        for vect_type in ['count', 'tfidf']:\n",
    "            for average_mtd in ['macro', 'weighted']:\n",
    "                dview['classifier']=classifier\n",
    "                dview['tokenizer']=tokenizer\n",
    "                dview['vect_type']=vect_type\n",
    "                dview['average_mtd']=average_mtd\n",
    "                t=func_naive_bayes.map(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>9</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.279592</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.255527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>14</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.235833</td>\n",
       "      <td>0.190699</td>\n",
       "      <td>0.105259</td>\n",
       "      <td>0.092207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.278028</td>\n",
       "      <td>0.280232</td>\n",
       "      <td>0.278028</td>\n",
       "      <td>0.254919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>16</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.277972</td>\n",
       "      <td>0.278541</td>\n",
       "      <td>0.277972</td>\n",
       "      <td>0.254734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.274389</td>\n",
       "      <td>0.266772</td>\n",
       "      <td>0.147704</td>\n",
       "      <td>0.160976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>5</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.237528</td>\n",
       "      <td>0.182862</td>\n",
       "      <td>0.103287</td>\n",
       "      <td>0.090467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>23</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.235889</td>\n",
       "      <td>0.237583</td>\n",
       "      <td>0.235889</td>\n",
       "      <td>0.205232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>22</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.167333</td>\n",
       "      <td>0.043979</td>\n",
       "      <td>0.167333</td>\n",
       "      <td>0.048070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>10</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.165833</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.011380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>8</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.281472</td>\n",
       "      <td>0.285078</td>\n",
       "      <td>0.281472</td>\n",
       "      <td>0.259149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial                                         classifier  \\\n",
       "235     9  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "344    14  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "95      3  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "403    16  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "94      3  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "132     5  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "567    23  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "531    22  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "246    10  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "215     8  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "\n",
       "            tokenizer vect_type average_mtd  accuracy  precision    recall  \\\n",
       "235   lemma_tokenizer     tfidf    weighted  0.278000   0.279592  0.278000   \n",
       "344   lemma_tokenizer     count       macro  0.235833   0.190699  0.105259   \n",
       "95   porter_tokenizer     tfidf    weighted  0.278028   0.280232  0.278028   \n",
       "403   lemma_tokenizer     tfidf    weighted  0.277972   0.278541  0.277972   \n",
       "94   porter_tokenizer     tfidf       macro  0.274389   0.266772  0.147704   \n",
       "132  porter_tokenizer     count       macro  0.237528   0.182862  0.103287   \n",
       "567  porter_tokenizer     tfidf    weighted  0.235889   0.237583  0.235889   \n",
       "531   lemma_tokenizer     tfidf    weighted  0.167333   0.043979  0.167333   \n",
       "246  porter_tokenizer     tfidf       macro  0.165833   0.006634  0.040000   \n",
       "215  porter_tokenizer     tfidf    weighted  0.281472   0.285078  0.281472   \n",
       "\n",
       "           f1  \n",
       "235  0.255527  \n",
       "344  0.092207  \n",
       "95   0.254919  \n",
       "403  0.254734  \n",
       "94   0.160976  \n",
       "132  0.090467  \n",
       "567  0.205232  \n",
       "531  0.048070  \n",
       "246  0.011380  \n",
       "215  0.259149  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
