{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234027"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234027, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['input_text']=df_train['mission_spellchk']+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_train['input_text']=[' '.join(s) for s in df_train['input_text']]\n",
    "len(df_train['input_text']), len(df_train['NTEE1'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE1\n",
      "A    10597\n",
      "B    16034\n",
      "C     2155\n",
      "D     2689\n",
      "E     6056\n",
      "F     1462\n",
      "G     3212\n",
      "H      325\n",
      "I     1765\n",
      "J     2817\n",
      "K     1233\n",
      "L     3703\n",
      "M     2753\n",
      "N     9230\n",
      "O     1042\n",
      "P     6057\n",
      "Q     1268\n",
      "R      730\n",
      "S     8844\n",
      "T     1359\n",
      "U      654\n",
      "V      224\n",
      "W     5108\n",
      "X     2727\n",
      "Y     3956\n",
      "Name: EIN, dtype: int64 \n",
      "\n",
      " NTEE1\n",
      "A    2623\n",
      "B    3942\n",
      "C     534\n",
      "D     650\n",
      "E    1543\n",
      "F     395\n",
      "G     870\n",
      "H      87\n",
      "I     437\n",
      "J     752\n",
      "K     325\n",
      "L     933\n",
      "M     687\n",
      "N    2329\n",
      "O     274\n",
      "P    1582\n",
      "Q     322\n",
      "R     159\n",
      "S    2084\n",
      "T     366\n",
      "U     146\n",
      "V      59\n",
      "W    1232\n",
      "X     699\n",
      "Y     970\n",
      "Name: EIN, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the sampling criteria can be satisfied.\n",
    "small_num=0\n",
    "while small_num<200: # Make sure each category has at least 200 records.\n",
    "    sampleDF = df_train[df_train.input_text.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "    trainDF, valDF =train_test_split(sampleDF, test_size=.2)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "# See the composition by broad category.\n",
    "print(trainDF.groupby('NTEE1').count()['EIN'], '\\n'*2, valDF.groupby('NTEE1').count()['EIN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Prepare parrallel envionment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "print(c.ids)\n",
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.execute('from sklearn import model_selection, preprocessing, naive_bayes, metrics')\n",
    "dview.execute('from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer')\n",
    "dview.execute('from sklearn import decomposition, ensemble')\n",
    "dview.execute('from nltk.stem import PorterStemmer')\n",
    "dview.execute('from nltk import word_tokenize')\n",
    "dview.execute('from nltk.stem import WordNetLemmatizer')\n",
    "dview.execute('from nltk.corpus import wordnet')\n",
    "dview.execute('import pandas as pd')\n",
    "dview.execute('import nltk')\n",
    "dview.execute('from sklearn.model_selection import train_test_split')\n",
    "dview['df_train']=df_train\n",
    "dview['df_performance']=pd.DataFrame(columns=['trial', 'classifier', 'tokenizer', 'vect_type', 'average_mtd', 'accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_naive_bayes(trial):\n",
    "    global df_train, df_performance, classifier, tokenizer, vect_type, average_mtd\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    # Build training and testing data frame.\n",
    "    small_num=0\n",
    "    while small_num<200: # Make sure each category has at least 500 records.\n",
    "        sampleDF = df_train[df_train.input_text.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "        trainDF, valDF = train_test_split(sampleDF, test_size=.3)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    x_train=trainDF['input_text']\n",
    "    y_train=trainDF['NTEE1']\n",
    "    x_valid=valDF['input_text']\n",
    "    y_valid=valDF['NTEE1']\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "\n",
    "    def porter_tokenizer(token_list):\n",
    "        return [PorterStemmer().stem(token) for token in token_list]\n",
    "    \n",
    "    # Lemmatize using POS tags, assume to improve accuracy.\n",
    "    # Ref: \n",
    "    #   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    #   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemma_tokenizer(token_list):\n",
    "        return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(token_list)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=lemma_tokenizer\n",
    "    elif tokenizer=='porter':\n",
    "        tokenizer=porter_tokenizer\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab.\n",
    "        vectorizer.fit(trainDF['input_text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['input_text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':tokenizer.__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                                            'f1':metrics.f1_score(y_pred=predictions, y_true=y_valid, average=average_mtd)\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Iterate different configurations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [naive_bayes.MultinomialNB(), naive_bayes.ComplementNB()]:\n",
    "    for tokenizer in ['lemma', 'porter']:\n",
    "        for vect_type in ['count', 'tfidf']:\n",
    "            for average_mtd in ['macro', 'weighted']:\n",
    "                dview['classifier']=classifier\n",
    "                dview['tokenizer']=tokenizer\n",
    "                dview['vect_type']=vect_type\n",
    "                dview['average_mtd']=average_mtd\n",
    "                t=func_naive_bayes.map(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>12</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.240194</td>\n",
       "      <td>0.251291</td>\n",
       "      <td>0.240194</td>\n",
       "      <td>0.195009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.239194</td>\n",
       "      <td>0.237461</td>\n",
       "      <td>0.239194</td>\n",
       "      <td>0.206953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.234611</td>\n",
       "      <td>0.235620</td>\n",
       "      <td>0.234611</td>\n",
       "      <td>0.204730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.165278</td>\n",
       "      <td>0.052585</td>\n",
       "      <td>0.165278</td>\n",
       "      <td>0.046978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.260389</td>\n",
       "      <td>0.195677</td>\n",
       "      <td>0.167313</td>\n",
       "      <td>0.162198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>12</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.235528</td>\n",
       "      <td>0.149415</td>\n",
       "      <td>0.117563</td>\n",
       "      <td>0.105657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.238333</td>\n",
       "      <td>0.144291</td>\n",
       "      <td>0.119297</td>\n",
       "      <td>0.107599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>13</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.238972</td>\n",
       "      <td>0.168982</td>\n",
       "      <td>0.119402</td>\n",
       "      <td>0.109173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.268694</td>\n",
       "      <td>0.202216</td>\n",
       "      <td>0.172679</td>\n",
       "      <td>0.167414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>18</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.262333</td>\n",
       "      <td>0.262380</td>\n",
       "      <td>0.262333</td>\n",
       "      <td>0.244441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial                                         classifier  \\\n",
       "201    12  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "31      1  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "59      3  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "87      5  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "64      4  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "202    12  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "30      1  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "218    13  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "96      6  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "289    18  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "            tokenizer vect_type average_mtd  accuracy  precision    recall  \\\n",
       "201   lemma_tokenizer     count    weighted  0.240194   0.251291  0.240194   \n",
       "31   porter_tokenizer     tfidf    weighted  0.239194   0.237461  0.239194   \n",
       "59    lemma_tokenizer     tfidf    weighted  0.234611   0.235620  0.234611   \n",
       "87   porter_tokenizer     tfidf    weighted  0.165278   0.052585  0.165278   \n",
       "64    lemma_tokenizer     count       macro  0.260389   0.195677  0.167313   \n",
       "202   lemma_tokenizer     tfidf       macro  0.235528   0.149415  0.117563   \n",
       "30   porter_tokenizer     tfidf       macro  0.238333   0.144291  0.119297   \n",
       "218   lemma_tokenizer     tfidf       macro  0.238972   0.168982  0.119402   \n",
       "96    lemma_tokenizer     count       macro  0.268694   0.202216  0.172679   \n",
       "289   lemma_tokenizer     count    weighted  0.262333   0.262380  0.262333   \n",
       "\n",
       "           f1  \n",
       "201  0.195009  \n",
       "31   0.206953  \n",
       "59   0.204730  \n",
       "87   0.046978  \n",
       "64   0.162198  \n",
       "202  0.105657  \n",
       "30   0.107599  \n",
       "218  0.109173  \n",
       "96   0.167414  \n",
       "289  0.244441  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"8\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.241269</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.235833</td>\n",
       "      <td>0.240069</td>\n",
       "      <td>0.241569</td>\n",
       "      <td>0.242604</td>\n",
       "      <td>0.244778</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.093399</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.089127</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>0.093258</td>\n",
       "      <td>0.095179</td>\n",
       "      <td>0.097384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.238564</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.232667</td>\n",
       "      <td>0.236396</td>\n",
       "      <td>0.237958</td>\n",
       "      <td>0.240625</td>\n",
       "      <td>0.245556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.108314</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.104967</td>\n",
       "      <td>0.107377</td>\n",
       "      <td>0.108080</td>\n",
       "      <td>0.109486</td>\n",
       "      <td>0.112052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.240762</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.235306</td>\n",
       "      <td>0.239028</td>\n",
       "      <td>0.240431</td>\n",
       "      <td>0.242806</td>\n",
       "      <td>0.245556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.092774</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.087243</td>\n",
       "      <td>0.091331</td>\n",
       "      <td>0.093029</td>\n",
       "      <td>0.094016</td>\n",
       "      <td>0.098436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.238947</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.236813</td>\n",
       "      <td>0.238861</td>\n",
       "      <td>0.240194</td>\n",
       "      <td>0.245972</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.108256</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.107222</td>\n",
       "      <td>0.108431</td>\n",
       "      <td>0.108919</td>\n",
       "      <td>0.112142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.263319</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.257361</td>\n",
       "      <td>0.261604</td>\n",
       "      <td>0.263056</td>\n",
       "      <td>0.265132</td>\n",
       "      <td>0.269222</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.163160</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.157605</td>\n",
       "      <td>0.161699</td>\n",
       "      <td>0.163116</td>\n",
       "      <td>0.164495</td>\n",
       "      <td>0.167805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.165811</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.164382</td>\n",
       "      <td>0.165625</td>\n",
       "      <td>0.167153</td>\n",
       "      <td>0.169306</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.011620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.263534</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.257250</td>\n",
       "      <td>0.261354</td>\n",
       "      <td>0.263750</td>\n",
       "      <td>0.265222</td>\n",
       "      <td>0.269250</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.163105</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.158064</td>\n",
       "      <td>0.161268</td>\n",
       "      <td>0.163216</td>\n",
       "      <td>0.164914</td>\n",
       "      <td>0.168329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.166251</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.161611</td>\n",
       "      <td>0.165111</td>\n",
       "      <td>0.166361</td>\n",
       "      <td>0.167458</td>\n",
       "      <td>0.168722</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.011459</td>\n",
       "      <td>0.011527</td>\n",
       "      <td>0.011642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              accuracy  \\\n",
       "                                                                                 count   \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "                                                   porter_tokenizer count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "                                                   porter_tokenizer count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                   mean   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.241269   \n",
       "                                                                    tfidf      0.238564   \n",
       "                                                   porter_tokenizer count      0.240762   \n",
       "                                                                    tfidf      0.238947   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.263319   \n",
       "                                                                    tfidf      0.165811   \n",
       "                                                   porter_tokenizer count      0.263534   \n",
       "                                                                    tfidf      0.166251   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    std   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002266   \n",
       "                                                                    tfidf      0.002922   \n",
       "                                                   porter_tokenizer count      0.002503   \n",
       "                                                                    tfidf      0.002754   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002883   \n",
       "                                                                    tfidf      0.001591   \n",
       "                                                   porter_tokenizer count      0.002993   \n",
       "                                                                    tfidf      0.001728   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    min   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.235833   \n",
       "                                                                    tfidf      0.232667   \n",
       "                                                   porter_tokenizer count      0.235306   \n",
       "                                                                    tfidf      0.233333   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.257361   \n",
       "                                                                    tfidf      0.163000   \n",
       "                                                   porter_tokenizer count      0.257250   \n",
       "                                                                    tfidf      0.161611   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    25%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.240069   \n",
       "                                                                    tfidf      0.236396   \n",
       "                                                   porter_tokenizer count      0.239028   \n",
       "                                                                    tfidf      0.236813   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.261604   \n",
       "                                                                    tfidf      0.164382   \n",
       "                                                   porter_tokenizer count      0.261354   \n",
       "                                                                    tfidf      0.165111   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    50%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.241569   \n",
       "                                                                    tfidf      0.237958   \n",
       "                                                   porter_tokenizer count      0.240431   \n",
       "                                                                    tfidf      0.238861   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.263056   \n",
       "                                                                    tfidf      0.165625   \n",
       "                                                   porter_tokenizer count      0.263750   \n",
       "                                                                    tfidf      0.166361   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    75%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.242604   \n",
       "                                                                    tfidf      0.240625   \n",
       "                                                   porter_tokenizer count      0.242806   \n",
       "                                                                    tfidf      0.240194   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.265132   \n",
       "                                                                    tfidf      0.167153   \n",
       "                                                   porter_tokenizer count      0.265222   \n",
       "                                                                    tfidf      0.167458   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    max   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.244778   \n",
       "                                                                    tfidf      0.245556   \n",
       "                                                   porter_tokenizer count      0.245556   \n",
       "                                                                    tfidf      0.245972   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.269222   \n",
       "                                                                    tfidf      0.169306   \n",
       "                                                   porter_tokenizer count      0.269250   \n",
       "                                                                    tfidf      0.168722   \n",
       "\n",
       "                                                                                 f1  \\\n",
       "                                                                              count   \n",
       "classifier                                         tokenizer        vect_type         \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "                                                   porter_tokenizer count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "                                                   porter_tokenizer count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                   mean   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.093399   \n",
       "                                                                    tfidf      0.108314   \n",
       "                                                   porter_tokenizer count      0.092774   \n",
       "                                                                    tfidf      0.108256   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.163160   \n",
       "                                                                    tfidf      0.011404   \n",
       "                                                   porter_tokenizer count      0.163105   \n",
       "                                                                    tfidf      0.011448   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    std   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002126   \n",
       "                                                                    tfidf      0.001699   \n",
       "                                                   porter_tokenizer count      0.002300   \n",
       "                                                                    tfidf      0.001617   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002632   \n",
       "                                                                    tfidf      0.000103   \n",
       "                                                   porter_tokenizer count      0.002602   \n",
       "                                                                    tfidf      0.000120   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    min   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.089127   \n",
       "                                                                    tfidf      0.104967   \n",
       "                                                   porter_tokenizer count      0.087243   \n",
       "                                                                    tfidf      0.105023   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.157605   \n",
       "                                                                    tfidf      0.011213   \n",
       "                                                   porter_tokenizer count      0.158064   \n",
       "                                                                    tfidf      0.011165   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    25%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.092017   \n",
       "                                                                    tfidf      0.107377   \n",
       "                                                   porter_tokenizer count      0.091331   \n",
       "                                                                    tfidf      0.107222   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.161699   \n",
       "                                                                    tfidf      0.011322   \n",
       "                                                   porter_tokenizer count      0.161268   \n",
       "                                                                    tfidf      0.011385   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    50%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.093258   \n",
       "                                                                    tfidf      0.108080   \n",
       "                                                   porter_tokenizer count      0.093029   \n",
       "                                                                    tfidf      0.108431   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.163116   \n",
       "                                                                    tfidf      0.011400   \n",
       "                                                   porter_tokenizer count      0.163216   \n",
       "                                                                    tfidf      0.011459   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    75%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.095179   \n",
       "                                                                    tfidf      0.109486   \n",
       "                                                   porter_tokenizer count      0.094016   \n",
       "                                                                    tfidf      0.108919   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.164495   \n",
       "                                                                    tfidf      0.011481   \n",
       "                                                   porter_tokenizer count      0.164914   \n",
       "                                                                    tfidf      0.011527   \n",
       "\n",
       "                                                                                         \n",
       "                                                                                    max  \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.097384  \n",
       "                                                                    tfidf      0.112052  \n",
       "                                                   porter_tokenizer count      0.098436  \n",
       "                                                                    tfidf      0.112142  \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.167805  \n",
       "                                                                    tfidf      0.011620  \n",
       "                                                   porter_tokenizer count      0.168329  \n",
       "                                                                    tfidf      0.011642  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance[df_performance.average_mtd=='macro'].groupby(['classifier', 'tokenizer', 'vect_type']).describe()[['accuracy','f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [ensemble.RandomForestClassifier()]:\n",
    "    for tokenizer in ['lemma', 'porter']:\n",
    "        for vect_type in ['count', 'tfidf']:\n",
    "            for average_mtd in ['macro', 'weighted']:\n",
    "                dview['classifier']=classifier\n",
    "                dview['tokenizer']=tokenizer\n",
    "                dview['vect_type']=vect_type\n",
    "                dview['average_mtd']=average_mtd\n",
    "                t=func_naive_bayes.map(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>9</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.279592</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.255527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>14</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.235833</td>\n",
       "      <td>0.190699</td>\n",
       "      <td>0.105259</td>\n",
       "      <td>0.092207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.278028</td>\n",
       "      <td>0.280232</td>\n",
       "      <td>0.278028</td>\n",
       "      <td>0.254919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>16</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.277972</td>\n",
       "      <td>0.278541</td>\n",
       "      <td>0.277972</td>\n",
       "      <td>0.254734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.274389</td>\n",
       "      <td>0.266772</td>\n",
       "      <td>0.147704</td>\n",
       "      <td>0.160976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>5</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.237528</td>\n",
       "      <td>0.182862</td>\n",
       "      <td>0.103287</td>\n",
       "      <td>0.090467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>23</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.235889</td>\n",
       "      <td>0.237583</td>\n",
       "      <td>0.235889</td>\n",
       "      <td>0.205232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>22</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.167333</td>\n",
       "      <td>0.043979</td>\n",
       "      <td>0.167333</td>\n",
       "      <td>0.048070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>10</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.165833</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.011380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>8</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, class_w...</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.281472</td>\n",
       "      <td>0.285078</td>\n",
       "      <td>0.281472</td>\n",
       "      <td>0.259149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial                                         classifier  \\\n",
       "235     9  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "344    14  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "95      3  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "403    16  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "94      3  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "132     5  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "567    23  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "531    22  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "246    10  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "215     8  RandomForestClassifier(bootstrap=True, class_w...   \n",
       "\n",
       "            tokenizer vect_type average_mtd  accuracy  precision    recall  \\\n",
       "235   lemma_tokenizer     tfidf    weighted  0.278000   0.279592  0.278000   \n",
       "344   lemma_tokenizer     count       macro  0.235833   0.190699  0.105259   \n",
       "95   porter_tokenizer     tfidf    weighted  0.278028   0.280232  0.278028   \n",
       "403   lemma_tokenizer     tfidf    weighted  0.277972   0.278541  0.277972   \n",
       "94   porter_tokenizer     tfidf       macro  0.274389   0.266772  0.147704   \n",
       "132  porter_tokenizer     count       macro  0.237528   0.182862  0.103287   \n",
       "567  porter_tokenizer     tfidf    weighted  0.235889   0.237583  0.235889   \n",
       "531   lemma_tokenizer     tfidf    weighted  0.167333   0.043979  0.167333   \n",
       "246  porter_tokenizer     tfidf       macro  0.165833   0.006634  0.040000   \n",
       "215  porter_tokenizer     tfidf    weighted  0.281472   0.285078  0.281472   \n",
       "\n",
       "           f1  \n",
       "235  0.255527  \n",
       "344  0.092207  \n",
       "95   0.254919  \n",
       "403  0.254734  \n",
       "94   0.160976  \n",
       "132  0.090467  \n",
       "567  0.205232  \n",
       "531  0.048070  \n",
       "246  0.011380  \n",
       "215  0.259149  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"8\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.241269</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.235833</td>\n",
       "      <td>0.240069</td>\n",
       "      <td>0.241569</td>\n",
       "      <td>0.242604</td>\n",
       "      <td>0.244778</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.093399</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.089127</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>0.093258</td>\n",
       "      <td>0.095179</td>\n",
       "      <td>0.097384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.238564</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.232667</td>\n",
       "      <td>0.236396</td>\n",
       "      <td>0.237958</td>\n",
       "      <td>0.240625</td>\n",
       "      <td>0.245556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.108314</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.104967</td>\n",
       "      <td>0.107377</td>\n",
       "      <td>0.108080</td>\n",
       "      <td>0.109486</td>\n",
       "      <td>0.112052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.240762</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.235306</td>\n",
       "      <td>0.239028</td>\n",
       "      <td>0.240431</td>\n",
       "      <td>0.242806</td>\n",
       "      <td>0.245556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.092774</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.087243</td>\n",
       "      <td>0.091331</td>\n",
       "      <td>0.093029</td>\n",
       "      <td>0.094016</td>\n",
       "      <td>0.098436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.238947</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.236813</td>\n",
       "      <td>0.238861</td>\n",
       "      <td>0.240194</td>\n",
       "      <td>0.245972</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.108256</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.107222</td>\n",
       "      <td>0.108431</td>\n",
       "      <td>0.108919</td>\n",
       "      <td>0.112142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.263319</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.257361</td>\n",
       "      <td>0.261604</td>\n",
       "      <td>0.263056</td>\n",
       "      <td>0.265132</td>\n",
       "      <td>0.269222</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.163160</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.157605</td>\n",
       "      <td>0.161699</td>\n",
       "      <td>0.163116</td>\n",
       "      <td>0.164495</td>\n",
       "      <td>0.167805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.165811</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.164382</td>\n",
       "      <td>0.165625</td>\n",
       "      <td>0.167153</td>\n",
       "      <td>0.169306</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.011620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.263534</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.257250</td>\n",
       "      <td>0.261354</td>\n",
       "      <td>0.263750</td>\n",
       "      <td>0.265222</td>\n",
       "      <td>0.269250</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.163105</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.158064</td>\n",
       "      <td>0.161268</td>\n",
       "      <td>0.163216</td>\n",
       "      <td>0.164914</td>\n",
       "      <td>0.168329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.166251</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.161611</td>\n",
       "      <td>0.165111</td>\n",
       "      <td>0.166361</td>\n",
       "      <td>0.167458</td>\n",
       "      <td>0.168722</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.011459</td>\n",
       "      <td>0.011527</td>\n",
       "      <td>0.011642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\\n            oob_score=False, random_state=None, verbose=0,\\n            warm_start=False)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemma_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.257443</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.251972</td>\n",
       "      <td>0.255514</td>\n",
       "      <td>0.257264</td>\n",
       "      <td>0.258389</td>\n",
       "      <td>0.265250</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.154982</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.145570</td>\n",
       "      <td>0.153302</td>\n",
       "      <td>0.154772</td>\n",
       "      <td>0.158298</td>\n",
       "      <td>0.160671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.279081</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.274444</td>\n",
       "      <td>0.277569</td>\n",
       "      <td>0.279111</td>\n",
       "      <td>0.280854</td>\n",
       "      <td>0.282556</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.166840</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.162550</td>\n",
       "      <td>0.165637</td>\n",
       "      <td>0.166418</td>\n",
       "      <td>0.168261</td>\n",
       "      <td>0.172533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">porter_tokenizer</th>\n",
       "      <th>count</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.258150</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.255028</td>\n",
       "      <td>0.256229</td>\n",
       "      <td>0.257833</td>\n",
       "      <td>0.259618</td>\n",
       "      <td>0.262361</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.155196</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.148966</td>\n",
       "      <td>0.153880</td>\n",
       "      <td>0.155371</td>\n",
       "      <td>0.156545</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.279334</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.273083</td>\n",
       "      <td>0.277868</td>\n",
       "      <td>0.279764</td>\n",
       "      <td>0.281431</td>\n",
       "      <td>0.285250</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.166404</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.159312</td>\n",
       "      <td>0.164288</td>\n",
       "      <td>0.166880</td>\n",
       "      <td>0.168643</td>\n",
       "      <td>0.172127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              accuracy  \\\n",
       "                                                                                 count   \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "                                                   porter_tokenizer count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "                                                   porter_tokenizer count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "                                                   porter_tokenizer count         30.0   \n",
       "                                                                    tfidf         30.0   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                   mean   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.241269   \n",
       "                                                                    tfidf      0.238564   \n",
       "                                                   porter_tokenizer count      0.240762   \n",
       "                                                                    tfidf      0.238947   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.263319   \n",
       "                                                                    tfidf      0.165811   \n",
       "                                                   porter_tokenizer count      0.263534   \n",
       "                                                                    tfidf      0.166251   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.257443   \n",
       "                                                                    tfidf      0.279081   \n",
       "                                                   porter_tokenizer count      0.258150   \n",
       "                                                                    tfidf      0.279334   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    std   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002266   \n",
       "                                                                    tfidf      0.002922   \n",
       "                                                   porter_tokenizer count      0.002503   \n",
       "                                                                    tfidf      0.002754   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002883   \n",
       "                                                                    tfidf      0.001591   \n",
       "                                                   porter_tokenizer count      0.002993   \n",
       "                                                                    tfidf      0.001728   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.002952   \n",
       "                                                                    tfidf      0.002217   \n",
       "                                                   porter_tokenizer count      0.002019   \n",
       "                                                                    tfidf      0.003055   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    min   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.235833   \n",
       "                                                                    tfidf      0.232667   \n",
       "                                                   porter_tokenizer count      0.235306   \n",
       "                                                                    tfidf      0.233333   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.257361   \n",
       "                                                                    tfidf      0.163000   \n",
       "                                                   porter_tokenizer count      0.257250   \n",
       "                                                                    tfidf      0.161611   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.251972   \n",
       "                                                                    tfidf      0.274444   \n",
       "                                                   porter_tokenizer count      0.255028   \n",
       "                                                                    tfidf      0.273083   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    25%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.240069   \n",
       "                                                                    tfidf      0.236396   \n",
       "                                                   porter_tokenizer count      0.239028   \n",
       "                                                                    tfidf      0.236813   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.261604   \n",
       "                                                                    tfidf      0.164382   \n",
       "                                                   porter_tokenizer count      0.261354   \n",
       "                                                                    tfidf      0.165111   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.255514   \n",
       "                                                                    tfidf      0.277569   \n",
       "                                                   porter_tokenizer count      0.256229   \n",
       "                                                                    tfidf      0.277868   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    50%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.241569   \n",
       "                                                                    tfidf      0.237958   \n",
       "                                                   porter_tokenizer count      0.240431   \n",
       "                                                                    tfidf      0.238861   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.263056   \n",
       "                                                                    tfidf      0.165625   \n",
       "                                                   porter_tokenizer count      0.263750   \n",
       "                                                                    tfidf      0.166361   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.257264   \n",
       "                                                                    tfidf      0.279111   \n",
       "                                                   porter_tokenizer count      0.257833   \n",
       "                                                                    tfidf      0.279764   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    75%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.242604   \n",
       "                                                                    tfidf      0.240625   \n",
       "                                                   porter_tokenizer count      0.242806   \n",
       "                                                                    tfidf      0.240194   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.265132   \n",
       "                                                                    tfidf      0.167153   \n",
       "                                                   porter_tokenizer count      0.265222   \n",
       "                                                                    tfidf      0.167458   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.258389   \n",
       "                                                                    tfidf      0.280854   \n",
       "                                                   porter_tokenizer count      0.259618   \n",
       "                                                                    tfidf      0.281431   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    max   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.244778   \n",
       "                                                                    tfidf      0.245556   \n",
       "                                                   porter_tokenizer count      0.245556   \n",
       "                                                                    tfidf      0.245972   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.269222   \n",
       "                                                                    tfidf      0.169306   \n",
       "                                                   porter_tokenizer count      0.269250   \n",
       "                                                                    tfidf      0.168722   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.265250   \n",
       "                                                                    tfidf      0.282556   \n",
       "                                                   porter_tokenizer count      0.262361   \n",
       "                                                                    tfidf      0.285250   \n",
       "\n",
       "                                                                                 f1  \\\n",
       "                                                                              count   \n",
       "classifier                                         tokenizer        vect_type         \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "                                                   porter_tokenizer count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "                                                   porter_tokenizer count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "                                                   porter_tokenizer count      30.0   \n",
       "                                                                    tfidf      30.0   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                   mean   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.093399   \n",
       "                                                                    tfidf      0.108314   \n",
       "                                                   porter_tokenizer count      0.092774   \n",
       "                                                                    tfidf      0.108256   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.163160   \n",
       "                                                                    tfidf      0.011404   \n",
       "                                                   porter_tokenizer count      0.163105   \n",
       "                                                                    tfidf      0.011448   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.154982   \n",
       "                                                                    tfidf      0.166840   \n",
       "                                                   porter_tokenizer count      0.155196   \n",
       "                                                                    tfidf      0.166404   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    std   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.002126   \n",
       "                                                                    tfidf      0.001699   \n",
       "                                                   porter_tokenizer count      0.002300   \n",
       "                                                                    tfidf      0.001617   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.002632   \n",
       "                                                                    tfidf      0.000103   \n",
       "                                                   porter_tokenizer count      0.002602   \n",
       "                                                                    tfidf      0.000120   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.003609   \n",
       "                                                                    tfidf      0.002430   \n",
       "                                                   porter_tokenizer count      0.002178   \n",
       "                                                                    tfidf      0.003135   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    min   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.089127   \n",
       "                                                                    tfidf      0.104967   \n",
       "                                                   porter_tokenizer count      0.087243   \n",
       "                                                                    tfidf      0.105023   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.157605   \n",
       "                                                                    tfidf      0.011213   \n",
       "                                                   porter_tokenizer count      0.158064   \n",
       "                                                                    tfidf      0.011165   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.145570   \n",
       "                                                                    tfidf      0.162550   \n",
       "                                                   porter_tokenizer count      0.148966   \n",
       "                                                                    tfidf      0.159312   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    25%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.092017   \n",
       "                                                                    tfidf      0.107377   \n",
       "                                                   porter_tokenizer count      0.091331   \n",
       "                                                                    tfidf      0.107222   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.161699   \n",
       "                                                                    tfidf      0.011322   \n",
       "                                                   porter_tokenizer count      0.161268   \n",
       "                                                                    tfidf      0.011385   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.153302   \n",
       "                                                                    tfidf      0.165637   \n",
       "                                                   porter_tokenizer count      0.153880   \n",
       "                                                                    tfidf      0.164288   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    50%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.093258   \n",
       "                                                                    tfidf      0.108080   \n",
       "                                                   porter_tokenizer count      0.093029   \n",
       "                                                                    tfidf      0.108431   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.163116   \n",
       "                                                                    tfidf      0.011400   \n",
       "                                                   porter_tokenizer count      0.163216   \n",
       "                                                                    tfidf      0.011459   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.154772   \n",
       "                                                                    tfidf      0.166418   \n",
       "                                                   porter_tokenizer count      0.155371   \n",
       "                                                                    tfidf      0.166880   \n",
       "\n",
       "                                                                                         \\\n",
       "                                                                                    75%   \n",
       "classifier                                         tokenizer        vect_type             \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.095179   \n",
       "                                                                    tfidf      0.109486   \n",
       "                                                   porter_tokenizer count      0.094016   \n",
       "                                                                    tfidf      0.108919   \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.164495   \n",
       "                                                                    tfidf      0.011481   \n",
       "                                                   porter_tokenizer count      0.164914   \n",
       "                                                                    tfidf      0.011527   \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.158298   \n",
       "                                                                    tfidf      0.168261   \n",
       "                                                   porter_tokenizer count      0.156545   \n",
       "                                                                    tfidf      0.168643   \n",
       "\n",
       "                                                                                         \n",
       "                                                                                    max  \n",
       "classifier                                         tokenizer        vect_type            \n",
       "ComplementNB(alpha=1.0, class_prior=None, fit_p... lemma_tokenizer  count      0.097384  \n",
       "                                                                    tfidf      0.112052  \n",
       "                                                   porter_tokenizer count      0.098436  \n",
       "                                                                    tfidf      0.112142  \n",
       "MultinomialNB(alpha=1.0, class_prior=None, fit_... lemma_tokenizer  count      0.167805  \n",
       "                                                                    tfidf      0.011620  \n",
       "                                                   porter_tokenizer count      0.168329  \n",
       "                                                                    tfidf      0.011642  \n",
       "RandomForestClassifier(bootstrap=True, class_we... lemma_tokenizer  count      0.160671  \n",
       "                                                                    tfidf      0.172533  \n",
       "                                                   porter_tokenizer count      0.159500  \n",
       "                                                                    tfidf      0.172127  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance[df_performance.average_mtd=='macro'].groupby(['classifier', 'tokenizer', 'vect_type']).describe()[['accuracy','f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Draft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - MultinomialNB - LemmaTokenizer - TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_MNB_lemma_tfidf(trial):\n",
    "    global df_train, df_performance, txt_field, classifier, tokenizer, vect_type, average_mtd\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.MultinomialNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='tfidf' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>97</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.673533</td>\n",
       "      <td>0.479451</td>\n",
       "      <td>0.639056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>51</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.595300</td>\n",
       "      <td>0.353232</td>\n",
       "      <td>0.595305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>85</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.703933</td>\n",
       "      <td>0.533654</td>\n",
       "      <td>0.635387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>81</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.662233</td>\n",
       "      <td>0.426434</td>\n",
       "      <td>0.689126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>34</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.598700</td>\n",
       "      <td>0.355358</td>\n",
       "      <td>0.670974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>49</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.700633</td>\n",
       "      <td>0.540676</td>\n",
       "      <td>0.633220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>94</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.656733</td>\n",
       "      <td>0.475081</td>\n",
       "      <td>0.608428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>10</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.699167</td>\n",
       "      <td>0.529788</td>\n",
       "      <td>0.612299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>47</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.561700</td>\n",
       "      <td>0.300220</td>\n",
       "      <td>0.537231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>39</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>0.363073</td>\n",
       "      <td>0.620524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "681    97  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "353    51  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "595    85    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "561    81    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "240    34  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "347    49    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "662    94  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "79     10    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "335    47    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "269    39  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "681  LemmaTokenizer     tfidf       macro  0.673533   0.479451  0.639056  \n",
       "353  LemmaTokenizer     count       macro  0.595300   0.353232  0.595305  \n",
       "595  LemmaTokenizer     count       macro  0.703933   0.533654  0.635387  \n",
       "561  LemmaTokenizer     count       macro  0.662233   0.426434  0.689126  \n",
       "240  LemmaTokenizer     count       macro  0.598700   0.355358  0.670974  \n",
       "347  LemmaTokenizer     tfidf       macro  0.700633   0.540676  0.633220  \n",
       "662  LemmaTokenizer     count       macro  0.656733   0.475081  0.608428  \n",
       "79   LemmaTokenizer     tfidf       macro  0.699167   0.529788  0.612299  \n",
       "335  LemmaTokenizer     tfidf       macro  0.561700   0.300220  0.537231  \n",
       "269  LemmaTokenizer     count       macro  0.609900   0.363073  0.620524  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_mission_MNB_lemma_tfidf.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - MultinomialNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_MNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.MultinomialNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=func_mission_MNB_lemma_count.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - MultinomialNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_prgrm_MNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='prgrm_dsc' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.MultinomialNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.598833</td>\n",
       "      <td>0.349254</td>\n",
       "      <td>0.579754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.422397</td>\n",
       "      <td>0.644755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>26</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.656933</td>\n",
       "      <td>0.420701</td>\n",
       "      <td>0.683195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>88</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.426009</td>\n",
       "      <td>0.652042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>28</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.667333</td>\n",
       "      <td>0.431431</td>\n",
       "      <td>0.671606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>83</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.600867</td>\n",
       "      <td>0.357212</td>\n",
       "      <td>0.680094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.355520</td>\n",
       "      <td>0.627683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>96</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.603167</td>\n",
       "      <td>0.353313</td>\n",
       "      <td>0.623195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.660667</td>\n",
       "      <td>0.426780</td>\n",
       "      <td>0.647043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>94</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.662300</td>\n",
       "      <td>0.423771</td>\n",
       "      <td>0.637518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "4       1  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "13      7    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "52     26    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "176    88    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "56     28    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "167    83  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "11      5  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "194    96  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "28     14    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "188    94    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "4    LemmaTokenizer     count       macro  0.598833   0.349254  0.579754  \n",
       "13   LemmaTokenizer     count       macro  0.658333   0.422397  0.644755  \n",
       "52   LemmaTokenizer     count       macro  0.656933   0.420701  0.683195  \n",
       "176  LemmaTokenizer     count       macro  0.662600   0.426009  0.652042  \n",
       "56   LemmaTokenizer     count       macro  0.667333   0.431431  0.671606  \n",
       "167  LemmaTokenizer     count       macro  0.600867   0.357212  0.680094  \n",
       "11   LemmaTokenizer     count       macro  0.604600   0.355520  0.627683  \n",
       "194  LemmaTokenizer     count       macro  0.603167   0.353313  0.623195  \n",
       "28   LemmaTokenizer     count       macro  0.660667   0.426780  0.647043  \n",
       "188  LemmaTokenizer     count       macro  0.662300   0.423771  0.637518  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_prgrm_MNB_lemma_count.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_CNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.ComplementNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>29</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.606267</td>\n",
       "      <td>0.356488</td>\n",
       "      <td>0.618608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>53</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.663600</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>0.640428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>36</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.353021</td>\n",
       "      <td>0.649588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>30</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.699667</td>\n",
       "      <td>0.534671</td>\n",
       "      <td>0.630701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>39</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>0.363073</td>\n",
       "      <td>0.620524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>51</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.595300</td>\n",
       "      <td>0.353232</td>\n",
       "      <td>0.595305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>53</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>0.534318</td>\n",
       "      <td>0.625981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.353904</td>\n",
       "      <td>0.650824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.355520</td>\n",
       "      <td>0.627683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>60</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.428999</td>\n",
       "      <td>0.678820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "115    29  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "209    53    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "146    36  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "126    30    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "155    39  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "203    51  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "215    53    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "15      3  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "17      5  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "240    60    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "115  LemmaTokenizer     count       macro  0.606267   0.356488  0.618608  \n",
       "209  LemmaTokenizer     count       macro  0.663600   0.426699  0.640428  \n",
       "146  LemmaTokenizer     count       macro  0.606667   0.353021  0.649588  \n",
       "126  LemmaTokenizer     count       macro  0.699667   0.534671  0.630701  \n",
       "155  LemmaTokenizer     count       macro  0.609900   0.363073  0.620524  \n",
       "203  LemmaTokenizer     count       macro  0.595300   0.353232  0.595305  \n",
       "215  LemmaTokenizer     count       macro  0.700400   0.534318  0.625981  \n",
       "15   LemmaTokenizer     count       macro  0.601300   0.353904  0.650824  \n",
       "17   LemmaTokenizer     count       macro  0.604600   0.355520  0.627683  \n",
       "240  LemmaTokenizer     count       macro  0.663200   0.428999  0.678820  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_mission_CNB_lemma_count.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_prgrm_CNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='prgrm_dsc' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.ComplementNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.601267</td>\n",
       "      <td>0.351739</td>\n",
       "      <td>0.625842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>67</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.598733</td>\n",
       "      <td>0.353515</td>\n",
       "      <td>0.625180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>97</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.657133</td>\n",
       "      <td>0.422356</td>\n",
       "      <td>0.644037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>94</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.598733</td>\n",
       "      <td>0.352534</td>\n",
       "      <td>0.630569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>37</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.661867</td>\n",
       "      <td>0.428198</td>\n",
       "      <td>0.653606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>82</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.600867</td>\n",
       "      <td>0.351543</td>\n",
       "      <td>0.663272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>80</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.659667</td>\n",
       "      <td>0.426818</td>\n",
       "      <td>0.636164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>62</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.657400</td>\n",
       "      <td>0.478270</td>\n",
       "      <td>0.614129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>34</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.655733</td>\n",
       "      <td>0.479513</td>\n",
       "      <td>0.613932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>68</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.659033</td>\n",
       "      <td>0.424482</td>\n",
       "      <td>0.680102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "3       0  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "201    67  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "289    97    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "284    94  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "109    37    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "248    82  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "240    80    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "190    62  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "106    34  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "204    68    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "3    LemmaTokenizer     count       macro  0.601267   0.351739  0.625842  \n",
       "201  LemmaTokenizer     count       macro  0.598733   0.353515  0.625180  \n",
       "289  LemmaTokenizer     count       macro  0.657133   0.422356  0.644037  \n",
       "284  LemmaTokenizer     count       macro  0.598733   0.352534  0.630569  \n",
       "109  LemmaTokenizer     count       macro  0.661867   0.428198  0.653606  \n",
       "248  LemmaTokenizer     count       macro  0.600867   0.351543  0.663272  \n",
       "240  LemmaTokenizer     count       macro  0.659667   0.426818  0.636164  \n",
       "190  LemmaTokenizer     count       macro  0.657400   0.478270  0.614129  \n",
       "106  LemmaTokenizer     count       macro  0.655733   0.479513  0.613932  \n",
       "204  LemmaTokenizer     count       macro  0.659033   0.424482  0.680102  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_prgrm_CNB_lemma_count.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - ComplementNB - LemmaTokenizer - TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_prgrm_CNB_lemma_tfidf(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='prgrm_dsc' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.ComplementNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='tfidf' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>0.528067</td>\n",
       "      <td>0.616052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>51</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.661267</td>\n",
       "      <td>0.426321</td>\n",
       "      <td>0.683731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>46</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>0.531807</td>\n",
       "      <td>0.626519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>56</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.653367</td>\n",
       "      <td>0.477555</td>\n",
       "      <td>0.613562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>46</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.655467</td>\n",
       "      <td>0.474524</td>\n",
       "      <td>0.621911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>81</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.484696</td>\n",
       "      <td>0.616098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>73</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.656233</td>\n",
       "      <td>0.477075</td>\n",
       "      <td>0.610757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>93</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.657600</td>\n",
       "      <td>0.423429</td>\n",
       "      <td>0.641899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>11</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.661233</td>\n",
       "      <td>0.424397</td>\n",
       "      <td>0.657256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.353904</td>\n",
       "      <td>0.650824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "96     18    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "251    51    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "236    46    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "284    56  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "234    46  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "405    81  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "365    73  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "461    93    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "47     11    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "18      3  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "96   LemmaTokenizer     count       macro  0.700400   0.528067  0.616052  \n",
       "251  LemmaTokenizer     count       macro  0.661267   0.426321  0.683731  \n",
       "236  LemmaTokenizer     count       macro  0.701200   0.531807  0.626519  \n",
       "284  LemmaTokenizer     count       macro  0.653367   0.477555  0.613562  \n",
       "234  LemmaTokenizer     count       macro  0.655467   0.474524  0.621911  \n",
       "405  LemmaTokenizer     count       macro  0.660900   0.484696  0.616098  \n",
       "365  LemmaTokenizer     count       macro  0.656233   0.477075  0.610757  \n",
       "461  LemmaTokenizer     count       macro  0.657600   0.423429  0.641899  \n",
       "47   LemmaTokenizer     count       macro  0.661233   0.424397  0.657256  \n",
       "18   LemmaTokenizer     count       macro  0.601300   0.353904  0.650824  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_prgrm_CNB_lemma_tfidf.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - ComplementNB - LemmaTokenizer - TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_CNB_lemma_tfidf(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.ComplementNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='tfidf' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>40</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.673467</td>\n",
       "      <td>0.481593</td>\n",
       "      <td>0.663747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>69</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.603033</td>\n",
       "      <td>0.353719</td>\n",
       "      <td>0.629368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6</td>\n",
       "      <td>mission</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.693267</td>\n",
       "      <td>0.533007</td>\n",
       "      <td>0.631130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>37</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.608833</td>\n",
       "      <td>0.358827</td>\n",
       "      <td>0.599976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>63</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.665100</td>\n",
       "      <td>0.430047</td>\n",
       "      <td>0.652673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>88</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.604800</td>\n",
       "      <td>0.353762</td>\n",
       "      <td>0.596581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>16</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.659567</td>\n",
       "      <td>0.425299</td>\n",
       "      <td>0.648161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>63</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.655667</td>\n",
       "      <td>0.474790</td>\n",
       "      <td>0.606161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>33</td>\n",
       "      <td>mission</td>\n",
       "      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.657933</td>\n",
       "      <td>0.423631</td>\n",
       "      <td>0.641981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>14</td>\n",
       "      <td>prgrm_dsc</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>LemmaTokenizer</td>\n",
       "      <td>count</td>\n",
       "      <td>macro</td>\n",
       "      <td>0.662267</td>\n",
       "      <td>0.481588</td>\n",
       "      <td>0.611395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  txt_field                                         classifier  \\\n",
       "248    40  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "411    69  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "51      6    mission  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "219    37  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "373    63    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "530    88  prgrm_dsc  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "96     16    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "377    63  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "193    33    mission  MultinomialNB(alpha=1.0, class_prior=None, fit...   \n",
       "88     14  prgrm_dsc  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "\n",
       "          tokenizer vect_type average_mtd  accuracy  precision    recall  \n",
       "248  LemmaTokenizer     tfidf       macro  0.673467   0.481593  0.663747  \n",
       "411  LemmaTokenizer     count       macro  0.603033   0.353719  0.629368  \n",
       "51   LemmaTokenizer     tfidf       macro  0.693267   0.533007  0.631130  \n",
       "219  LemmaTokenizer     count       macro  0.608833   0.358827  0.599976  \n",
       "373  LemmaTokenizer     count       macro  0.665100   0.430047  0.652673  \n",
       "530  LemmaTokenizer     count       macro  0.604800   0.353762  0.596581  \n",
       "96   LemmaTokenizer     count       macro  0.659567   0.425299  0.648161  \n",
       "377  LemmaTokenizer     count       macro  0.655667   0.474790  0.606161  \n",
       "193  LemmaTokenizer     count       macro  0.657933   0.423631  0.641981  \n",
       "88   LemmaTokenizer     count       macro  0.662267   0.481588  0.611395  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=func_mission_CNB_lemma_tfidf.map(range(100))\n",
    "df_performance=pd.concat(dview.gather('df_performance'), ignore_index=True)\n",
    "df_performance.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"5\" halign=\"left\">precision</th>\n",
       "      <th colspan=\"8\" halign=\"left\">recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_field</th>\n",
       "      <th>classifier</th>\n",
       "      <th>vect_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">mission</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.699907</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.693900</td>\n",
       "      <td>0.698317</td>\n",
       "      <td>0.700100</td>\n",
       "      <td>0.701692</td>\n",
       "      <td>0.706467</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.531594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533821</td>\n",
       "      <td>0.541320</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.627921</td>\n",
       "      <td>0.008283</td>\n",
       "      <td>0.608945</td>\n",
       "      <td>0.622043</td>\n",
       "      <td>0.627389</td>\n",
       "      <td>0.632986</td>\n",
       "      <td>0.647726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.697841</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.690767</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.698233</td>\n",
       "      <td>0.699642</td>\n",
       "      <td>0.703133</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.533794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536262</td>\n",
       "      <td>0.541629</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.622604</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.600930</td>\n",
       "      <td>0.617881</td>\n",
       "      <td>0.622611</td>\n",
       "      <td>0.626997</td>\n",
       "      <td>0.646784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.661342</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.659417</td>\n",
       "      <td>0.661550</td>\n",
       "      <td>0.663217</td>\n",
       "      <td>0.668867</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.425750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428148</td>\n",
       "      <td>0.433099</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.659809</td>\n",
       "      <td>0.021351</td>\n",
       "      <td>0.622731</td>\n",
       "      <td>0.643071</td>\n",
       "      <td>0.650996</td>\n",
       "      <td>0.682431</td>\n",
       "      <td>0.703958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">prgrm_dsc</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)</th>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.657213</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.650667</td>\n",
       "      <td>0.655233</td>\n",
       "      <td>0.657233</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.663833</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.476934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479399</td>\n",
       "      <td>0.485138</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.614657</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.591100</td>\n",
       "      <td>0.609157</td>\n",
       "      <td>0.614943</td>\n",
       "      <td>0.619995</td>\n",
       "      <td>0.641197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.673393</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.667533</td>\n",
       "      <td>0.671317</td>\n",
       "      <td>0.673467</td>\n",
       "      <td>0.675242</td>\n",
       "      <td>0.680067</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.479188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481655</td>\n",
       "      <td>0.485097</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.644290</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>0.617138</td>\n",
       "      <td>0.635860</td>\n",
       "      <td>0.644252</td>\n",
       "      <td>0.650899</td>\n",
       "      <td>0.684130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</th>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.602443</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.595300</td>\n",
       "      <td>0.600658</td>\n",
       "      <td>0.602350</td>\n",
       "      <td>0.604150</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.355167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356803</td>\n",
       "      <td>0.364457</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.627163</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>0.577607</td>\n",
       "      <td>0.609106</td>\n",
       "      <td>0.625511</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.706715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       accuracy  \\\n",
       "                                                                          count   \n",
       "txt_field classifier                                         vect_type            \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count        100.0   \n",
       "                                                             tfidf        100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count        100.0   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count        100.0   \n",
       "                                                             tfidf        100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count        100.0   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                            mean   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.699907   \n",
       "                                                             tfidf      0.697841   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.661342   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.657213   \n",
       "                                                             tfidf      0.673393   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.602443   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             std   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.002527   \n",
       "                                                             tfidf      0.002500   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.002978   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.002682   \n",
       "                                                             tfidf      0.002592   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.002772   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             min   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.693900   \n",
       "                                                             tfidf      0.690767   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.651700   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.650667   \n",
       "                                                             tfidf      0.667533   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.595300   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             25%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.698317   \n",
       "                                                             tfidf      0.695900   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.659417   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.655233   \n",
       "                                                             tfidf      0.671317   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.600658   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             50%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.700100   \n",
       "                                                             tfidf      0.698233   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.661550   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.657233   \n",
       "                                                             tfidf      0.673467   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.602350   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             75%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.701692   \n",
       "                                                             tfidf      0.699642   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.663217   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.659000   \n",
       "                                                             tfidf      0.675242   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.604150   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             max   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.706467   \n",
       "                                                             tfidf      0.703133   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.668867   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.663833   \n",
       "                                                             tfidf      0.680067   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.609900   \n",
       "\n",
       "                                                                       precision  \\\n",
       "                                                                           count   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count         100.0   \n",
       "                                                             tfidf         100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count         100.0   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count         100.0   \n",
       "                                                             tfidf         100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count         100.0   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                            mean   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.531594   \n",
       "                                                             tfidf      0.533794   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.425750   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.476934   \n",
       "                                                             tfidf      0.479188   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.355167   \n",
       "\n",
       "                                                                          ...     \\\n",
       "                                                                          ...      \n",
       "txt_field classifier                                         vect_type    ...      \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count        ...      \n",
       "                                                             tfidf        ...      \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count        ...      \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count        ...      \n",
       "                                                             tfidf        ...      \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count        ...      \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             75%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.533821   \n",
       "                                                             tfidf      0.536262   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.428148   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.479399   \n",
       "                                                             tfidf      0.481655   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.356803   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             max   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.541320   \n",
       "                                                             tfidf      0.541629   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.433099   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.485138   \n",
       "                                                             tfidf      0.485097   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.364457   \n",
       "\n",
       "                                                                       recall  \\\n",
       "                                                                        count   \n",
       "txt_field classifier                                         vect_type          \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      100.0   \n",
       "                                                             tfidf      100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      100.0   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      100.0   \n",
       "                                                             tfidf      100.0   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      100.0   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                            mean   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.627921   \n",
       "                                                             tfidf      0.622604   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.659809   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.614657   \n",
       "                                                             tfidf      0.644290   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.627163   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             std   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.008283   \n",
       "                                                             tfidf      0.007315   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.021351   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.008150   \n",
       "                                                             tfidf      0.012344   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.028436   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             min   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.608945   \n",
       "                                                             tfidf      0.600930   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.622731   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.591100   \n",
       "                                                             tfidf      0.617138   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.577607   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             25%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.622043   \n",
       "                                                             tfidf      0.617881   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.643071   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.609157   \n",
       "                                                             tfidf      0.635860   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.609106   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             50%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.627389   \n",
       "                                                             tfidf      0.622611   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.650996   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.614943   \n",
       "                                                             tfidf      0.644252   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.625511   \n",
       "\n",
       "                                                                                  \\\n",
       "                                                                             75%   \n",
       "txt_field classifier                                         vect_type             \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.632986   \n",
       "                                                             tfidf      0.626997   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.682431   \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.619995   \n",
       "                                                             tfidf      0.650899   \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.647579   \n",
       "\n",
       "                                                                                  \n",
       "                                                                             max  \n",
       "txt_field classifier                                         vect_type            \n",
       "mission   ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.647726  \n",
       "                                                             tfidf      0.646784  \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.703958  \n",
       "prgrm_dsc ComplementNB(alpha=1.0, class_prior=None, fit_p... count      0.641197  \n",
       "                                                             tfidf      0.684130  \n",
       "          MultinomialNB(alpha=1.0, class_prior=None, fit_... count      0.706715  \n",
       "\n",
       "[6 rows x 24 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance.groupby(['txt_field', 'classifier', 'vect_type']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.661060</td>\n",
       "      <td>0.425503</td>\n",
       "      <td>0.658984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.020294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.655533</td>\n",
       "      <td>0.419339</td>\n",
       "      <td>0.627483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.659017</td>\n",
       "      <td>0.423844</td>\n",
       "      <td>0.644080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.661350</td>\n",
       "      <td>0.425360</td>\n",
       "      <td>0.650408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.662775</td>\n",
       "      <td>0.427471</td>\n",
       "      <td>0.677598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.668267</td>\n",
       "      <td>0.432884</td>\n",
       "      <td>0.705346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy   precision      recall\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     0.661060    0.425503    0.658984\n",
       "std      0.002628    0.002539    0.020294\n",
       "min      0.655533    0.419339    0.627483\n",
       "25%      0.659017    0.423844    0.644080\n",
       "50%      0.661350    0.425360    0.650408\n",
       "75%      0.662775    0.427471    0.677598\n",
       "max      0.668267    0.432884    0.705346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mission_MNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [3:10:23<33:50, 135.39s/it]"
     ]
    }
   ],
   "source": [
    "df_mission_CNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['mission'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.ComplementNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_mission_CNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mission_CNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - MultinomialNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [11:35:52<00:00, 419.77s/it]\n"
     ]
    }
   ],
   "source": [
    "df_prgrm_MNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['prgrm_dsc'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.MultinomialNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_prgrm_MNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_MNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_CNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['prgrm_dsc'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: Oâ€™Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.ComplementNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_prgrm_CNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_CNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, TF IDF Vectors:  [0.5583666666666667, 0.29982304712090974, 0.5961215483457072, datetime.timedelta(microseconds=498992)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(classifier=naive_bayes.MultinomialNB(), \n",
    "                       x_train=x_train_vect_tfidf,\n",
    "                       y_train= y_train, \n",
    "                       x_valid=x_valid_vect_tfidf,\n",
    "                       y_valid=y_valid\n",
    "                      )\n",
    "results.loc[len(results)] = [\"NB, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NB, TF IDF Vectors: \", accuracy)\n",
    "\n",
    "\n",
    "# Raw: MNB, Count Vectors:  [0.6732666666666667, 0.4503852200520778, 0.6661678975327818, datetime.timedelta(microseconds=489443)]\n",
    "# Raw: CNB, Count Vectors:  [0.7013333333333334, 0.5275971009306734, 0.6170202787685387, datetime.timedelta(microseconds=521657)]\n",
    "# Stemmed: MNB, Count Vectors:  [0.6601, 0.4243013182900721, 0.639930088219624, datetime.timedelta(microseconds=443571)]\n",
    "# Stemmed: CNB, Count Vectors:  [0.7, 0.5320034132229355, 0.6383184621220114, datetime.timedelta(microseconds=504788)]\n",
    "# Lemma: MNB, Count Vectors:  [0.6615, 0.42366552445612, 0.6783346552845068, datetime.timedelta(microseconds=572416)]\n",
    "# Lemma: CNB, Count Vectors:  [0.7021333333333334, 0.5341465196920103, 0.6318440519847688, datetime.timedelta(microseconds=586597)]\n",
    "\n",
    "# Raw: MNB, TF IDF Vectors:  [0.5859, 0.32327746269021723, 0.5485416630089535, datetime.timedelta(microseconds=536424)]\n",
    "# Raw: CNB, TF IDF Vectors:  [0.6992333333333334, 0.5306246321804787, 0.6136847132057796, datetime.timedelta(microseconds=580513)]\n",
    "# Stemmed: MNB, TF IDF Vectors:  [0.553, 0.2949552239378839, 0.536824161513825, datetime.timedelta(microseconds=453431)]\n",
    "# Stemmed: CNB, TF IDF Vectors:  [0.6967666666666666, 0.5349583083797411, 0.6223488506468897, datetime.timedelta(microseconds=520307)]\n",
    "# Lemma: MNB, TF IDF Vectors:  [0.5589666666666666, 0.297005157029138, 0.5293106710625075, datetime.timedelta(microseconds=529497)]\n",
    "# Lemma: CNB, TF IDF Vectors:  [0.7012333333333334, 0.536867845262306, 0.6273414839488708, datetime.timedelta(microseconds=546401)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like `Lemma-CNB-Count` produces best results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
