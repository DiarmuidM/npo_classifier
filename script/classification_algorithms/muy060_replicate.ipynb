{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# obtain reproducible results\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from keras import backend as K\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "# Rest of code follows ...\n",
    "\n",
    "# Check GPU device.\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HITID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Answer1</th>\n",
       "      <th>Answer2</th>\n",
       "      <th>Agreement</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Date</th>\n",
       "      <th>JasonCode</th>\n",
       "      <th>moral_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>3AXFSPQOYQYNNLFNVXHLO2FHNRHFJ3</td>\n",
       "      <td>RT NWSNewOrleans Our 6ft 6in MIC next to 11ft ...</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>Performative Reputation</td>\n",
       "      <td>No</td>\n",
       "      <td>no_agreement</td>\n",
       "      <td>2018-07-06 18:03:42 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>3N5YJ55YXG3OAKP0ZFNL38L7ZV9ANP</td>\n",
       "      <td>RT fema Storm surge can be one of the most dan...</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>Technical Reputation</td>\n",
       "      <td>No</td>\n",
       "      <td>no_agreement</td>\n",
       "      <td>2018-07-06 17:59:34 UTC</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35A1YQPVFEGZQD2S73JCQP94LIEI5B</td>\n",
       "      <td>RT statedeptspox Thank you  Dr Alison Mann fro...</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>2018-07-06 17:59:48 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3TX9T2ZCB91FYM6M38U7GKP71SJWZ2</td>\n",
       "      <td>CommerceGov announces preliminary countervaili...</td>\n",
       "      <td>None of the above.</td>\n",
       "      <td>Performative Reputation</td>\n",
       "      <td>No</td>\n",
       "      <td>no_agreement</td>\n",
       "      <td>2018-07-06 18:04:45 UTC</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>3GVPRXWRPHUEDHGBVWY9O9N0CO97IF</td>\n",
       "      <td>Whos a good boy?A USAirForce airman rewards Ni...</td>\n",
       "      <td>Moral Reputation</td>\n",
       "      <td>Technical Reputation</td>\n",
       "      <td>No</td>\n",
       "      <td>no_agreement</td>\n",
       "      <td>2018-07-06 18:08:55 UTC</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              HITID  \\\n",
       "65   3AXFSPQOYQYNNLFNVXHLO2FHNRHFJ3   \n",
       "114  3N5YJ55YXG3OAKP0ZFNL38L7ZV9ANP   \n",
       "16   35A1YQPVFEGZQD2S73JCQP94LIEI5B   \n",
       "141  3TX9T2ZCB91FYM6M38U7GKP71SJWZ2   \n",
       "156  3GVPRXWRPHUEDHGBVWY9O9N0CO97IF   \n",
       "\n",
       "                                                  Text             Answer1  \\\n",
       "65   RT NWSNewOrleans Our 6ft 6in MIC next to 11ft ...  None of the above.   \n",
       "114  RT fema Storm surge can be one of the most dan...  None of the above.   \n",
       "16   RT statedeptspox Thank you  Dr Alison Mann fro...  None of the above.   \n",
       "141  CommerceGov announces preliminary countervaili...  None of the above.   \n",
       "156  Whos a good boy?A USAirForce airman rewards Ni...    Moral Reputation   \n",
       "\n",
       "                     Answer2 Agreement              Answer  \\\n",
       "65   Performative Reputation        No        no_agreement   \n",
       "114     Technical Reputation        No        no_agreement   \n",
       "16        None of the above.       Yes  None of the above.   \n",
       "141  Performative Reputation        No        no_agreement   \n",
       "156     Technical Reputation        No        no_agreement   \n",
       "\n",
       "                        Date  JasonCode  moral_bin  \n",
       "65   2018-07-06 18:03:42 UTC          0          0  \n",
       "114  2018-07-06 17:59:34 UTC          2          1  \n",
       "16   2018-07-06 17:59:48 UTC          0          0  \n",
       "141  2018-07-06 18:04:45 UTC          1          0  \n",
       "156  2018-07-06 18:08:55 UTC          2          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coded_tweet=pd.read_csv('../../dataset/muy060_suppl_supplementary_appendix/muy060_Appendix_2.csv')\n",
    "df_coded_tweet.loc[df_coded_tweet.JasonCode==2, 'moral_bin']=1\n",
    "df_coded_tweet.moral_bin.fillna('0', inplace=True)\n",
    "df_coded_tweet['moral_bin']=[int(s) for s in df_coded_tweet.moral_bin]\n",
    "df_coded_tweet.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "lb.fit(df_coded_tweet.moral_bin.unique())\n",
    "y_train=lb.transform(df_coded_tweet['moral_bin'])\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# lb.fit(df_coded_tweet.JasonCode.unique())\n",
    "# y_train=lb.transform(df_coded_tweet['JasonCode'])\n",
    "\n",
    "# y_test=lb.transform(df_test['broad_cat']) # No need to transform Y for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=df_coded_tweet['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('to', 2), ('rt', 3), ('of', 4), ('a', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequences.\n",
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "\n",
    "# Pads sequences to the same length (i.e., prepare matrix).\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM)) # Plus one: embedding matrix starts from 0, word index starts from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "num_filters=[32, 64, 128]\n",
    "kernel_size=[3,5,7]\n",
    "conv_act=['sigmoid', 'softplus', 'tanh', 'softmax']\n",
    "act_32=['sigmoid', 'softplus', 'tanh', 'softmax']\n",
    "act_16=['sigmoid', 'softplus', 'tanh', 'softmax']\n",
    "out_act=['sigmoid', 'softplus', 'tanh', 'softmax']\n",
    "param_list=list(itertools.product(num_filters, kernel_size, conv_act, act_32, act_16, out_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2304 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 78/2304 [03:38<1:42:44,  2.77s/it]"
     ]
    }
   ],
   "source": [
    "result_list=[]\n",
    "\n",
    "for num_filters, kernel_size, conv_act, act_32, act_16, out_act in tqdm(param_list):\n",
    "    with tf.device('/gpu:0'): # Small dataset, may even increase time because of data transfer between C/GPU.\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        # model.add(Flatten())\n",
    "        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dense(units=32, activation=act_32))\n",
    "        model.add(Dense(units=16, activation=act_16))\n",
    "        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "        # compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "        # fit the model\n",
    "        history=model.fit(x_train, y_train, validation_split=0.3, epochs=100, verbose=0)\n",
    "    result_list+=[[max(history.history['val_acc']), [num_filters, kernel_size, conv_act, act_32, act_16, out_act]]]\n",
    "    ############### Clean up and rebuid objects; otherwise, tensor graph becomes larger and larger and requires more time. ##########\n",
    "    del model\n",
    "    K.clear_session() # Will cause error: Tensor must from the same graph. Cause: Embedding layers changed. Solution: rebuild embedding layer.\n",
    "    ##### Initialize session for reproducibility #####\n",
    "    np.random.seed(42)\n",
    "    rn.seed(12345)\n",
    "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                                  inter_op_parallelism_threads=1)\n",
    "    tf.set_random_seed(1234)\n",
    "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)\n",
    "    ##################################################\n",
    "    # Rebuid embedding layer.\n",
    "    embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                                input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                                output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "    #################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result_list).sort_values(0, ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 138 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 0.6789 - acc: 0.5870 - val_loss: 0.6791 - val_acc: 0.5833\n",
      "Epoch 2/100\n",
      "138/138 [==============================] - 0s 179us/step - loss: 0.6767 - acc: 0.5870 - val_loss: 0.6789 - val_acc: 0.5833\n",
      "Epoch 3/100\n",
      "138/138 [==============================] - 0s 185us/step - loss: 0.6735 - acc: 0.5870 - val_loss: 0.6785 - val_acc: 0.5833\n",
      "Epoch 4/100\n",
      "138/138 [==============================] - 0s 176us/step - loss: 0.6712 - acc: 0.5870 - val_loss: 0.6781 - val_acc: 0.5833\n",
      "Epoch 5/100\n",
      "138/138 [==============================] - 0s 177us/step - loss: 0.6676 - acc: 0.5870 - val_loss: 0.6778 - val_acc: 0.5833\n",
      "Epoch 6/100\n",
      "138/138 [==============================] - 0s 176us/step - loss: 0.6629 - acc: 0.5870 - val_loss: 0.6773 - val_acc: 0.5833\n",
      "Epoch 7/100\n",
      "138/138 [==============================] - 0s 180us/step - loss: 0.6564 - acc: 0.5870 - val_loss: 0.6757 - val_acc: 0.5833\n",
      "Epoch 8/100\n",
      "138/138 [==============================] - 0s 175us/step - loss: 0.6464 - acc: 0.5870 - val_loss: 0.6724 - val_acc: 0.5833\n",
      "Epoch 9/100\n",
      "138/138 [==============================] - 0s 177us/step - loss: 0.6309 - acc: 0.5870 - val_loss: 0.6684 - val_acc: 0.5833\n",
      "Epoch 10/100\n",
      "138/138 [==============================] - 0s 173us/step - loss: 0.6097 - acc: 0.6159 - val_loss: 0.6625 - val_acc: 0.5833\n",
      "Epoch 11/100\n",
      "138/138 [==============================] - 0s 160us/step - loss: 0.5793 - acc: 0.7174 - val_loss: 0.6544 - val_acc: 0.6000\n",
      "Epoch 12/100\n",
      "138/138 [==============================] - 0s 163us/step - loss: 0.5390 - acc: 0.8333 - val_loss: 0.6449 - val_acc: 0.6000\n",
      "Epoch 13/100\n",
      "138/138 [==============================] - 0s 210us/step - loss: 0.4922 - acc: 0.9058 - val_loss: 0.6346 - val_acc: 0.6000\n",
      "Epoch 14/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.4357 - acc: 0.9565 - val_loss: 0.6226 - val_acc: 0.6167\n",
      "Epoch 15/100\n",
      "138/138 [==============================] - 0s 152us/step - loss: 0.3738 - acc: 0.9855 - val_loss: 0.6099 - val_acc: 0.6500\n",
      "Epoch 16/100\n",
      "138/138 [==============================] - 0s 166us/step - loss: 0.3130 - acc: 0.9928 - val_loss: 0.5989 - val_acc: 0.7000\n",
      "Epoch 17/100\n",
      "138/138 [==============================] - 0s 155us/step - loss: 0.2540 - acc: 0.9928 - val_loss: 0.5947 - val_acc: 0.6667\n",
      "Epoch 18/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.2036 - acc: 0.9928 - val_loss: 0.5840 - val_acc: 0.6500\n",
      "Epoch 19/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.1599 - acc: 1.0000 - val_loss: 0.5738 - val_acc: 0.7000\n",
      "Epoch 20/100\n",
      "138/138 [==============================] - 0s 158us/step - loss: 0.1272 - acc: 1.0000 - val_loss: 0.5727 - val_acc: 0.6667\n",
      "Epoch 21/100\n",
      "138/138 [==============================] - 0s 158us/step - loss: 0.1030 - acc: 1.0000 - val_loss: 0.5759 - val_acc: 0.6667\n",
      "Epoch 22/100\n",
      "138/138 [==============================] - 0s 162us/step - loss: 0.0847 - acc: 1.0000 - val_loss: 0.5664 - val_acc: 0.6833\n",
      "Epoch 23/100\n",
      "138/138 [==============================] - 0s 157us/step - loss: 0.0715 - acc: 1.0000 - val_loss: 0.5626 - val_acc: 0.7000\n",
      "Epoch 24/100\n",
      "138/138 [==============================] - 0s 159us/step - loss: 0.0618 - acc: 1.0000 - val_loss: 0.5657 - val_acc: 0.7000\n",
      "Epoch 25/100\n",
      "138/138 [==============================] - 0s 160us/step - loss: 0.0547 - acc: 1.0000 - val_loss: 0.5706 - val_acc: 0.7000\n",
      "Epoch 26/100\n",
      "138/138 [==============================] - 0s 159us/step - loss: 0.0491 - acc: 1.0000 - val_loss: 0.5780 - val_acc: 0.6833\n",
      "Epoch 27/100\n",
      "138/138 [==============================] - 0s 155us/step - loss: 0.0448 - acc: 1.0000 - val_loss: 0.5821 - val_acc: 0.6833\n",
      "Epoch 28/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.0412 - acc: 1.0000 - val_loss: 0.5838 - val_acc: 0.6833\n",
      "Epoch 29/100\n",
      "138/138 [==============================] - 0s 155us/step - loss: 0.0383 - acc: 1.0000 - val_loss: 0.5809 - val_acc: 0.7000\n",
      "Epoch 30/100\n",
      "138/138 [==============================] - 0s 160us/step - loss: 0.0358 - acc: 1.0000 - val_loss: 0.5803 - val_acc: 0.7000\n",
      "Epoch 31/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.5800 - val_acc: 0.7000\n",
      "Epoch 32/100\n",
      "138/138 [==============================] - 0s 149us/step - loss: 0.0320 - acc: 1.0000 - val_loss: 0.5834 - val_acc: 0.7000\n",
      "Epoch 33/100\n",
      "138/138 [==============================] - 0s 155us/step - loss: 0.0303 - acc: 1.0000 - val_loss: 0.5873 - val_acc: 0.7000\n",
      "Epoch 34/100\n",
      "138/138 [==============================] - 0s 154us/step - loss: 0.0289 - acc: 1.0000 - val_loss: 0.5904 - val_acc: 0.7000\n",
      "Epoch 35/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.0276 - acc: 1.0000 - val_loss: 0.5929 - val_acc: 0.7000\n",
      "Epoch 36/100\n",
      "138/138 [==============================] - 0s 152us/step - loss: 0.0264 - acc: 1.0000 - val_loss: 0.5969 - val_acc: 0.7167\n",
      "Epoch 37/100\n",
      "138/138 [==============================] - 0s 163us/step - loss: 0.0253 - acc: 1.0000 - val_loss: 0.6031 - val_acc: 0.6833\n",
      "Epoch 38/100\n",
      "138/138 [==============================] - 0s 152us/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.6055 - val_acc: 0.7000\n",
      "Epoch 39/100\n",
      "138/138 [==============================] - 0s 154us/step - loss: 0.0235 - acc: 1.0000 - val_loss: 0.6054 - val_acc: 0.7000\n",
      "Epoch 40/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 0.6065 - val_acc: 0.7000\n",
      "Epoch 41/100\n",
      "138/138 [==============================] - 0s 149us/step - loss: 0.0218 - acc: 1.0000 - val_loss: 0.6093 - val_acc: 0.7000\n",
      "Epoch 42/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.6094 - val_acc: 0.7000\n",
      "Epoch 43/100\n",
      "138/138 [==============================] - 0s 147us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.6102 - val_acc: 0.7000\n",
      "Epoch 44/100\n",
      "138/138 [==============================] - 0s 149us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.6157 - val_acc: 0.7000\n",
      "Epoch 45/100\n",
      "138/138 [==============================] - 0s 154us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.6188 - val_acc: 0.7000\n",
      "Epoch 46/100\n",
      "138/138 [==============================] - 0s 155us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.6246 - val_acc: 0.7000\n",
      "Epoch 47/100\n",
      "138/138 [==============================] - 0s 158us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.6273 - val_acc: 0.7000\n",
      "Epoch 48/100\n",
      "138/138 [==============================] - 0s 159us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.6286 - val_acc: 0.7000\n",
      "Epoch 49/100\n",
      "138/138 [==============================] - 0s 154us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.6300 - val_acc: 0.7000\n",
      "Epoch 50/100\n",
      "138/138 [==============================] - 0s 149us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.6331 - val_acc: 0.7000\n",
      "Epoch 51/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.6363 - val_acc: 0.7000\n",
      "Epoch 52/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.6400 - val_acc: 0.7000\n",
      "Epoch 53/100\n",
      "138/138 [==============================] - 0s 151us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.6456 - val_acc: 0.6833\n",
      "Epoch 54/100\n",
      "138/138 [==============================] - 0s 152us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.6488 - val_acc: 0.6833\n",
      "Epoch 55/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.6545 - val_acc: 0.7000\n",
      "Epoch 56/100\n",
      "138/138 [==============================] - 0s 151us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.6591 - val_acc: 0.7000\n",
      "Epoch 57/100\n",
      "138/138 [==============================] - 0s 150us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.6650 - val_acc: 0.7000\n",
      "Epoch 58/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.6688 - val_acc: 0.7000\n",
      "Epoch 59/100\n",
      "138/138 [==============================] - 0s 157us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.6695 - val_acc: 0.7000\n",
      "Epoch 60/100\n",
      "138/138 [==============================] - 0s 158us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.6718 - val_acc: 0.7000\n",
      "Epoch 61/100\n",
      "138/138 [==============================] - 0s 154us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.6725 - val_acc: 0.7000\n",
      "Epoch 62/100\n",
      "138/138 [==============================] - 0s 149us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.6738 - val_acc: 0.7000\n",
      "Epoch 63/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.6756 - val_acc: 0.7000\n",
      "Epoch 64/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.6769 - val_acc: 0.7000\n",
      "Epoch 65/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.6797 - val_acc: 0.7000\n",
      "Epoch 66/100\n",
      "138/138 [==============================] - 0s 152us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.6834 - val_acc: 0.7000\n",
      "Epoch 67/100\n",
      "138/138 [==============================] - 0s 149us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.6875 - val_acc: 0.7000\n",
      "Epoch 68/100\n",
      "138/138 [==============================] - 0s 149us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.6902 - val_acc: 0.7000\n",
      "Epoch 69/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.6928 - val_acc: 0.7000\n",
      "Epoch 70/100\n",
      "138/138 [==============================] - 0s 167us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.6951 - val_acc: 0.7000\n",
      "Epoch 71/100\n",
      "138/138 [==============================] - 0s 157us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.6990 - val_acc: 0.7000\n",
      "Epoch 72/100\n",
      "138/138 [==============================] - 0s 149us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.7038 - val_acc: 0.7000\n",
      "Epoch 73/100\n",
      "138/138 [==============================] - 0s 152us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.7066 - val_acc: 0.7000\n",
      "Epoch 74/100\n",
      "138/138 [==============================] - 0s 151us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.7089 - val_acc: 0.7000\n",
      "Epoch 75/100\n",
      "138/138 [==============================] - 0s 154us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.7123 - val_acc: 0.7000\n",
      "Epoch 76/100\n",
      "138/138 [==============================] - 0s 157us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.7137 - val_acc: 0.7000\n",
      "Epoch 77/100\n",
      "138/138 [==============================] - 0s 151us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.7155 - val_acc: 0.7000\n",
      "Epoch 78/100\n",
      "138/138 [==============================] - 0s 160us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.7172 - val_acc: 0.7000\n",
      "Epoch 79/100\n",
      "138/138 [==============================] - 0s 163us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.7172 - val_acc: 0.7000\n",
      "Epoch 80/100\n",
      "138/138 [==============================] - 0s 150us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.7187 - val_acc: 0.7000\n",
      "Epoch 81/100\n",
      "138/138 [==============================] - 0s 154us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.7204 - val_acc: 0.7000\n",
      "Epoch 82/100\n",
      "138/138 [==============================] - 0s 150us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.7208 - val_acc: 0.7000\n",
      "Epoch 83/100\n",
      "138/138 [==============================] - 0s 155us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.7218 - val_acc: 0.7000\n",
      "Epoch 84/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.7236 - val_acc: 0.7000\n",
      "Epoch 85/100\n",
      "138/138 [==============================] - 0s 155us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.7263 - val_acc: 0.7000\n",
      "Epoch 86/100\n",
      "138/138 [==============================] - 0s 158us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.7283 - val_acc: 0.7000\n",
      "Epoch 87/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.7299 - val_acc: 0.7000\n",
      "Epoch 88/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.7335 - val_acc: 0.7000\n",
      "Epoch 89/100\n",
      "138/138 [==============================] - 0s 150us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.7364 - val_acc: 0.7000\n",
      "Epoch 90/100\n",
      "138/138 [==============================] - 0s 165us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.7396 - val_acc: 0.7000\n",
      "Epoch 91/100\n",
      "138/138 [==============================] - 0s 148us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.7410 - val_acc: 0.7000\n",
      "Epoch 92/100\n",
      "138/138 [==============================] - 0s 150us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.7453 - val_acc: 0.7000\n",
      "Epoch 93/100\n",
      "138/138 [==============================] - 0s 156us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.7492 - val_acc: 0.7000\n",
      "Epoch 94/100\n",
      "138/138 [==============================] - 0s 164us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.7522 - val_acc: 0.7000\n",
      "Epoch 95/100\n",
      "138/138 [==============================] - 0s 151us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.7543 - val_acc: 0.7000\n",
      "Epoch 96/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.7563 - val_acc: 0.7000\n",
      "Epoch 97/100\n",
      "138/138 [==============================] - 0s 162us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.7587 - val_acc: 0.7000\n",
      "Epoch 98/100\n",
      "138/138 [==============================] - 0s 162us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.7620 - val_acc: 0.7000\n",
      "Epoch 99/100\n",
      "138/138 [==============================] - 0s 171us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.7647 - val_acc: 0.7000\n",
      "Epoch 100/100\n",
      "138/138 [==============================] - 0s 153us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.7679 - val_acc: 0.7000\n"
     ]
    }
   ],
   "source": [
    "############### Clean up and rebuid objects; otherwise, tensor graph becomes larger and larger and requires more time. ##########\n",
    "K.clear_session() # Will cause error: Tensor must from the same graph. Cause: Embedding layers changed. Solution: rebuild embedding layer.\n",
    "##### Initialize session for reproducibility #####\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "##################################################\n",
    "# Rebuid embedding layer.\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "#################################################################################################################################\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "# model.add(Flatten())\n",
    "model.add(Conv1D(128, 5, activation='softmax'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(units=32, activation='tanh'))\n",
    "model.add(Dense(units=16, activation='sigmoid'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "# fit the model\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traps.\n",
    "\n",
    "- Tried GridSearchCV, suffers from the \"tensor graph becomes larger and larger\" problem.\n",
    "\n",
    "<s>\n",
    "    \n",
    "```Python\n",
    "def create_model(num_filters, kernel_size, conv_act, act_first, act_second, out_act):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, activation=conv_act))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation=act_first))\n",
    "    model.add(Dense(units=16, activation=act_second))\n",
    "    model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "neural_network = KerasClassifier(build_fn=create_model, validation_split=0.3, verbose=0)\n",
    "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "grid_result = grid.fit(x_train, y_train, verbose=1)\n",
    "```\n",
    "    \n",
    "</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Give up trying multiprocessing: fail to send embedding layer (a pickle object) to threads.\n",
    "    - possible solution: buid embedding layer on remote engines/threads.\n",
    "        - Tried, looks like not working. Since it now only takes about 1.5 hours to complete the job, give up, move on.\n",
    "\n",
    "<s>\n",
    "\n",
    "```Python\n",
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "print(c.ids)\n",
    "dview = c[:]\n",
    "\n",
    "dview['Sequential']=Sequential\n",
    "dview['embedding_layer']=embedding_layer\n",
    "dview['Conv1D']=Conv1D\n",
    "dview['GlobalMaxPool1D']=GlobalMaxPool1D\n",
    "dview['Dense']=Dense\n",
    "\n",
    "@dview.parallel(block=True)\n",
    "def grid_search(params):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(params[0], params[1], activation=params[2]))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation=params[3]))\n",
    "    model.add(Dense(units=16, activation=params[4]))\n",
    "    model.add(Dense(units=len(y_train[0]), activation=params[5]))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.3, epochs=100, verbose=0)\n",
    "    return [[max(history.history['val_acc']), [num_filters, kernel_size, conv_act, act_32, act_16, out_act]]]\n",
    "\n",
    "grid_search.map(param_list)\n",
    "\n",
    "def grid_search(params):\n",
    "    with tf.device('/gpu:0'): # Specify which GPU to use.\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        # model.add(Flatten())\n",
    "        model.add(Conv1D(params[0], params[1], activation=params[2]))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dense(units=32, activation=params[3]))\n",
    "        model.add(Dense(units=16, activation=params[4]))\n",
    "        model.add(Dense(units=len(y_train[0]), activation=params[5]))\n",
    "        # compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "        # fit the model\n",
    "        history=model.fit(x_train, y_train, validation_split=0.3, epochs=100, verbose=0)\n",
    "    return [[max(history.history['val_acc']), [num_filters, kernel_size, conv_act, act_32, act_16, out_act]]]\n",
    "\n",
    "from multiprocessing import Pool\n",
    "# if __name__== \"__main__\":\n",
    "p=Pool(10)\n",
    "result_list=p.map(grid_search, param_list)\n",
    "```\n",
    "\n",
    "</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
