{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139291/139291 [==============================] - 4s 29us/step\n",
      "[0.8402533923364086, 0.8643846342917275, 0.886735872199002, 0.852459955874956]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "import keras.backend as K\n",
    "\n",
    "#precision & recall by: https://github.com/keras-team/keras/issues/5400\n",
    "#Code Source: https://cloud.google.com/blog/products/gcp/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts\n",
    "\n",
    "trainDF = pd.concat([pd.read_pickle('../../data/raw_data/MasterData_2015.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2014.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2013.pkl.gz'),\n",
    "                    pd.read_pickle('../../data/raw_data/MasterData_2012.pkl.gz')])\n",
    "                                   \n",
    "trainDF = trainDF[trainDF.TEXT.notna() & trainDF.NTEE.notna()]\n",
    "trainDF['text'] = trainDF['TEXT'].astype(str)\n",
    "trainDF['label'] = trainDF['NTEE'].astype(str)\n",
    "\n",
    "trainDF = trainDF.drop(['NTEE', 'NTEECC', 'IRS_URL', 'TEXT'], axis=1)\n",
    "trainDF = trainDF[~ (trainDF.label == 'nan')]\n",
    "\n",
    "import statistics\n",
    "\n",
    "counts = trainDF['label'].value_counts().sort_index().to_frame()\n",
    "counts['category'] = counts.index\n",
    "counts['train_sample']=(counts['label']/2).astype(int)\n",
    "    \n",
    "train_df, test_df = np.split(trainDF, [int(.7*len(trainDF))])\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "train_posts = train_df['text']\n",
    "train_tags = train_df['label']\n",
    "test_posts = test_df['text']\n",
    "test_tags = test_df['label']\n",
    "vocab_size = 1000\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = preprocessing.LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "    \n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "     Only computes a batch-wise average of recall.\n",
    "     Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "num_labels=len(train_df['label'].drop_duplicates())\n",
    "batch_size = 500\n",
    "epochs = 50\n",
    "\n",
    "# Recreate the exact same model, including weights and optimizer.\n",
    "new_model = keras.models.load_model('model_ntee_50epochs.h5', custom_objects={'precision': precision, 'recall' : recall})\n",
    "\n",
    "new_score = new_model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(new_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
