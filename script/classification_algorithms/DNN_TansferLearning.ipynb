{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9\n",
    "import os, pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "\n",
    "small_num=0\n",
    "while small_num<100: # Make sure each category has at least 100 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(40000)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    \n",
    "#### Sample ####\n",
    "small_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['labels'] = df_train['NTEE1'].apply(lambda x:ord(x)-65)\n",
    "#df_train.labels.head()\n",
    "\n",
    "trainDF['labels'] = trainDF['NTEE1'].apply(lambda x:ord(x)-65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000,), (5000,), (5000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = trainDF['mission_prgrm'].values\n",
    "labels = trainDF['labels'].values\n",
    "\n",
    "train_texts = texts[:35000]\n",
    "train_labels = labels[:35000]\n",
    "\n",
    "val_texts = texts[35000:40000]\n",
    "val_labels = labels[35000:40000]\n",
    "\n",
    "test_texts = texts[35000:]\n",
    "test_labels = labels[35000:]\n",
    "\n",
    "train_texts.shape, val_texts.shape, test_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text-preprocessing: Eliminating steps for contractions (We've --> We have)\n",
    "\n",
    "#import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def pre_process_document(document):\n",
    "    # strip HTML\n",
    "    #document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    #document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    #document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    #document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    #special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    #document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    #document = remove_special_characters(document, remove_digits=True)  \n",
    "    # remove extra whitespace\n",
    "    #document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)\n",
    "\n",
    "train_texts = pre_process_corpus(train_texts)\n",
    "val_texts = pre_process_corpus(val_texts)\n",
    "test_texts = pre_process_corpus(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, \n",
    "    batch_size=256, num_epochs=None, shuffle=True)\n",
    "    \n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the whole validation set.\n",
    "predict_val_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': val_texts}, val_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': test_texts}, test_labels, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoders: https://tfhub.dev/s?module-type=text-embedding\n",
    "\n",
    "embedding_feature = hub.text_embedding_column(\n",
    "    key='sentence', \n",
    "    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\",\n",
    "    #module_spec=\"https://tfhub.dev/google/universal-sentence-encoder-large/3\",\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndnn = tf.estimator.DNNClassifier(\\n          hidden_units=[512, 128],\\n          feature_columns=[embedding_feature],\\n          n_classes=26,\\n          activation_fn=tf.nn.relu,\\n          dropout=0.2,\\n          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\\n#75.2\\n\\n#TRY TANH Next\\n\\n\\ndnn = tf.estimator.DNNClassifier(\\n          hidden_units=[512, 128],\\n          feature_columns=[embedding_feature],\\n          n_classes=26,\\n          activation_fn=tf.nn.elu,\\n          dropout=0.3,\\n          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.1,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))\n",
    "\n",
    "#73.6\n",
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=25,\n",
    "          activation_fn=tf.nn.tanh, #relu\n",
    "          dropout=0.3, #0.3\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02)) #0.02 or 0.01\n",
    "#75\n",
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.2,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\n",
    "#75.2\n",
    "\n",
    "#TRY TANH Next\n",
    "\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.elu,\n",
    "          dropout=0.3,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 0\n",
      "Train Time (s): 96.14360070228577\n",
      "Eval Metrics (Train): {'accuracy': 0.6744, 'average_loss': 1.1618211, 'loss': 148.4078, 'global_step': 100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6772, 'average_loss': 1.1851091, 'loss': 148.13864, 'global_step': 100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 100\n",
      "Train Time (s): 101.6494493484497\n",
      "Eval Metrics (Train): {'accuracy': 0.6941429, 'average_loss': 1.1003813, 'loss': 140.55965, 'global_step': 200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6866, 'average_loss': 1.1307547, 'loss': 141.34433, 'global_step': 200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 200\n",
      "Train Time (s): 107.51870703697205\n",
      "Eval Metrics (Train): {'accuracy': 0.7026, 'average_loss': 1.0706333, 'loss': 136.75972, 'global_step': 300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.698, 'average_loss': 1.1094329, 'loss': 138.67911, 'global_step': 300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 300\n",
      "Train Time (s): 104.72884488105774\n",
      "Eval Metrics (Train): {'accuracy': 0.7047714, 'average_loss': 1.0589021, 'loss': 135.26122, 'global_step': 400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6978, 'average_loss': 1.0966697, 'loss': 137.08371, 'global_step': 400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 400\n",
      "Train Time (s): 107.41877603530884\n",
      "Eval Metrics (Train): {'accuracy': 0.70902854, 'average_loss': 1.0465826, 'loss': 133.68756, 'global_step': 500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.701, 'average_loss': 1.093874, 'loss': 136.73424, 'global_step': 500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 500\n",
      "Train Time (s): 104.02815222740173\n",
      "Eval Metrics (Train): {'accuracy': 0.70874286, 'average_loss': 1.0416639, 'loss': 133.05925, 'global_step': 600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7006, 'average_loss': 1.0894958, 'loss': 136.18698, 'global_step': 600}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 600\n",
      "Train Time (s): 107.73493766784668\n",
      "Eval Metrics (Train): {'accuracy': 0.71068573, 'average_loss': 1.0286788, 'loss': 131.40057, 'global_step': 700}\n",
      "Eval Metrics (Validation): {'accuracy': 0.704, 'average_loss': 1.0802822, 'loss': 135.03528, 'global_step': 700}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 700\n",
      "Train Time (s): 106.05744409561157\n",
      "Eval Metrics (Train): {'accuracy': 0.71374285, 'average_loss': 1.0249496, 'loss': 130.92421, 'global_step': 800}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7042, 'average_loss': 1.0793403, 'loss': 134.91754, 'global_step': 800}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 800\n",
      "Train Time (s): 104.67657160758972\n",
      "Eval Metrics (Train): {'accuracy': 0.7163714, 'average_loss': 1.0199902, 'loss': 130.29071, 'global_step': 900}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7094, 'average_loss': 1.0790606, 'loss': 134.88257, 'global_step': 900}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 900\n",
      "Train Time (s): 105.77685236930847\n",
      "Eval Metrics (Train): {'accuracy': 0.71505713, 'average_loss': 1.0122447, 'loss': 129.30133, 'global_step': 1000}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7046, 'average_loss': 1.0783886, 'loss': 134.79857, 'global_step': 1000}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1000\n",
      "Train Time (s): 105.12647318840027\n",
      "Eval Metrics (Train): {'accuracy': 0.71905714, 'average_loss': 1.0078263, 'loss': 128.73694, 'global_step': 1100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7044, 'average_loss': 1.0751076, 'loss': 134.38844, 'global_step': 1100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1100\n",
      "Train Time (s): 104.91015887260437\n",
      "Eval Metrics (Train): {'accuracy': 0.71802855, 'average_loss': 1.0039868, 'loss': 128.24649, 'global_step': 1200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7042, 'average_loss': 1.0714957, 'loss': 133.93697, 'global_step': 1200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1200\n",
      "Train Time (s): 107.8892126083374\n",
      "Eval Metrics (Train): {'accuracy': 0.7202, 'average_loss': 1.0017542, 'loss': 127.961296, 'global_step': 1300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.71, 'average_loss': 1.073149, 'loss': 134.14362, 'global_step': 1300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1300\n",
      "Train Time (s): 104.91544961929321\n",
      "Eval Metrics (Train): {'accuracy': 0.71897143, 'average_loss': 1.0012035, 'loss': 127.89097, 'global_step': 1400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.71, 'average_loss': 1.0739443, 'loss': 134.24304, 'global_step': 1400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1400\n",
      "Train Time (s): 104.92716908454895\n",
      "Eval Metrics (Train): {'accuracy': 0.72185713, 'average_loss': 0.9915897, 'loss': 126.662926, 'global_step': 1500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7096, 'average_loss': 1.0677339, 'loss': 133.46674, 'global_step': 1500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1500\n",
      "Train Time (s): 109.72622275352478\n",
      "Eval Metrics (Train): {'accuracy': 0.72382855, 'average_loss': 0.9886002, 'loss': 126.28105, 'global_step': 1600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7084, 'average_loss': 1.0652121, 'loss': 133.15152, 'global_step': 1600}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1600\n",
      "Train Time (s): 106.02084040641785\n",
      "Eval Metrics (Train): {'accuracy': 0.7237143, 'average_loss': 0.9873813, 'loss': 126.12534, 'global_step': 1700}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7096, 'average_loss': 1.0639889, 'loss': 132.99861, 'global_step': 1700}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1700\n",
      "Train Time (s): 105.53819632530212\n",
      "Eval Metrics (Train): {'accuracy': 0.72351426, 'average_loss': 0.98471934, 'loss': 125.78532, 'global_step': 1800}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7138, 'average_loss': 1.0614266, 'loss': 132.67833, 'global_step': 1800}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1800\n",
      "Train Time (s): 106.53541231155396\n",
      "Eval Metrics (Train): {'accuracy': 0.7244, 'average_loss': 0.9816684, 'loss': 125.3956, 'global_step': 1900}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7086, 'average_loss': 1.0595652, 'loss': 132.44565, 'global_step': 1900}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1900\n",
      "Train Time (s): 104.5387499332428\n",
      "Eval Metrics (Train): {'accuracy': 0.7274, 'average_loss': 0.97683394, 'loss': 124.77805, 'global_step': 2000}\n",
      "Eval Metrics (Validation): {'accuracy': 0.716, 'average_loss': 1.0563811, 'loss': 132.04764, 'global_step': 2000}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 2000\n",
      "Train Time (s): 104.49713063240051\n",
      "Eval Metrics (Train): {'accuracy': 0.7264857, 'average_loss': 0.976371, 'loss': 124.71892, 'global_step': 2100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7112, 'average_loss': 1.0573891, 'loss': 132.17365, 'global_step': 2100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 2100\n",
      "Train Time (s): 109.35731792449951\n",
      "Eval Metrics (Train): {'accuracy': 0.7286286, 'average_loss': 0.97425044, 'loss': 124.44805, 'global_step': 2200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7158, 'average_loss': 1.0564016, 'loss': 132.0502, 'global_step': 2200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 2200\n",
      "Train Time (s): 102.8009569644928\n",
      "Eval Metrics (Train): {'accuracy': 0.7290286, 'average_loss': 0.9706644, 'loss': 123.989975, 'global_step': 2300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7114, 'average_loss': 1.0525224, 'loss': 131.56529, 'global_step': 2300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 2300\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import time\n",
    "\n",
    "TOTAL_STEPS = 5000   #1500\n",
    "STEP_SIZE = 100   #100\n",
    "for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
    "    print()\n",
    "    print('-'*100)\n",
    "    print('Training for step =', step)\n",
    "    start_time = time.time()\n",
    "    dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Train Time (s):', elapsed_time)\n",
    "    print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n",
    "    print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
