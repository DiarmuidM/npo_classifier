{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9\n",
    "import os, pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "\n",
    "small_num=0\n",
    "while small_num<100: # Make sure each category has at least 100 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(40000)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    \n",
    "#### Sample ####\n",
    "small_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['labels'] = df_train['NTEE1'].apply(lambda x:ord(x)-65)\n",
    "#df_train.labels.head()\n",
    "\n",
    "trainDF['labels'] = trainDF['NTEE1'].apply(lambda x:ord(x)-65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000,), (5000,), (5000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = trainDF['mission_prgrm'].values\n",
    "labels = trainDF['labels'].values\n",
    "\n",
    "train_texts = texts[:35000]\n",
    "train_labels = labels[:35000]\n",
    "\n",
    "val_texts = texts[35000:40000]\n",
    "val_labels = labels[35000:40000]\n",
    "\n",
    "test_texts = texts[35000:]\n",
    "test_labels = labels[35000:]\n",
    "\n",
    "train_texts.shape, val_texts.shape, test_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text-preprocessing: Eliminating steps for contractions (We've --> We have)\n",
    "\n",
    "#import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def pre_process_document(document):\n",
    "    # strip HTML\n",
    "    #document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    #document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    #document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    #document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    #special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    #document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    #document = remove_special_characters(document, remove_digits=True)  \n",
    "    # remove extra whitespace\n",
    "    #document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)\n",
    "\n",
    "train_texts = pre_process_corpus(train_texts)\n",
    "val_texts = pre_process_corpus(val_texts)\n",
    "test_texts = pre_process_corpus(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, \n",
    "    batch_size=256, num_epochs=None, shuffle=True)\n",
    "    \n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the whole validation set.\n",
    "predict_val_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': val_texts}, val_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': test_texts}, test_labels, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoders: https://tfhub.dev/s?module-type=text-embedding\n",
    "\n",
    "embedding_feature = hub.text_embedding_column(\n",
    "    key='sentence', \n",
    "    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\",\n",
    "    #module_spec=\"https://tfhub.dev/google/universal-sentence-encoder-large/3\",\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndnn = tf.estimator.DNNClassifier(\\n          hidden_units=[512, 128],\\n          feature_columns=[embedding_feature],\\n          n_classes=26,\\n          activation_fn=tf.nn.elu,\\n          dropout=0.3,\\n          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.1,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))\n",
    "\n",
    "#73.6\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=25,\n",
    "          activation_fn=tf.nn.tanh, #relu\n",
    "          dropout=0.3, #0.3\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02)) #0.02 or 0.01\n",
    "#72.3\n",
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=25,\n",
    "          activation_fn=tf.nn.tanh,\n",
    "          dropout=0.2,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\n",
    "#75.2\n",
    "\n",
    "#TRY TANH Next\n",
    "'''\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.elu,\n",
    "          dropout=0.3,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 0\n",
      "Train Time (s): 110.79673957824707\n",
      "Eval Metrics (Train): {'accuracy': 0.68011427, 'average_loss': 1.1530118, 'loss': 147.28253, 'global_step': 100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6824, 'average_loss': 1.1789219, 'loss': 147.36525, 'global_step': 100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 100\n",
      "Train Time (s): 113.37750744819641\n",
      "Eval Metrics (Train): {'accuracy': 0.6937714, 'average_loss': 1.0876743, 'loss': 138.9365, 'global_step': 200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6894, 'average_loss': 1.127357, 'loss': 140.91963, 'global_step': 200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 200\n",
      "Train Time (s): 107.76857161521912\n",
      "Eval Metrics (Train): {'accuracy': 0.7028, 'average_loss': 1.06499, 'loss': 136.03888, 'global_step': 300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6988, 'average_loss': 1.1081483, 'loss': 138.51854, 'global_step': 300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 300\n",
      "Train Time (s): 108.8239803314209\n",
      "Eval Metrics (Train): {'accuracy': 0.70585716, 'average_loss': 1.055994, 'loss': 134.88976, 'global_step': 400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7016, 'average_loss': 1.1087714, 'loss': 138.59644, 'global_step': 400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 400\n",
      "Train Time (s): 126.03208017349243\n",
      "Eval Metrics (Train): {'accuracy': 0.7090857, 'average_loss': 1.0446602, 'loss': 133.442, 'global_step': 500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.701, 'average_loss': 1.096785, 'loss': 137.09811, 'global_step': 500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 500\n",
      "Train Time (s): 110.1582498550415\n",
      "Eval Metrics (Train): {'accuracy': 0.7122857, 'average_loss': 1.0269684, 'loss': 131.18208, 'global_step': 600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7046, 'average_loss': 1.0845995, 'loss': 135.57494, 'global_step': 600}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 600\n",
      "Train Time (s): 105.76253914833069\n",
      "Eval Metrics (Train): {'accuracy': 0.71297145, 'average_loss': 1.0199916, 'loss': 130.29091, 'global_step': 700}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7066, 'average_loss': 1.0791396, 'loss': 134.89246, 'global_step': 700}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 700\n",
      "Train Time (s): 104.38970375061035\n",
      "Eval Metrics (Train): {'accuracy': 0.71617144, 'average_loss': 1.0129378, 'loss': 129.38986, 'global_step': 800}\n",
      "Eval Metrics (Validation): {'accuracy': 0.704, 'average_loss': 1.0770591, 'loss': 134.6324, 'global_step': 800}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 800\n",
      "Train Time (s): 126.0518491268158\n",
      "Eval Metrics (Train): {'accuracy': 0.71982855, 'average_loss': 1.0048577, 'loss': 128.35774, 'global_step': 900}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7062, 'average_loss': 1.0738666, 'loss': 134.23332, 'global_step': 900}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 900\n",
      "Train Time (s): 107.31530404090881\n",
      "Eval Metrics (Train): {'accuracy': 0.7147143, 'average_loss': 1.0136372, 'loss': 129.4792, 'global_step': 1000}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7, 'average_loss': 1.0820526, 'loss': 135.25656, 'global_step': 1000}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1000\n",
      "Train Time (s): 101.32593202590942\n",
      "Eval Metrics (Train): {'accuracy': 0.7214286, 'average_loss': 1.0007392, 'loss': 127.83165, 'global_step': 1100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7054, 'average_loss': 1.0754448, 'loss': 134.4306, 'global_step': 1100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1100\n",
      "Train Time (s): 105.90526056289673\n",
      "Eval Metrics (Train): {'accuracy': 0.72197145, 'average_loss': 0.9928893, 'loss': 126.828926, 'global_step': 1200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7134, 'average_loss': 1.0685073, 'loss': 133.56342, 'global_step': 1200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1200\n",
      "Train Time (s): 116.68977379798889\n",
      "Eval Metrics (Train): {'accuracy': 0.72414285, 'average_loss': 0.9885458, 'loss': 126.27409, 'global_step': 1300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7126, 'average_loss': 1.066667, 'loss': 133.33337, 'global_step': 1300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1300\n",
      "Train Time (s): 106.0686502456665\n",
      "Eval Metrics (Train): {'accuracy': 0.72282857, 'average_loss': 0.98158884, 'loss': 125.38544, 'global_step': 1400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7106, 'average_loss': 1.0640535, 'loss': 133.00668, 'global_step': 1400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1400\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import time\n",
    "\n",
    "TOTAL_STEPS = 5000   #1500\n",
    "STEP_SIZE = 100   #100\n",
    "for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
    "    print()\n",
    "    print('-'*100)\n",
    "    print('Training for step =', step)\n",
    "    start_time = time.time()\n",
    "    dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Train Time (s):', elapsed_time)\n",
    "    print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n",
    "    print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
