{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9\n",
    "import os, pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "\n",
    "small_num=0\n",
    "while small_num<100: # Make sure each category has at least 100 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(40000)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    \n",
    "#### Sample ####\n",
    "#len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592785    10\n",
       "592787    10\n",
       "592789    13\n",
       "592797    11\n",
       "592800    14\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['labels'] = df_train['NTEE1'].apply(lambda x:ord(x)-65)\n",
    "df_train.labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500,), (500,), (225472,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df_train['mission_prgrm'].values\n",
    "labels = df_train['labels'].values\n",
    "\n",
    "train_texts = texts[:3500]\n",
    "train_labels = labels[:3500]\n",
    "\n",
    "val_texts = texts[3500:4000]\n",
    "val_labels = labels[3500:4000]\n",
    "\n",
    "test_texts = texts[4000:]\n",
    "test_labels = labels[4000:]\n",
    "\n",
    "train_texts.shape, val_texts.shape, test_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-c64df78b4bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mval_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mtest_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0motypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2052\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 res = tuple([array(x, copy=False, subok=True, dtype=t)\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#text-preprocessing: Eliminating steps for contractions (We've --> We have)\n",
    "\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def pre_process_document(document):\n",
    "    # strip HTML\n",
    "    #document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    #document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    #document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    document = remove_special_characters(document, remove_digits=True)  \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)\n",
    "\n",
    "train_texts = pre_process_corpus(train_texts)\n",
    "val_texts = pre_process_corpus(val_texts)\n",
    "test_texts = pre_process_corpus(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, \n",
    "    batch_size=256, num_epochs=None, shuffle=True)\n",
    "    \n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the whole validation set.\n",
    "predict_val_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': val_texts}, val_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': test_texts}, test_labels, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_feature = hub.text_embedding_column(\n",
    "    key='sentence', \n",
    "    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\",\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.1,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))\n",
    "\n",
    "#73.6\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.3,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.01))\n",
    "#75\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 0\n",
      "Train Time (s): 115.39055562019348\n",
      "Eval Metrics (Train): {'accuracy': 0.71685714, 'average_loss': 0.9685228, 'loss': 121.06535, 'global_step': 100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.724, 'average_loss': 1.0634102, 'loss': 132.92627, 'global_step': 100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 100\n",
      "Train Time (s): 126.86871266365051\n",
      "Eval Metrics (Train): {'accuracy': 0.76428574, 'average_loss': 0.7857052, 'loss': 98.21315, 'global_step': 200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.738, 'average_loss': 0.98027015, 'loss': 122.53377, 'global_step': 200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 200\n",
      "Train Time (s): 124.98955655097961\n",
      "Eval Metrics (Train): {'accuracy': 0.7962857, 'average_loss': 0.6892647, 'loss': 86.15809, 'global_step': 300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.732, 'average_loss': 0.9607733, 'loss': 120.096664, 'global_step': 300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 300\n",
      "Train Time (s): 123.26715016365051\n",
      "Eval Metrics (Train): {'accuracy': 0.8214286, 'average_loss': 0.6200419, 'loss': 77.50524, 'global_step': 400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.748, 'average_loss': 0.9575961, 'loss': 119.69952, 'global_step': 400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 400\n",
      "Train Time (s): 126.2143452167511\n",
      "Eval Metrics (Train): {'accuracy': 0.8371429, 'average_loss': 0.5631081, 'loss': 70.38851, 'global_step': 500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.746, 'average_loss': 0.95005554, 'loss': 118.75694, 'global_step': 500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 500\n",
      "Train Time (s): 124.17299962043762\n",
      "Eval Metrics (Train): {'accuracy': 0.84828573, 'average_loss': 0.5183535, 'loss': 64.7942, 'global_step': 600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.754, 'average_loss': 0.96020496, 'loss': 120.02562, 'global_step': 600}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 600\n",
      "Train Time (s): 124.12235951423645\n",
      "Eval Metrics (Train): {'accuracy': 0.8665714, 'average_loss': 0.47146845, 'loss': 58.933556, 'global_step': 700}\n",
      "Eval Metrics (Validation): {'accuracy': 0.744, 'average_loss': 0.9718365, 'loss': 121.47956, 'global_step': 700}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 700\n",
      "Train Time (s): 128.80561637878418\n",
      "Eval Metrics (Train): {'accuracy': 0.87857145, 'average_loss': 0.4328252, 'loss': 54.10315, 'global_step': 800}\n",
      "Eval Metrics (Validation): {'accuracy': 0.75, 'average_loss': 0.9753708, 'loss': 121.92136, 'global_step': 800}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 800\n",
      "Train Time (s): 123.8553740978241\n",
      "Eval Metrics (Train): {'accuracy': 0.886, 'average_loss': 0.4023743, 'loss': 50.296787, 'global_step': 900}\n",
      "Eval Metrics (Validation): {'accuracy': 0.746, 'average_loss': 0.98864233, 'loss': 123.58029, 'global_step': 900}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 900\n",
      "Train Time (s): 123.14798927307129\n",
      "Train Time (s): 125.61188268661499\n",
      "Eval Metrics (Train): {'accuracy': 0.9045714, 'average_loss': 0.343137, 'loss': 42.892124, 'global_step': 1100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.742, 'average_loss': 1.0079255, 'loss': 125.990685, 'global_step': 1100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1100\n",
      "Train Time (s): 124.38835263252258\n",
      "Eval Metrics (Train): {'accuracy': 0.9137143, 'average_loss': 0.31635812, 'loss': 39.544765, 'global_step': 1200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.748, 'average_loss': 1.0061785, 'loss': 125.772316, 'global_step': 1200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1200\n",
      "Train Time (s): 126.83906507492065\n",
      "Eval Metrics (Train): {'accuracy': 0.92, 'average_loss': 0.2927244, 'loss': 36.59055, 'global_step': 1300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.75, 'average_loss': 1.0186821, 'loss': 127.335266, 'global_step': 1300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1300\n",
      "Train Time (s): 124.37533950805664\n",
      "Eval Metrics (Train): {'accuracy': 0.928, 'average_loss': 0.2711435, 'loss': 33.892937, 'global_step': 1400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.748, 'average_loss': 1.031054, 'loss': 128.88176, 'global_step': 1400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1400\n",
      "Train Time (s): 130.55411291122437\n",
      "Eval Metrics (Train): {'accuracy': 0.93628573, 'average_loss': 0.25171003, 'loss': 31.46375, 'global_step': 1500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.746, 'average_loss': 1.0470088, 'loss': 130.8761, 'global_step': 1500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1500\n",
      "Train Time (s): 133.80135822296143\n",
      "Eval Metrics (Train): {'accuracy': 0.9408572, 'average_loss': 0.23307054, 'loss': 29.133818, 'global_step': 1600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.738, 'average_loss': 1.0643631, 'loss': 133.0454, 'global_step': 1600}\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import time\n",
    "\n",
    "TOTAL_STEPS = 1500\n",
    "STEP_SIZE = 100\n",
    "for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
    "    print()\n",
    "    print('-'*100)\n",
    "    print('Training for step =', step)\n",
    "    start_time = time.time()\n",
    "    dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Train Time (s):', elapsed_time)\n",
    "    print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n",
    "    print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
