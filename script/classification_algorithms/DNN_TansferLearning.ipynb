{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9\n",
    "import os, pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "\n",
    "small_num=0\n",
    "while small_num<100: # Make sure each category has at least 100 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(40000)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    \n",
    "#### Sample ####\n",
    "#len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592785    10\n",
       "592787    10\n",
       "592789    13\n",
       "592797    11\n",
       "592800    14\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['labels'] = df_train['NTEE1'].apply(lambda x:ord(x)-65)\n",
    "df_train.labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500,), (500,), (225472,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df_train['mission_prgrm'].values\n",
    "labels = df_train['labels'].values\n",
    "\n",
    "train_texts = texts[:3500]\n",
    "train_labels = labels[:3500]\n",
    "\n",
    "val_texts = texts[3500:4000]\n",
    "val_labels = labels[3500:4000]\n",
    "\n",
    "test_texts = texts[4000:]\n",
    "test_labels = labels[4000:]\n",
    "\n",
    "train_texts.shape, val_texts.shape, test_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0b607e86a46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mval_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mtest_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0motypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2052\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 res = tuple([array(x, copy=False, subok=True, dtype=t)\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#text-preprocessing: Eliminating steps for contractions (We've --> We have)\n",
    "\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def pre_process_document(document):\n",
    "    # strip HTML\n",
    "    #document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    #document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    #document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    document = remove_special_characters(document, remove_digits=True)  \n",
    "    # remove extra whitespace\n",
    "    #document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)\n",
    "\n",
    "train_texts = pre_process_corpus(train_texts)\n",
    "val_texts = pre_process_corpus(val_texts)\n",
    "test_texts = pre_process_corpus(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, \n",
    "    batch_size=256, num_epochs=None, shuffle=True)\n",
    "    \n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the whole validation set.\n",
    "predict_val_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': val_texts}, val_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': test_texts}, test_labels, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_feature = hub.text_embedding_column(\n",
    "    key='sentence', \n",
    "    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\",\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.1,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))\n",
    "\n",
    "#73.6\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.3,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.01))\n",
    "#75\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.3,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\n",
    "#75.2\n",
    "\n",
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.elu,\n",
    "          dropout=0.3,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 0\n",
      "Train Time (s): 161.90317130088806\n",
      "Eval Metrics (Train): {'accuracy': 0.76257145, 'average_loss': 0.7984377, 'loss': 99.80471, 'global_step': 100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.742, 'average_loss': 0.9658379, 'loss': 120.72974, 'global_step': 100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 100\n",
      "Train Time (s): 206.26475620269775\n",
      "Eval Metrics (Train): {'accuracy': 0.80057144, 'average_loss': 0.6698809, 'loss': 83.735115, 'global_step': 200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.748, 'average_loss': 0.9680547, 'loss': 121.006836, 'global_step': 200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 200\n",
      "Train Time (s): 188.30295634269714\n",
      "Eval Metrics (Train): {'accuracy': 0.8165714, 'average_loss': 0.6092719, 'loss': 76.15899, 'global_step': 300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.744, 'average_loss': 1.0095692, 'loss': 126.19615, 'global_step': 300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 300\n",
      "Train Time (s): 149.1354784965515\n",
      "Eval Metrics (Train): {'accuracy': 0.8388571, 'average_loss': 0.540041, 'loss': 67.50513, 'global_step': 400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.744, 'average_loss': 1.019331, 'loss': 127.416374, 'global_step': 400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 400\n",
      "Train Time (s): 127.6613757610321\n",
      "Eval Metrics (Train): {'accuracy': 0.85714287, 'average_loss': 0.48758495, 'loss': 60.94812, 'global_step': 500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.734, 'average_loss': 1.0470324, 'loss': 130.87904, 'global_step': 500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 500\n",
      "Train Time (s): 125.44848608970642\n",
      "Eval Metrics (Train): {'accuracy': 0.864, 'average_loss': 0.4556264, 'loss': 56.9533, 'global_step': 600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.728, 'average_loss': 1.0807782, 'loss': 135.09727, 'global_step': 600}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 600\n",
      "Train Time (s): 127.40340566635132\n",
      "Eval Metrics (Train): {'accuracy': 0.88771427, 'average_loss': 0.40668735, 'loss': 50.83592, 'global_step': 700}\n",
      "Eval Metrics (Validation): {'accuracy': 0.74, 'average_loss': 1.07873, 'loss': 134.84125, 'global_step': 700}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 700\n",
      "Train Time (s): 129.14493012428284\n",
      "Eval Metrics (Train): {'accuracy': 0.88771427, 'average_loss': 0.38115963, 'loss': 47.644955, 'global_step': 800}\n",
      "Eval Metrics (Validation): {'accuracy': 0.732, 'average_loss': 1.1318148, 'loss': 141.47685, 'global_step': 800}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 800\n",
      "Train Time (s): 128.22770357131958\n",
      "Eval Metrics (Train): {'accuracy': 0.8997143, 'average_loss': 0.35748243, 'loss': 44.685303, 'global_step': 900}\n",
      "Eval Metrics (Validation): {'accuracy': 0.728, 'average_loss': 1.1353164, 'loss': 141.91455, 'global_step': 900}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 900\n",
      "Train Time (s): 129.1201455593109\n",
      "Eval Metrics (Train): {'accuracy': 0.90885717, 'average_loss': 0.32374275, 'loss': 40.467842, 'global_step': 1000}\n",
      "Eval Metrics (Validation): {'accuracy': 0.726, 'average_loss': 1.1572611, 'loss': 144.65764, 'global_step': 1000}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1000\n",
      "Train Time (s): 125.25383281707764\n",
      "Eval Metrics (Train): {'accuracy': 0.91571426, 'average_loss': 0.30495986, 'loss': 38.119984, 'global_step': 1100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.712, 'average_loss': 1.1808828, 'loss': 147.61035, 'global_step': 1100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1100\n",
      "Train Time (s): 129.99562406539917\n",
      "Eval Metrics (Train): {'accuracy': 0.92542857, 'average_loss': 0.2805358, 'loss': 35.066975, 'global_step': 1200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.708, 'average_loss': 1.2109067, 'loss': 151.36334, 'global_step': 1200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1200\n",
      "Train Time (s): 126.56912040710449\n",
      "Eval Metrics (Train): {'accuracy': 0.9265714, 'average_loss': 0.2658194, 'loss': 33.227425, 'global_step': 1300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.71, 'average_loss': 1.2455735, 'loss': 155.69669, 'global_step': 1300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1300\n",
      "Train Time (s): 124.38610553741455\n",
      "Eval Metrics (Train): {'accuracy': 0.932, 'average_loss': 0.24635716, 'loss': 30.794645, 'global_step': 1400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.708, 'average_loss': 1.28218, 'loss': 160.27249, 'global_step': 1400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1400\n",
      "Train Time (s): 126.22411489486694\n",
      "Eval Metrics (Train): {'accuracy': 0.9374286, 'average_loss': 0.24028915, 'loss': 30.036144, 'global_step': 1500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.706, 'average_loss': 1.3118161, 'loss': 163.977, 'global_step': 1500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1500\n",
      "Train Time (s): 133.35636138916016\n",
      "Eval Metrics (Train): {'accuracy': 0.94314283, 'average_loss': 0.2202055, 'loss': 27.525686, 'global_step': 1600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.696, 'average_loss': 1.3372328, 'loss': 167.1541, 'global_step': 1600}\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import time\n",
    "\n",
    "TOTAL_STEPS = 1500   #1500\n",
    "STEP_SIZE = 100   #100\n",
    "for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
    "    print()\n",
    "    print('-'*100)\n",
    "    print('Training for step =', step)\n",
    "    start_time = time.time()\n",
    "    dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Train Time (s):', elapsed_time)\n",
    "    print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n",
    "    print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
