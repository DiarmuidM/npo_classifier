{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9\n",
    "import os, pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "\n",
    "small_num=0\n",
    "while small_num<100: # Make sure each category has at least 100 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(40000)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    \n",
    "#### Sample ####\n",
    "len(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['labels'] = df_train['NTEE1'].apply(lambda x:ord(x)-65)\n",
    "#df_train.labels.head()\n",
    "\n",
    "trainDF['labels'] = trainDF['NTEE1'].apply(lambda x:ord(x)-65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000,), (5000,), (5000,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = trainDF['mission_prgrm'].values\n",
    "labels = trainDF['labels'].values\n",
    "\n",
    "train_texts = texts[:30000]\n",
    "train_labels = labels[:30000]\n",
    "\n",
    "val_texts = texts[30000:35000]\n",
    "val_labels = labels[30000:35000]\n",
    "\n",
    "test_texts = texts[35000:]\n",
    "test_labels = labels[35000:]\n",
    "\n",
    "train_texts.shape, val_texts.shape, test_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text-preprocessing: Eliminating steps for contractions (We've --> We have)\n",
    "\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def pre_process_document(document):\n",
    "    # strip HTML\n",
    "    #document = strip_html_tags(document)\n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    #document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    # remove accented characters\n",
    "    #document = remove_accented_chars(document)\n",
    "    # expand contractions    \n",
    "    #document = expand_contractions(document)  \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    #special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    #document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    #document = remove_special_characters(document, remove_digits=True)  \n",
    "    # remove extra whitespace\n",
    "    #document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "pre_process_corpus = np.vectorize(pre_process_document)\n",
    "\n",
    "train_texts = pre_process_corpus(train_texts)\n",
    "val_texts = pre_process_corpus(val_texts)\n",
    "test_texts = pre_process_corpus(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, \n",
    "    batch_size=256, num_epochs=None, shuffle=True)\n",
    "    \n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': train_texts}, train_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the whole validation set.\n",
    "predict_val_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': val_texts}, val_labels, shuffle=False)\n",
    "    \n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': test_texts}, test_labels, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoders: https://tfhub.dev/s?module-type=text-embedding\n",
    "\n",
    "embedding_feature = hub.text_embedding_column(\n",
    "    key='sentence', \n",
    "    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\",\n",
    "    #module_spec=\"https://tfhub.dev/google/universal-sentence-encoder-large/3\",\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndnn = tf.estimator.DNNClassifier(\\n          hidden_units=[512, 128],\\n          feature_columns=[embedding_feature],\\n          n_classes=26,\\n          activation_fn=tf.nn.elu,\\n          dropout=0.3,\\n          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.1,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))\n",
    "\n",
    "#73.6\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.3,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.01))\n",
    "#75\n",
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.2,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\n",
    "#75.2\n",
    "\n",
    "#TRY TANH Next\n",
    "\n",
    "'''\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=26,\n",
    "          activation_fn=tf.nn.elu,\n",
    "          dropout=0.3,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.02))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 0\n",
      "Train Time (s): 103.63856220245361\n",
      "Eval Metrics (Train): {'accuracy': 0.6708, 'average_loss': 1.1908067, 'loss': 152.01788, 'global_step': 100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6722, 'average_loss': 1.1874663, 'loss': 148.43327, 'global_step': 100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 100\n",
      "Train Time (s): 147.3950629234314\n",
      "Eval Metrics (Train): {'accuracy': 0.69593334, 'average_loss': 1.0851234, 'loss': 138.5264, 'global_step': 200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6952, 'average_loss': 1.0916027, 'loss': 136.45033, 'global_step': 200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 200\n",
      "Train Time (s): 143.4216492176056\n",
      "Eval Metrics (Train): {'accuracy': 0.7097, 'average_loss': 1.0391227, 'loss': 132.65396, 'global_step': 300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.707, 'average_loss': 1.0582274, 'loss': 132.27843, 'global_step': 300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 300\n",
      "Train Time (s): 134.68166661262512\n",
      "Eval Metrics (Train): {'accuracy': 0.71743333, 'average_loss': 1.0140214, 'loss': 129.44954, 'global_step': 400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7132, 'average_loss': 1.0418526, 'loss': 130.23158, 'global_step': 400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 400\n",
      "Train Time (s): 131.3998556137085\n",
      "Eval Metrics (Train): {'accuracy': 0.72173333, 'average_loss': 0.99362147, 'loss': 126.8453, 'global_step': 500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7146, 'average_loss': 1.0323523, 'loss': 129.04404, 'global_step': 500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 500\n",
      "Train Time (s): 123.90803980827332\n",
      "Eval Metrics (Train): {'accuracy': 0.7267, 'average_loss': 0.9753877, 'loss': 124.51758, 'global_step': 600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7154, 'average_loss': 1.0246305, 'loss': 128.07883, 'global_step': 600}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 600\n",
      "Train Time (s): 130.6242973804474\n",
      "Eval Metrics (Train): {'accuracy': 0.7309333, 'average_loss': 0.95500433, 'loss': 121.91545, 'global_step': 700}\n",
      "Eval Metrics (Validation): {'accuracy': 0.72, 'average_loss': 1.0145682, 'loss': 126.82103, 'global_step': 700}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 700\n",
      "Train Time (s): 134.46007752418518\n",
      "Eval Metrics (Train): {'accuracy': 0.73686665, 'average_loss': 0.9376875, 'loss': 119.70479, 'global_step': 800}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7206, 'average_loss': 1.0058261, 'loss': 125.72826, 'global_step': 800}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 800\n",
      "Train Time (s): 120.27437019348145\n",
      "Eval Metrics (Train): {'accuracy': 0.7374, 'average_loss': 0.92697555, 'loss': 118.3373, 'global_step': 900}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7236, 'average_loss': 1.0022963, 'loss': 125.28703, 'global_step': 900}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 900\n",
      "Train Time (s): 126.47259569168091\n",
      "Eval Metrics (Train): {'accuracy': 0.7432, 'average_loss': 0.913616, 'loss': 116.63183, 'global_step': 1000}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7258, 'average_loss': 0.9987692, 'loss': 124.84615, 'global_step': 1000}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1000\n",
      "Train Time (s): 127.22819685935974\n",
      "Eval Metrics (Train): {'accuracy': 0.74633336, 'average_loss': 0.89557886, 'loss': 114.329216, 'global_step': 1100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7268, 'average_loss': 0.99117005, 'loss': 123.896255, 'global_step': 1100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1100\n",
      "Train Time (s): 123.7989239692688\n",
      "Eval Metrics (Train): {'accuracy': 0.75013334, 'average_loss': 0.88516575, 'loss': 112.999886, 'global_step': 1200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7256, 'average_loss': 0.9862969, 'loss': 123.28711, 'global_step': 1200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1200\n",
      "Train Time (s): 143.08107924461365\n",
      "Eval Metrics (Train): {'accuracy': 0.7525, 'average_loss': 0.8766838, 'loss': 111.917076, 'global_step': 1300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7252, 'average_loss': 0.98500615, 'loss': 123.12577, 'global_step': 1300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1300\n",
      "Train Time (s): 123.18200969696045\n",
      "Eval Metrics (Train): {'accuracy': 0.75616664, 'average_loss': 0.864879, 'loss': 110.41009, 'global_step': 1400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7274, 'average_loss': 0.98355466, 'loss': 122.944336, 'global_step': 1400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1400\n",
      "Train Time (s): 127.80571961402893\n",
      "Eval Metrics (Train): {'accuracy': 0.75706667, 'average_loss': 0.8555449, 'loss': 109.2185, 'global_step': 1500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.728, 'average_loss': 0.98537034, 'loss': 123.17129, 'global_step': 1500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1500\n",
      "Train Time (s): 137.93267178535461\n",
      "Eval Metrics (Train): {'accuracy': 0.76376665, 'average_loss': 0.8404038, 'loss': 107.28559, 'global_step': 1600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.7324, 'average_loss': 0.9774794, 'loss': 122.18492, 'global_step': 1600}\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import time\n",
    "\n",
    "TOTAL_STEPS = 1500   #1500\n",
    "STEP_SIZE = 100   #100\n",
    "for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
    "    print()\n",
    "    print('-'*100)\n",
    "    print('Training for step =', step)\n",
    "    start_time = time.time()\n",
    "    dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Train Time (s):', elapsed_time)\n",
    "    print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n",
    "    print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
