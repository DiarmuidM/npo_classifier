{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "import os, pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229472"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "\n",
    "len(df_train['mission_prgrm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_num=0\n",
    "while small_num<100: # Make sure each category has at least 100 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(60000)\n",
    "    small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "small_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"    \n",
    "    return string.strip().lower()\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(len(trainDF)):\n",
    "    text = trainDF['mission_prgrm'].values[idx]\n",
    "    texts.append(clean_str(text))\n",
    "    labels.append(ord(trainDF['NTEE1'].values[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84204 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "MAX_NB_WORDS = 2000\n",
    "MAX_SEQUENCE_LENGTH=1000\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (60000, 1000)\n",
      "Shape of label tensor: (60000, 26)\n"
     ]
    }
   ],
   "source": [
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "labels1 = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    pos = np.zeros((26,), dtype=int)\n",
    "    pos[labels[i]-65] = 1\n",
    "    labels1.append(np.asarray(pos))\n",
    "\n",
    "labels = np.asarray(labels1)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "VALIDATION_SPLIT = 0.7\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:nb_validation_samples]\n",
    "y_train = labels[:nb_validation_samples]\n",
    "x_val = data[nb_validation_samples:]\n",
    "y_val = labels[nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-6f1d876e78a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "EMBEDDING_DIM=100\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 1000, 100)         8420500   \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 996, 256)          128256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 199, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 195, 256)          327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 39, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 35, 256)           327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 9,306,670\n",
      "Trainable params: 886,170\n",
      "Non-trainable params: 8,420,500\n",
      "_________________________________________________________________\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 125s 3ms/step - loss: 2.7077 - acc: 0.2208 - val_loss: 2.2155 - val_acc: 0.3669\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 124s 3ms/step - loss: 2.0525 - acc: 0.4265 - val_loss: 1.7724 - val_acc: 0.5030\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 126s 3ms/step - loss: 1.7627 - acc: 0.5156 - val_loss: 1.8635 - val_acc: 0.4864\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 124s 3ms/step - loss: 1.6005 - acc: 0.5630 - val_loss: 1.6353 - val_acc: 0.5556\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 124s 3ms/step - loss: 1.4771 - acc: 0.5988 - val_loss: 1.4699 - val_acc: 0.6034\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 126s 3ms/step - loss: 1.3985 - acc: 0.6189 - val_loss: 1.5191 - val_acc: 0.5930\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 124s 3ms/step - loss: 1.3464 - acc: 0.6323 - val_loss: 1.4645 - val_acc: 0.5997\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 124s 3ms/step - loss: 1.2858 - acc: 0.6447 - val_loss: 1.3350 - val_acc: 0.6331\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 123s 3ms/step - loss: 1.2564 - acc: 0.6541 - val_loss: 1.3212 - val_acc: 0.6428\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 125s 3ms/step - loss: 1.2156 - acc: 0.6628 - val_loss: 1.2864 - val_acc: 0.6553\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 125s 3ms/step - loss: 1.1706 - acc: 0.6725 - val_loss: 1.4524 - val_acc: 0.6141\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 126s 3ms/step - loss: 1.1486 - acc: 0.6784 - val_loss: 1.2984 - val_acc: 0.6541\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 125s 3ms/step - loss: 1.1082 - acc: 0.6871 - val_loss: 1.3212 - val_acc: 0.6504\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 126s 3ms/step - loss: 1.0831 - acc: 0.6949 - val_loss: 1.2945 - val_acc: 0.6529\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 124s 3ms/step - loss: 1.0483 - acc: 0.7017 - val_loss: 1.4529 - val_acc: 0.6161\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 125s 3ms/step - loss: 1.0313 - acc: 0.7032 - val_loss: 1.3551 - val_acc: 0.6489\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 124s 3ms/step - loss: 0.9990 - acc: 0.7121 - val_loss: 1.3255 - val_acc: 0.6553\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 125s 3ms/step - loss: 0.9724 - acc: 0.7197 - val_loss: 1.3248 - val_acc: 0.6519\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 125s 3ms/step - loss: 0.9468 - acc: 0.7233 - val_loss: 1.4094 - val_acc: 0.6642\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 125s 3ms/step - loss: 0.9163 - acc: 0.7313 - val_loss: 1.3087 - val_acc: 0.6732\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(filters=256, kernel_size=5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(pool_size=5)(x)\n",
    "x = Conv1D(filters=256, kernel_size=5, activation='relu')(x)\n",
    "x = MaxPooling1D(pool_size=5)(x)\n",
    "x = Conv1D(filters=256, kernel_size=5, activation='relu')(x)\n",
    "x = MaxPooling1D(pool_size=35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(units=256, activation='relu')(x)\n",
    "x = Dense(units=128, activation='relu')(x)\n",
    "preds = Dense(units=26, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=500)\n",
    "\n",
    "#128,5 ; 5; 128,5; 5; 128; 5; 35; 128; 26 : 67.64\n",
    "# 3,,,25,: 29.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 1s 295us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0775147477785745, 0.431999996304512]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, \n",
    "                   batch_size=500, verbose=1)\n",
    "\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
