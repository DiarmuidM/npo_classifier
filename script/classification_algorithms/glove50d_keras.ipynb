{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "- [x] Build NTEE-10 major groups.\n",
    "- [x] Vectorize output labels.\n",
    "- [x] Vectorize input texts.\n",
    "- [x] Spell check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code as 10 major groups.\n",
    "major_group_dict={'I': ['A'],\n",
    "                  'II': ['B'],\n",
    "                  'III': ['C', 'D'],\n",
    "                  'IV': ['E', 'F', 'G', 'H'],\n",
    "                  'V': ['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'],\n",
    "                  'VI': ['Q'],\n",
    "                  'VII': ['R', 'S', 'T', 'U', 'V', 'W'],\n",
    "                  'VIII': ['X'],\n",
    "                  'IX': ['Y'],\n",
    "                  'X': ['Z'],\n",
    "                 }\n",
    "def ntee2major(string):\n",
    "    global major_group_dict\n",
    "    return [s for s in major_group_dict.keys() if string in major_group_dict[s]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229472, 25, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "df_train['mission_prgrm']=df_train['mission']+'; '+df_train['prgrm_dsc']\n",
    "df_train['NTEE_M']=df_train['NTEE1'].apply(ntee2major)\n",
    "\n",
    "len(df_train['mission_prgrm']), len(df_train['NTEE1'].drop_duplicates()), len(df_train['NTEE_M'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training data frame.\n",
    "small_num=0\n",
    "while small_num<500: # Make sure each category has at least 500 records.\n",
    "    trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(60000)\n",
    "    small_num=trainDF.groupby('NTEE_M').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "# Build validation data frame.\n",
    "small_num=0\n",
    "while small_num<500: # Make sure each category has at least 500 records.\n",
    "    valDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(60000)\n",
    "    small_num=valDF.groupby('NTEE_M').count().sort_values('EIN').iloc[0]['EIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE_M\n",
      "I       0.111017\n",
      "II      0.165617\n",
      "III     0.049750\n",
      "IV      0.117050\n",
      "IX      0.038683\n",
      "V       0.300617\n",
      "VI      0.013383\n",
      "VII     0.175250\n",
      "VIII    0.028633\n",
      "Name: EIN, dtype: float64 \n",
      "\n",
      " NTEE_M\n",
      "I       0.110983\n",
      "II      0.168417\n",
      "III     0.051317\n",
      "IV      0.114367\n",
      "IX      0.037967\n",
      "V       0.299867\n",
      "VI      0.013500\n",
      "VII     0.174300\n",
      "VIII    0.029283\n",
      "Name: EIN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# See the composition by NTEE major groups.\n",
    "print(trainDF.groupby('NTEE_M').count()['EIN']/len(trainDF), '\\n'*2, valDF.groupby('NTEE_M').count()['EIN']/len(valDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label_list, class_list):\n",
    "    int_encoder=LabelEncoder().fit(class_list) # Build the encoder.\n",
    "    label_int_encoded=int_encoder.transform(label_list) # One-dimensional integer encoded.\n",
    "    return np_utils.to_categorical(label_int_encoded) # Multi-dimensional binary/one-hot encoded.\n",
    "\n",
    "y_train=one_hot(label_list=trainDF['NTEE_M'], class_list=list(major_group_dict.keys()))\n",
    "y_val=one_hot(label_list=valDF['NTEE_M'], class_list=list(major_group_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=stopwords.words('english')+list(string.punctuation)\n",
    "def tokenize_stopwords_remove(string):\n",
    "    global stop_list\n",
    "    return [s for s in nltk.word_tokenize(string) if s not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=trainDF['mission_prgrm'].apply(tokenize_stopwords_remove)\n",
    "text_token_list_val=valDF['mission_prgrm'].apply(tokenize_stopwords_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell check function. Return corrected word if unknown; return original word if known.\n",
    "def spellcheck(word_string_list):\n",
    "    return [SpellChecker().correction(word=s).upper() for s in word_string_list]\n",
    "\n",
    "# Parallel computing\n",
    "p = Pool(48)\n",
    "text_token_list_train=p.map(spellcheck, text_token_list_train)\n",
    "text_token_list_val=p.map(spellcheck, text_token_list_val)\n",
    "# Pool.map keep the original order of data passed to map.\n",
    "# https://stackoverflow.com/questions/41273960/python-3-does-pool-keep-the-original-order-of-data-passed-to-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_val.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_val=tokenizer.texts_to_sequences(text_token_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "x_val=pad_sequences(sequences=seq_encoding_text_val,\n",
    "                    maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                    dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                    value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@Petuum/embeddings-a-matrix-of-meaning-4de877c9aa27\n",
    "# Note that in the embedding matrix above, each row corresponds to a word and each column corresponds to a dimension (axis). \n",
    "# Typically, we store this in a dense fashion, where we have a list of words and row ID’s which map to the corresponding row of the matrix. \n",
    "# For the above example, we’d have the following list in addition to the matrix:\n",
    "# { hello: 0, there: 1, texas: 2, world: 3, … }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd ~/work_dir/npo_classifier/dataset; mkdir glove.6B; wget http://nlp.stanford.edu/data/glove.6B.zip; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not using pre-trained embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index), # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=32, # Size of the vector space in which words will be embedded.\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(max([len(s) for s in seq_encoding_text_train]),), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=len(y_train[0]), activation='softmax')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics.\n",
    "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 33350, 32)         5723136   \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1067200)           0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               546406912 \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 552,263,689\n",
      "Trainable params: 552,263,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='sigmoid'))\n",
    "model.add(Dense(units=256, activation='sigmoid'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='relu'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', precision, recall])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/25\n",
      "42000/42000 [==============================] - 4069s 97ms/step - loss: 5.1290 - acc: 0.6759 - precision: 0.0382 - recall: 0.0840 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0347 - val_recall: 0.0704\n",
      "Epoch 2/25\n",
      "42000/42000 [==============================] - 4118s 98ms/step - loss: 5.0941 - acc: 0.6784 - precision: 0.0347 - recall: 0.0699 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0343 - val_recall: 0.0690\n",
      "Epoch 3/25\n",
      "42000/42000 [==============================] - 4128s 98ms/step - loss: 5.0941 - acc: 0.6784 - precision: 0.0343 - recall: 0.0689 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0341 - val_recall: 0.0685\n",
      "Epoch 4/25\n",
      "42000/42000 [==============================] - 4137s 99ms/step - loss: 5.0941 - acc: 0.6784 - precision: 0.0341 - recall: 0.0684 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0341 - val_recall: 0.0683\n",
      "Epoch 5/25\n",
      "42000/42000 [==============================] - 4153s 99ms/step - loss: 5.0941 - acc: 0.6784 - precision: 0.0341 - recall: 0.0683 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0340 - val_recall: 0.0682\n",
      "Epoch 6/25\n",
      "42000/42000 [==============================] - 4171s 99ms/step - loss: 5.0941 - acc: 0.6784 - precision: 0.0341 - recall: 0.0683 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0340 - val_recall: 0.0681\n",
      "Epoch 7/25\n",
      "42000/42000 [==============================] - 4173s 99ms/step - loss: 5.0941 - acc: 0.6784 - precision: 0.0340 - recall: 0.0680 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0340 - val_recall: 0.0681\n",
      "Epoch 8/25\n",
      "42000/42000 [==============================] - 4170s 99ms/step - loss: 5.0941 - acc: 0.6784 - precision: 0.0340 - recall: 0.0681 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0340 - val_recall: 0.0680\n",
      "Epoch 9/25\n",
      "42000/42000 [==============================] - 4175s 99ms/step - loss: 5.0941 - acc: 0.6784 - precision: 0.0340 - recall: 0.0680 - val_loss: 5.0882 - val_acc: 0.6787 - val_precision: 0.0340 - val_recall: 0.0680\n",
      "Epoch 10/25\n",
      "32384/42000 [======================>.......] - ETA: 15:14 - loss: 5.0957 - acc: 0.6783 - precision: 0.0340 - recall: 0.0680"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-d4923d4cdd43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# Batch size: https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pre-trained GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics.\n",
    "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "# model.add(Dense(units=256, activation='sigmoid'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='relu'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', precision, recall])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 200)         6692600   \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               102400512 \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 109,159,937\n",
      "Trainable params: 102,467,337\n",
      "Non-trainable params: 6,692,600\n",
      "_________________________________________________________________\n",
      "Train on 8399 samples, validate on 33601 samples\n",
      "Epoch 1/2\n",
      "8399/8399 [==============================] - 125s 15ms/step - loss: 0.2489 - acc: 0.9057 - precision: 0.1104 - recall: 0.9940 - val_loss: 0.2185 - val_acc: 0.9155 - val_precision: 0.1111 - val_recall: 1.0000\n",
      "Epoch 2/2\n",
      "8399/8399 [==============================] - 121s 14ms/step - loss: 0.1594 - acc: 0.9394 - precision: 0.1111 - recall: 1.0000 - val_loss: 0.2231 - val_acc: 0.9167 - val_precision: 0.1111 - val_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "'''\n",
    "x = GRU(units=128, activation='tanh', return_sequences=True)(embedded_sequences)\n",
    "\n",
    "x = LSTM(units=256, activation='tanh', return_sequences=False)(embedded_sequences)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#x = LSTM(units=128, activation='tanh', return_sequences=True)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "'''\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=9, activation='softmax')(x) #softmax\n",
    "\n",
    "# x = Dense(units=512, activation='relu')(x)\n",
    "# x = Dense(units=128, activation='relu')(x)\n",
    "# preds = Dense(units=25, activation='sigmoid')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 25s 412us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19112927243113517, 0.9295742606123288, 0.1111111119389534, 1.0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=500, verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-116e2c604a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "history_dict = history.history\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9686"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([s.argmax() for s in y_prob if s.max()>.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(x_val)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-25481f11954d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_classes_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_classes_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_prob = model.predict(x_train[:-2000])\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_train[:-2000].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33516666666666667"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.266944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.313711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.357811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.422023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.287897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.439866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.391452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.264276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.337474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.216379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.336559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.287897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.826867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.280626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.517110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.253568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.333495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.467348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.657671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.287897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.267330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.540488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59971</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.605781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59972</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.295723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59973</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.238934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59974</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.397029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59975</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.266141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59976</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.286032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59977</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.287897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59978</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.275897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59979</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.406962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59981</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.322466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59982</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.287844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59983</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.402449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59984</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59985</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59986</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.380037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59987</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.612744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59988</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.346275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59989</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.299897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59990</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.336333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59991</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.241241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59992</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.212497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59993</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.324581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59994</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.370932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.299296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.534488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.281838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.288593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.244781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  true      prob\n",
       "0         5     5  0.266944\n",
       "1         0     0  0.350441\n",
       "2         5     5  0.313711\n",
       "3         0     4  0.357811\n",
       "4         5     4  0.422023\n",
       "5         3     5  0.287897\n",
       "6         0     5  0.439866\n",
       "7         5     7  0.391452\n",
       "8         5     5  0.264276\n",
       "9         0     0  0.383985\n",
       "10        0     1  0.423134\n",
       "11        5     5  0.337474\n",
       "12        5     8  0.216379\n",
       "13        5     3  0.336559\n",
       "14        5     0  0.213091\n",
       "15        3     2  0.287897\n",
       "16        0     0  0.826867\n",
       "17        3     8  0.280626\n",
       "18        5     5  0.517110\n",
       "19        3     4  0.253568\n",
       "20        5     7  0.333495\n",
       "21        0     0  0.454535\n",
       "22        5     5  0.467348\n",
       "23        0     7  0.657671\n",
       "24        3     3  0.287897\n",
       "25        5     5  0.267330\n",
       "26        0     0  0.402537\n",
       "27        0     0  0.583564\n",
       "28        5     1  0.262408\n",
       "29        0     0  0.453298\n",
       "...     ...   ...       ...\n",
       "59970     0     0  0.540488\n",
       "59971     5     5  0.605781\n",
       "59972     5     3  0.295723\n",
       "59973     5     7  0.238934\n",
       "59974     5     8  0.397029\n",
       "59975     5     7  0.266141\n",
       "59976     3     5  0.286032\n",
       "59977     3     6  0.287897\n",
       "59978     5     7  0.275897\n",
       "59979     3     0  0.272551\n",
       "59980     0     0  0.406962\n",
       "59981     5     5  0.322466\n",
       "59982     5     5  0.287844\n",
       "59983     0     5  0.402449\n",
       "59984     0     0  0.427968\n",
       "59985     3     7  0.285558\n",
       "59986     5     5  0.380037\n",
       "59987     5     1  0.612744\n",
       "59988     5     0  0.346275\n",
       "59989     0     5  0.299897\n",
       "59990     5     5  0.336333\n",
       "59991     5     7  0.241241\n",
       "59992     5     0  0.212497\n",
       "59993     0     1  0.324581\n",
       "59994     5     4  0.370932\n",
       "59995     5     5  0.299296\n",
       "59996     5     7  0.534488\n",
       "59997     3     2  0.281838\n",
       "59998     0     5  0.288593\n",
       "59999     5     5  0.244781\n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_95=df_val[df_val.prob>.95]\n",
    "len(df_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_95[df_95.pred==df_95.true])/len(df_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20110"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val[df_val.pred==df_val.true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 3, 2, 7])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.pred.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229472"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14906.0</td>\n",
       "      <td>0.411876</td>\n",
       "      <td>0.130990</td>\n",
       "      <td>0.122192</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.396974</td>\n",
       "      <td>0.443390</td>\n",
       "      <td>1.265611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344.0</td>\n",
       "      <td>0.305225</td>\n",
       "      <td>0.186596</td>\n",
       "      <td>0.107255</td>\n",
       "      <td>0.114524</td>\n",
       "      <td>0.324234</td>\n",
       "      <td>0.424039</td>\n",
       "      <td>1.071373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12621.0</td>\n",
       "      <td>0.282041</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.182491</td>\n",
       "      <td>0.278944</td>\n",
       "      <td>0.286534</td>\n",
       "      <td>0.287893</td>\n",
       "      <td>0.537756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31841.0</td>\n",
       "      <td>0.326083</td>\n",
       "      <td>0.080582</td>\n",
       "      <td>0.137453</td>\n",
       "      <td>0.266784</td>\n",
       "      <td>0.301840</td>\n",
       "      <td>0.367833</td>\n",
       "      <td>0.775390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>288.0</td>\n",
       "      <td>0.307610</td>\n",
       "      <td>0.062912</td>\n",
       "      <td>0.100972</td>\n",
       "      <td>0.264594</td>\n",
       "      <td>0.299208</td>\n",
       "      <td>0.342576</td>\n",
       "      <td>0.498169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0     14906.0  0.411876  0.130990  0.122192  0.333200  0.396974  0.443390   \n",
       "2       344.0  0.305225  0.186596  0.107255  0.114524  0.324234  0.424039   \n",
       "3     12621.0  0.282041  0.010827  0.182491  0.278944  0.286534  0.287893   \n",
       "5     31841.0  0.326083  0.080582  0.137453  0.266784  0.301840  0.367833   \n",
       "7       288.0  0.307610  0.062912  0.100972  0.264594  0.299208  0.342576   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     1.265611  \n",
       "2     1.071373  \n",
       "3     0.537756  \n",
       "5     0.775390  \n",
       "7     0.498169  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49082</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.287850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43941</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.286016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58881</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.287897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44115</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.186938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37787</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58171</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.317108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.275014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47869</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9467</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.365315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58390</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.210359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29934</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.339086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20916</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.305196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19256</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.265777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43353</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.764275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28575</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.283366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.286105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56275</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.282286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  true      prob\n",
       "49082     3     5  0.287850\n",
       "43941     5     8  0.286016\n",
       "58881     3     3  0.287897\n",
       "44115     0     5  0.186938\n",
       "37787     5     5  0.269300\n",
       "58171     5     3  0.317108\n",
       "5517      3     3  0.275014\n",
       "47869     5     1  0.274429\n",
       "9467      5     6  0.365315\n",
       "58390     5     1  0.210359\n",
       "29934     5     7  0.339086\n",
       "1257      5     0  0.343527\n",
       "20916     5     5  0.305196\n",
       "19256     5     7  0.265777\n",
       "43353     5     1  0.278196\n",
       "2734      0     0  0.764275\n",
       "28575     3     7  0.283366\n",
       "2078      3     4  0.286105\n",
       "59027     0     0  0.460411\n",
       "56275     3     3  0.282286"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_95.pred.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
