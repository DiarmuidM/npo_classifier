\documentclass[11pt]{article}
\usepackage[style=apa]{biblatex}
\DeclareLanguageMapping{english}{english-apa} % Resolve labelyearlabelmonthlabelday errors.
\AtEveryBibitem{\clearfield{note}} % Clear note fields.
\usepackage{listings}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage[symbol]{footmisc}
\usepackage{graphicx}
\usepackage[capposition=top]{floatrow}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{bibentry}
\usepackage[letterpaper, left=1in,top=1in,right=1in,bottom=1in]{geometry}
\usepackage{setspace}
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{lineno}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{array}
\usepackage{color,soul}
\usepackage{tabularx}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage[group-separator={,}, group-minimum-digits=3]{siunitx}
\usepackage{appendix}
\usepackage{pdfpages}
\usepackage{titlesec}
\usepackage{mfirstuc}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{color,soul}
\usepackage{todonotes}
\usepackage{mathptmx} % almost times new roman font.
\usepackage{arydshln} % Dashed line.
\usepackage{enumitem} % enumerate line space.


\renewcommand{\baselinestretch}{1.5}
\renewcommand{\thefootnote}{\arabic{footnote}}
\bibliography{../reference/classification.bib}
% \bibliography{../reference/algorithms/ml_algorithms.bib}
\setcounter{tocdepth}{2}
\onehalfspacing

\usepackage{ragged2e}
\setlength{\RaggedRightParindent}{\parindent}

% 

\title{{\textsc{Classifying nonprofits using supervised machine-learning: Benchmark and guide for social scientists solving real-world problems}}}
\author{%
\textsc{Ji Ma and Isha Kanani} \thanks{J.M.: maji@austin.utexas.edu, LBJ School of Public Affairs and RGK Center for Philanthropy and Community Service; I.K.: ishakanani@utexas.edu, School of Information.} \\[1ex] % Your name \thanks{}
\normalsize University of Texas at Austin \\ % Your institution
% \normalsize {Email: maji@austin.utexas.edu} \\ % Your email address
}


\date{\today} % Leave empty to omit a date \today

%----------------------------------------------------------------------------------------

\begin{document}

\RaggedRight

\maketitle

\begin{abstract}

This research classified nonprofit organizations in the United States using supervised machine-learning algorithms according to text descriptions, built a workflow and benchmark to apply computational linguistics for future sociological studies, and can help researchers more accurately profile a civil society. We achieved 83\% overall accuracy for classifying the nonprofits into 10 broad categories, and 78\% for classifying them into 25 major groups. Our study suggests that machine-learning algorithms, and neural networks in particular, can substantially and reliably improve researchers' productivity because they can approximate or even outperform human coders on many categories. This leaves human researchers free to focus on the categories on which the algorithms have poor performance. We provided a clear strategy and operationalization method on how machine-learning classifiers can ``complement'' human researchers. A Python software package has been developed for scholars. Practical suggestions and future directions are also discussed. 

\end{abstract}
\clearpage

% \listoftodos
\tableofcontents
\listoftables
\listoffigures
\clearpage

\section{Introduction}

Although voluntary and philanthropic organizations have long existed in numerous centuries, the term ``nonprofit sector'' was only coined in the 1970s by scholars, policy makers, and nonprofit practitioners \parencite{HallHistoricalOverviewPhilanthropy2006}. A major reason for assembling the diverse organizations as a conceptual whole was to legitimize their existence and the benefits they receive \parencites[54-55]{HallHistoricalOverviewPhilanthropy2006}{BarmanClassificatoryStrugglesNonprofit2013}. From Durkheim's (\citeyear{DurkheimElementaryFormsReligious2012}) perspective, the order and structure of a society can be reflected by a classification system. The National Taxonomy of Exempt Entities (NTEE) developed by the National Center for Charitable Statistics (NCCS) is the most widely used classification system and represents one of the efforts put forth to legitimize the existence of the nonprofit sector \parencite{Hodgkinsonnewresearchplanning1991,HodgkinsonMappingnonprofitsector1990}. \textcite[105]{BarmanClassificatoryStrugglesNonprofit2013} cites the following observation from \textcite[601]{ClarkeSimpleTechnologyComplex1996}: ``The ways in which different entities (people, animals, plants, diseases, etc.) are organized into classificatory groups reveal something of the social, cultural, symbolic, and political contexts within which classifications occur.''

The development of NTEE classifications can be dated back to the 1980s \parencite[8-9, 11]{HodgkinsonMappingnonprofitsector1990}. In 1982, the NCCS assembled a team of experts who were working on creating a taxonomy for nonprofit organizations. The first draft of the NTEE came out in 1986 and was published in 1987. In the early 1990s, the NCCS had classified nearly one million nonprofits using the NTEE. In 1995, the Internal Revenue Service (IRS) adopted the NTEE coding system, took over the task of assigning and maintaining the classifications, and started to release the Business Master File with NTEE codes \parencite{USInternalRevenueServiceExemptOrganizationsBusiness2014,USInternalRevenueServiceIRSStaticFiles2013}.

Two agencies were responsible for assigning the NTEE codes: the NCCS and the IRS. Before 1995, the NCCS coded nonprofits according to the program descriptions in Parts III and VIII of Form 990, which were supplemented with information from Form 1023 (``Application for Recognition of Exemption'') and additional research \parencite[16]{NationalCenterforCharitableStatisticsGuideUsingNCCS2006}. After 1995, the IRS began to issue ``new exempt organizations an NTEE code as part of the determination process,'' and ``the determination specialist [assigned] an NTEE code to each organization exempt under I.R.C. \S 501(a) as part of the process of closing a case when the organization [was] recognized as tax-exempt'' \parencite[1]{USInternalRevenueServiceIRSStaticFiles2013}.

The NTEE classification system has supported many applied and academic studies on nonprofit organizations that have critical economic and political roles in society. For example, the NTEE provides a framework through which the social and economic activities of civil society can be mapped and compared with other sectors in the society \parencite[e.g.,][]{RoegerNonprofitSectorIts2015}. Scholars can use NTEE codes to sample nonprofits of interests \parencite[e.g.,][]{OktenDeterminantsdonationsprivate2000,SharkeyCommunityCrimeDecline2017,McVeighStructuralInfluencesActivism2006,VasiNoFrackingWay2015} or as independent variables \parencite{SloanEffectsNonprofitAccountability2009}. The NTEE can also serve as an analytical tool to measure organizational capacity in different service domains and inform practitioners and policy makers' decision making \parencite{Hodgkinsonnewresearchplanning1991}. Moreover, the NTEE is a fundamental necessity for comparative international research and facilitates the study of ``global civil society'' \parencite{VakilConfrontingclassificationproblem1997,Salamonsearchnonprofitsector1992,Salamoninternationalclassificationnonprofit1996,HodgkinsonMappingnonprofitsector1990}.

But the NTEE classification system, although one of the best we have so far, still has several drawbacks. First, because the NTEE only assigns one major category code to an organization, it cannot accurately describe a nonprofit's programs which are usually diverse and spread across several service domains \parencite[303]{GronbjergUsingNTEEclassify1994}. Even though a program classification system was later developed \parencite{LampkinIntroducingNonprofitProgram2001}, it is not widely used, probably because it is impractical to assign codes to a massive number of programs.

Second, the assignment of NTEE codes is not complete because it is ``based on an assessment of program descriptions contained in Parts 3 and 8 of the Form 990'' and ``program descriptions were only available for some organizations'' \parencite[16]{NationalCenterforCharitableStatisticsGuideUsingNCCS2006}. A recent study found the number of organizations in Washington state with a specific NTEE code could be significantly increased if mission statements were used for coding \parencite{FyallNTEECodesOpportunities2018}.

Third, NTEE codes are static while nonprofit organizations' activities may change over time. Recoding existent NTEE assignments is extremely onerous, and this may be one of the reasons that the IRS does not have a procedure through which nonprofits can request a change to their NTEE codes \parencite{USInternalRevenueServiceIRSStaticFiles2013}.

Fourth, a vast amount of grassroots organizations are not classified and remain missing in existing datasets because an organization ``that normally has annual gross receipts of \$50,000 or less'' is not required to report to the IRS \parencite{USInternalRevenueServiceAnnualExemptOrganization2019}. As \textcite{SmithRestNonprofitSector1997} estimates, the IRS listings ignore about 90\% of nonprofits, most of which are grassroots associations. Organizational activities at the grassroots level are particularly important for sociological and political studies, and most studies fail to consider these grassroots organizations because of the dataset limitation \parencite[e.g.,][]{McVeighStructuralInfluencesActivism2006,VasiNoFrackingWay2015,SharkeyCommunityCrimeDecline2017}.

The tremendous amount of human labor needed for classification is a prominent challenge and is also an evident barrier to improving any classification system. Numerous social scientists have experimented with applying computational methods to classify objects and solve real-world problems \parencite[e.g.,][]{BacakPrincipledMachineLearning2018,NelsonFutureCodingComparison2018,FyallNTEECodesOpportunities2018,GrimmerTextDataPromise2013}. We respond to this challenge by applying the advances made in computational linguistics and contribute to the growing literature from four aspects: 1) we established a standardized workflow and benchmarks that future studies of nonprofits or typologies in other social science disciplines can build on and make comparisons to, 2) we achieved {83\%} overall accuracy for classifying the nonprofits into 10 broad categories and {78\%} for classifying them into 25 major groups, 3) we developed a Python software package for scholars to classify text descriptions using NTEE codes, and 4) we released all source codes, data, and work history for replication purposes and future studies. This last point is particularly important since there are many caveats in tuning algorithms that cannot be detailed in this paper.\footnote{Follow this link for the complete working directory with detailed instructions: https://github.com/***}


\section{Method}

Classifying texts is a typical task in automatic content analysis and usually employs three types of methods: the dictionary, supervised, and unsupervised methods \parencite[268-269]{GrimmerTextDataPromise2013}. The dictionary method uses a predefined dictionary of words to classify the texts. Although accurate, this approach is not capable of dealing with the variations in and contexts of language. The supervised method is an improved solution that uses computer algorithms to ``learn'' the linguistic patterns in a dataset classified by human coders. Unlike the dictionary and supervised methods, which require predefined categories of interest, the unsupervised method can discover linguistic patterns in texts without inputting any knowledge for classification. However, the unsupervised method's validity can be problematic because the returned classifications may not be theoretically and practically meaningful. To take advantage of existing human-coded NTEE classifications and the experiences documented in past studies \parencite{NelsonFutureCodingComparison2018,FyallNTEECodesOpportunities2018}, this study employs a supervised approach as Figure \ref{fig:workflow} illustrates.

\begin{figure}
	\centering
	\caption{\textsc{Research workflow}} \label{fig:workflow}
	\includegraphics[width=0.7\textwidth]{tbl_fig/ntee_classification.pdf}
\end{figure}

Figure \ref{fig:workflow} presents this paper's complete workflow. We implement four stages of analysis: 1) the \textit{preprocessing stage} includes data acquisition and the preprocessing of datasets and texts; 2) \textit{feature extraction} includes a bag-of-words representation (used by na\"ive Bayes and random forest algorithms) and word embedding (used by neural network algorithms); 3) the \textit{training and intermediate decision-making} phase, is where we use stochastic and grid search to train, search, and optimize the machine-learning algorithms; and 4) the last phase involves \textit{training the model finalist} with the complete dataset and preparing the trained model for public use. The rest of this section introduces the four phases in detail.

\subsection{Data preprocessing}

\begin{table}[]
    \centering
    \begin{tabularx}{0.8\textwidth}{r|X|X}
    	 \hline
         & Mission Statement & Program Description \\
         \hline
         990 & Part I, Line 1; Part III, Line 1 & Part III, Line 4; Part VIII, Lines 2a-e, Lines 11a-c; Schedule O \\
         \hdashline
         990-EZ & Part III & Part III, Lines 28-30; Schedule O \\
         \hdashline
         990-PF & -- & Part IX-A; Part XVI-B \\
         \hline
    \end{tabularx}
    \caption{\textsc{Locations of text fields in different forms}} \label{tab:text_loc}
\end{table}

\textit{Data acquisition and dataset preprocessing.} We collected text records from Forms 990, 990-EZ, and 990-PF and supplemented these records with program descriptions from Schedule O. Form 990 (``Return of Organization Exempt From Income Tax'') is submitted by most nonprofit organizations. Smaller organizations with ``gross receipts of less than \$200,000 and total assets of less than \$500,000 at the end of their tax year'' \parencite[1]{USInternalRevenueService2017InstructionsForm2018} can file Form 990-EZ (``Short Form Return of Organization Exempt From Income Tax''), a shorter version of Form 990. Private foundations use Form 990-PF (``Return of Private Foundation''). The texts describe organizational activities in two forms: the overall mission statement and specific program descriptions. Table \ref{tab:text_loc} summarizes these text fields' specific locations on the different forms.

Classification records (i.e., NTEE codes) were collected from the 2014--2016 Business Master Files on the NCCS website.\footnote{https://nccs-data.urban.org} This study deals with two types of NTEE classifications: 10 broad categories and 26 major groups. Table \ref{tab:classification} shows the relationship between the broad categories and major groups. A detailed list of the 26 major groups can be found through the \textcite{USInternalRevenueServiceExemptOrganizationsBusiness2014}. The accuracy of a classification is indicated by the letters of A, B, and C, where a ``confidence level of A ... indicates that there is at least a 90 percent probability that the major group classification is correct'' \parencite[16]{NationalCenterforCharitableStatisticsGuideUsingNCCS2006}. The intercoder reliability of records at confidence level A should approximate 100\% \parencite[147]{StengelGettingItRight1998}--this measure is particularly important because the NTEE codes were assigned by human coders from different organizations (i.e., the IRS and NCCS) over different periods of time.

From 2014 to 2016, 56.12\% of records were classified at level A, 37.32\% at level B, and 6.56\% at level C. For training purposes, we only used records at confidence level A and dropped all records in the \textit{X/Z} category (i.e., unknown or unclassified). About 1.76\% of organizations changed their NTEE codes between 2014 and 2016. We dropped the records of these organizations as well since these records are less credible.

\begin{table}[]
    \centering
    \begin{tabularx}{0.9\textwidth}{r|X|X}
    	 \hline
         Broad Category Code & Explanation & Major Group Code \\
         \hline
		I & Arts, Culture, and Humanities & A \\
		II & Education & B \\
		III & Environment and Animals & C, D \\
		IV & Health & E, F, G, H \\
		V & Human Services & I, J, K, L, M, N, O, P \\
		VI & International, Foreign Affairs & Q \\
		VII & Public, Societal Benefit & R, S, T, U, V, W \\
		VIII & Religion Related & X \\
		IX & Mutual/Membership Benefit & Y \\
		X & Unknown, Unclassified & Z \\
         \hline
    \end{tabularx}
    \caption{\textsc{NTEE-CC classification system}} \label{tab:classification}
\end{table}


\textit{Text Preprocessing.} Texts in sentences need to be ``tokenized'' into words before analysis, which is called ``tokenization'' in natural language processing. We also removed stop words (e.g., ``the,'' ``a,'' and punctuation marks) and checked spelling errors using algorithms based on ``minimum edit distance'' \parencite[i.e., the minimum number of editing operations needed to change one word into another;][26]{JurafskySpeechLanguageProcessing2017}.

\textit{Universal Classification Files (UCFs).} The final step in the data preprocessing stage is to divide data records into training and testing datasets (i.e., files in \texttt{/dataset/UCF/}) that are mutually exclusive and can be used to benchmark future models. The \textit{Universal Classification File Training} (UCF-Training; \texttt{df\_ucf\_train.pkl.gz}) is used to develop models and comprises 80\% of the total records. The \textit{Universal Classification File Testing} (UCF-Testing; \texttt{df\_ucf\_test.pkl.gz}) is used to test a trained model's performance and comprises 20\% of the total records. Table \ref{tab:universal_file} presents the two datasets' composition by major groups. The UCFs approximate the composition of organizations reported to the IRS except for groups \textit{A} (``arts, culture, and humanities'') and \textit{T} (``philanthropy, voluntarism, and grantmaking foundations''). The consequence is that, compared to human coders, the trained models are less likely to categorize organizations as \textit{T} and more likely to categorize organizations as \textit{A}.

\begin{table}[t]
    \centering
    \begin{threeparttable}
    \caption{\textsc{Composition of Universal Classification Files}} \label{tab:universal_file}
    \begin{tabularx}{\textwidth}{c|r|r|r|r|r|r}
    	 \hline
         Major Group & Training (\#) & Training (\%) & Testing (\#) & Testing (\%) & Reported (\#) & Reported (\%) \\
         \hline
			A & \num{17010} & \num{11.02}\% & \num{4291} & \num{11.11}\% & \num{35813} & \num{6.77}\% \\
			\hdashline
			B & \num{25827} & \num{16.72}\% & \num{6419} & \num{16.63}\% & \num{67879} & \num{12.83}\% \\
			\hdashline
			C & \num{3323} & \num{2.15}\% & \num{827} & \num{2.14}\% & \num{9054} & \num{1.71}\% \\
			D & \num{4239} & \num{2.75}\% & \num{1034} & \num{2.68}\% & \num{8740} & \num{1.65}\% \\
			\hdashline
			E & \num{9015} & \num{5.84}\% & \num{2307} & \num{5.98}\% & \num{25643} & \num{4.85}\% \\
			F & \num{2301} & \num{1.49}\% & \num{543} & \num{1.41}\% & \num{8481} & \num{1.60}\% \\
			G & \num{5053} & \num{3.27}\% & \num{1353} & \num{3.50}\% & \num{10697} & \num{2.02}\% \\
			H & \num{467} & \num{0.30}\% & \num{126} & \num{0.33}\% & \num{2203} & \num{0.42}\% \\
			\hdashline
			I & \num{2947} & \num{1.91}\% & \num{740} & \num{1.92}\% & \num{8687} & \num{1.64}\% \\
			J & \num{4772} & \num{3.09}\% & \num{1132} & \num{2.93}\% & \num{15841} & \num{2.99}\% \\
			K & \num{2009} & \num{1.30}\% & \num{522} & \num{1.35}\% & \num{7444} & \num{1.41}\% \\
			L & \num{5942} & \num{3.85}\% & \num{1537} & \num{3.98}\% & \num{20428} & \num{3.86}\% \\
			M & \num{4693} & \num{3.04}\% & \num{1140} & \num{2.95}\% & \num{10857} & \num{2.05}\% \\
			N & \num{15460} & \num{10.01}\% & \num{3925} & \num{10.17}\% & \num{43987} & \num{8.31}\% \\
			O & \num{1731} & \num{1.12}\% & \num{409} & \num{1.06}\% & \num{7878} & \num{1.49}\% \\
			P & \num{9180} & \num{5.94}\% & \num{2318} & \num{6.00}\% & \num{40880} & \num{7.73}\% \\
			\hdashline
			Q & \num{1987} & \num{1.29}\% & \num{436} & \num{1.13}\% & \num{7288} & \num{1.38}\% \\
			\hdashline
			R & \num{1064} & \num{0.69}\% & \num{257} & \num{0.67}\% & \num{2830} & \num{0.53}\% \\
			S & \num{14459} & \num{9.36}\% & \num{3603} & \num{9.33}\% & \num{48387} & \num{9.14}\% \\
			T & \num{2032} & \num{1.32}\% & \num{541} & \num{1.40}\% & \num{84338} & \num{15.94}\% \\
			U & \num{1000} & \num{0.65}\% & \num{225} & \num{0.58}\% & \num{3039} & \num{0.57}\% \\
			V & \num{350} & \num{0.23}\% & \num{85} & \num{0.22}\% & \num{940} & \num{0.18}\% \\
			W & \num{8357} & \num{5.41}\% & \num{2038} & \num{5.28}\% & \num{20862} & \num{3.94}\% \\
			\hdashline
			X & \num{4566} & \num{2.96}\% & \num{1098} & \num{2.84}\% & \num{20699} & \num{3.91}\% \\
			\hdashline
			Y & \num{6640} & \num{4.30}\% & \num{1701} & \num{4.41}\% & \num{15712} & \num{2.97}\% \\
			\hdashline
			Z & -- & -- & -- & -- & \num{547} & \num{0.10}\% \\
			\hline
			Total & \num{154424} & \num{100.00}\% & \num{38607} & \num{100.00}\% & \num{529154} & \num{100.00}\% \\
         \hline
    \end{tabularx}
    \begin{tablenotes}
    \item\footnotesize \textit{Note}: Numbers and percentages reported to the Internal Revenue Service (i.e., the last two columns) are from \textcite{McKeeverNonprofitAlmanacEssential2016}. Dashed lines separate the 10 broad categories.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\subsection{Word representation and feature extraction}

The machine-learning algorithms can only work on numeric vectors that are transformed from the tokenized sentences. A variety of transformation methods can ``represent'' words as vectors, and good methods should be able to ease the process of extracting ``features'' from texts. In general, there are two approaches to word representation: bag-of-words and word embedding.

\subsubsection{Bag-of-words approach}

The bag-of-words approach considers words in texts as being mutually independent and thus disregards the order of the words. For example, ``we are health service organization'' and ``health organization service are we'' are the same from a bag-of-words perspective. This method serves as the basis for developing many simple language models because it can efficiently represent the possibility of a word's occurrence in texts \parencite{BengfortAppliedTextAnalysis2018}. We adopted two methods in this study to represent the texts: count vector and term frequency-inverse document frequency.

\textit{Count vector} counts the number of occurrences of all the words in a given text. Given a set of statements, the algorithm first builds an index of all unique words from the collection that is called the vocabulary index. The algorithm then represent the texts using word frequencies and the vocabulary index. Table \ref{tab:count_vector} presents a simple example of count vectors in which the statement ``we focus on education'' is represented as the vector $[1, 1, 1, 0, 0, 0, 0]$

\begin{table}
\caption{\textsc{Example of Count Vectors}} \label{tab:count_vector}
\begin{tabular}{m{3.9cm}|m{0.9cm}|m{1cm}|m{0.9cm}|m{1.5cm}|m{1cm}|m{1cm}|m{0.8cm}}
    \hline
    statements X vocabulary & we & focus & on & education & health & care & about \\
    \hline
    we focus on education & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\ 
    \hdashline
    health care care  & 0 & 0 & 0 & 0 & 1 & 2 & 0 \\ 
    \hdashline
    we care about & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\ 
    \hline
\end{tabular}
\end{table}

\textit{Term frequency-inverse document frequency} (TF-IDF) normalizes raw word frequencies using the number of documents in which a given word appears. As Eq. \ref{eq:tfidf} presents, $tf_{ij}$ is the frequency of word $i$ in mission statement $j$, weighted by the inverse document frequency (i.e., $idf_i$; Eq. \ref{eq:idf}), where $N^{total}$ is the number of total mission statements and $N^{i}$ is the number of mission statements in which word $i$ appears. The underlying assumption of TF-IDF is that any words appearing in all the statements are not as important as those occurring in a limited number of statements \parencite[278]{JurafskySpeechLanguageProcessing2017}.

\begin{equation} \label{eq:tfidf}
 w_{ij} = tf_{ij} \cdot idf_i 
\end{equation}
\begin{equation} \label{eq:idf}
idf_i = log(\frac{N^{total}}{N^{i}})
\end{equation}

We need to ``normalize'' the texts to reduce the vocabulary size before transforming using either count vector or TF-IDF because the same word can have numerous spelling variations. For example, ``environments,'' ``environmental,'' and ``environment'' represent the same root word (i.e., \textit{stem}) ``environ.'' Otherwise, the machine-learning models will suffer from ``the curse of dimensionality'': as the feature increases, the data becomes more discrete and less informative to decision making \parencite[94]{BellmanAdaptiveControlProcesses2015}. 

The process of finding stems is called ``morphological parsing,'' which includes two primary methods: \textit{stemming} and \textit{lemmatizing} \parencite[25]{JurafskySpeechLanguageProcessing2017}. Stemming slices longer strings into smaller ones according to a series of predefined rules. For example, ``ational'' is transformed to ``ate'' in all words ending with the former string. Therefore, stemming tends to have both over- and under-parsing errors. Lemmatizing is a more advanced method that reduces a word to its stem by analyzing its meaning.

\subsubsection{Word embedding approach}

Disregarding the contexts in which the words appear is an evident drawback of the bag-of-words approach. The word embedding approach is a new advancement \parencite{MikolovEfficientEstimationWord2013} and was suggested by \textcite[28]{NelsonFutureCodingComparison2018} as a future direction for sociological studies. It represents words in a multidimensional space (i.e., each word has a vector value), and in this space words that often appear together in texts are closer in distance to each other \parencites[][290]{JurafskySpeechLanguageProcessing2017}[][65]{BengfortAppliedTextAnalysis2018}. We can either train our own word vectors, which would require a large corpus and is time-consuming, or use pretrained word vectors. In this study, we use 100-dimension word vectors pretrained from a corpus of 6 billion word tokens \parencite{PenningtonGloveGlobalVectors2014}.

\subsection{Training and intermediate decision making}

\subsubsection{Imbalanced dataset resampling}

Training using an imbalanced dataset such as UCF-Training can bias our prediction of minor classes because machine-learning algorithms cannot extract enough information from these classes (e.g., groups \textit{H} and \textit{V}). Therefore, resampling the imbalanced dataset to build a more balanced one is crucial for predicting minority classes. We experimented with three strategies of over-sampling (i.e., ADASYN, RandomOverSampler, and SMOTE) and two strategies of over-sampling followed by under-sampling to reduce the noise \parencite[i.e., SMOTEENN and SMOTETomek;][]{LemaitreImbalancedlearnPythonToolbox2017}. The influence of resampling is substantial: the $F_1$ score of predicting minority class major group \textit{Q} was improved from 15\% to over 30\% in our pilot experiments.\footnote{Although major group \textit{Q} and broad category {VI} represent the same group of organizations, for computer algorithms, the classification contexts are different; therefore, performance on this category varies.}

\subsubsection{Classifiers for training}

The \textit{na\"ive Bayes (NB) classifier} is built on Bayes' theorem. It is one of the simplest classifiers to learn and implement among all machine-learning algorithms and is built on simple conditional probability principles. The classifier assumes all features extracted from the texts are conditionally independent, which is wrong in most cases. But the classifier is efficient and has proven to be useful for a variety of tasks even on a small dataset \parencites[][76]{JurafskySpeechLanguageProcessing2017}[][277]{GrimmerTextDataPromise2013}. We tested two types of NB classifiers: the multinomial and complement NB classifiers \parencite{RennieTacklingPoorAssumptions2003}.

The \textit{random forest (RF) classifier} is implemented by developing multiple prediction models. Each model in this algorithm is trained by different data, and then all of these models are asked to make a prediction for the same record. A prediction class that is elected by most of these small algorithms is given as the prediction result by the RF algorithm. It uses the word ``forest'' because each small algorithm trained is a decision tree \parencites[83]{QuinlanInductiondecisiontrees1986}. A decision tree represents a set of questions that usually have yes/no answers. The process starts from the top of the tree with one question, and based on the answer, we run down either side of the tree and answer another question. We repeat this process until reaching the end of the tree. Each decision tree is trained on a different training set \parencites[124]{BreimanBaggingpredictors1996}. 

Take our study for example. If we provide 5,000 statements with their NTEE codes to an RF with 9 trees (i.e., each tree corresponds to a broad category code), each tree will randomly select a thousand records to train. Each tree includes new words at different levels of the tree. For example, a tree starting with the word ``emergency'' will have a branch for ``yes'' and ``no'' that leads to another word and so on--all the way to the bottom of the tree where the NTEE code is. Since each tree is trained on a different set, when a record is given for prediction, each tree predicts the class independent of the other trees. A total of 9 class predictions were collected in this case, and the class with the highest occurrence in the prediction results was given as the final predicted class by the RF algorithm. 

Since each decision tree in an RF classifier is supplied a unique set of records for training purposes, the performance of the overall forest is stronger. The classifier however is difficult to visually interpret. Unlike the Na\"ive Bayes approach, it takes some efforts to visualize how decision trees work and understand the algorithm.

\textit{Neural network (NN) classification} is built on the structure of a neuron in the human mind. Each neuron in the network is connected to a few other neurons of the network by a numerical value called ``weight.'' Each neuron processes records each one in turn, and learn by looking at their classification (i.e., NTEE code in this case) with the known previous NTEE codes of records. With every new record the neurons learn, they update the connection value ``weight'' to update the model \parencites[163]{CollobertUnifiedArchitectureNatural2008}. After the network is done processing each record of the training set, it has final weights for each connection between two neurons. When a testing set is provided, the neurons use the final weights to predict the NTEE code. Depending on the architecture of the neurons, we can design a variety of NNs (e.g., the basic fully connected, recurrent, or long short-term memory). This study uses convolutional NN (CNN) following other scholars' recommendation \parencite{ZhangSensitivityAnalysisPractitioners2015}.

\subsubsection{Measuring algorithm performance}

An algorithm's performance can be measured by many metrics, but social scientists particularly concern two questions while solving real-word problems: 1) how many predicted observations are correct (i.e., \textit{precision} calculated by Eq. \ref{precision})? 2) how many observations are correctly predicted (i.e., \textit{recall} calculated by Eq. \ref{recall}). Answering the two questions is critical for social scientists to apply ML research methods, and we will analyze the methodological implications and recommended practices in discussion section.

In Eq. \ref{precision}, $k$ is one of the NTEE codes, $\#Org^{corr}_{k}$ is the number of organizations correctly classified as $k$, and ${\#Org^{pred}_{k}}$ is the number of organizations predicted as $k$. $\#Org^{corr}_{k}$ will always be smaller than or equal to ${\#Org^{pred}_{k}}$ because ML algorithms can hardly predict every observation right. For example, $Precision_{B}=0.75$ indicates that 75\% of all the organizations classified as ``education'' are correct.

\begin{equation} \label{precision}
    Precision_{k}=\frac{Org^{corr}_{k}}{Org^{pred}_{k}}
\end{equation}

Assuming robust human-coders' coding of nonprofits as true value, in Eq. \ref{recall}, $Org^{true}_{k}$ is the number of organizations that belong to $k$ category. For example, $Recall_{B}=0.80$ denotes that 80\% of the organizations classified as ``education'' by robust human-coding are correctly identified by the algorithm.

\begin{equation} \label{recall}
    Recall_{k}=\frac{Org^{corr}_{k}}{Org^{true}_{k}}
\end{equation}

$F_1$ score (Eq. \ref{eq:f1}), the harmonic mean of precision and recall, was introduced to balance the two measures.

\begin{equation} \label{eq:f1}
    F_{1k}=\frac{2 \cdot Precision_{k} \cdot Recall_{k}}{Precision_{k} + Recall_{k}}
\end{equation}


\subsubsection{Intermediate decision making}

The goal of this study is to find the best ML algorithm with appropriate parameters. We can either try some of the configurations randomly (i.e., \textit{stochastic search}), or iterate all possible configurations (i.e., \textit{grid search}). For NB and RF algorithms, we used the latter approach. For NN algorithms, we first used stochastic search to narrow down the configurations of hidden layers, and then conducted a grid search for the input and output layers' parameters using CNN. The grid search for all possible parameters (over 2 million combinations) is impossible even by using one of the most advanced super computing clusters in the world.

We conduced two rounds of grid search. The first found is for \textit{satisficing decision making} in which we only considered the configurations that can perform at the top 5 percent (240 parameter combinations for NB and RF, 7,200 for NN, detail history files are in folder \texttt{output}). Then we ran the second found grid search for \textit{optimizing decision making} in which we increased the values of some parameters to allow the algorithms to reach their performance ceilings. We then choose the best algorithm and parameters for final training.


\section{Results}


\subsection{Selecting the best classifier}

For multi-class classification task (i.e., more than two classes to predict), it is difficult to measure the overall performance because for each category the performance differs. Table \ref{tab:perf_compare} presents the performance of CNN classifiers with and without resampling. Because the dataset is imbalanced, the classifier has a poor performance on category \textit{VI International, Foreign Affairs} without resampling. Training the classifier with resampled dataset substantially improves the $F_1$ score from 14\% to 29\%, but slightly sacrifices the performance on other categories. So which one should we choose?

We choose the classifier trained without resampling as the best model because even the $F_1$ score of \textit{VI} is substantially improved, we still cannot use the predicted results of this category (21\% identified among which only 44\% are correct). We recommend not sacrificing the performance on other categories as researchers need to manually check or completely drop this category in their analysis anyway. For social scientists, mathematical improvements may not make substantial and practical meaning. This rationale applies to selecting other classifiers.


\begin{table}
\centering
\begin{threeparttable}
    \caption{\textsc{Comparing Convolutional Neural Network classifiers}} \label{tab:perf_compare}
    \begin{tabular}{r|r|r|r|r|r|r|r}
		\hline
			Code & Precision-\textit{N} & Precision-\textit{R} & Recall-\textit{N} & Recall-\textit{R} & $F_1$-\textit{N} & $F_1$-\textit{R} & \%Obs.\\
		\hline
			I & 87\% & 83\% & 85\% & 87\% & 86\% & 85\% & 11\% \\
			II & 85\% & 91\% & 88\% & 78\% & 86\% & 84\% & 17\% \\
			III & 76\% & 83\% & 90\% & 82\% & 82\% & 82\% & 5\% \\
			IV & 76\% & 88\% & 87\% & 70\% & 81\% & 78\% & 11\% \\
			V & 85\% & 77\% & 86\% & 90\% & 85\% & 83\% & 30\% \\
			VI & 59\% & 44\% & 8\% & 21\% & 14\% & 29\% & 1\% \\
			VII & 88\% & 83\% & 76\% & 79\% & 81\% & 81\% & 17\% \\
			VIII & 65\% & 71\% & 77\% & 70\% & 71\% & 71\% & 3\% \\
			IX & 90\% & 80\% & 85\% & 92\% & 88\% & 85\% & 4\% \\
    	 \hline
    \end{tabular}
\begin{tablenotes}
\footnotesize
\item \emph{Note}: \textit{N} = No resampling; \textit{R} = Resampling.
\end{tablenotes}
\end{threeparttable}
\end{table}



\subsection{Performance of best model}

In general, CNN classifier achieves the best performance. For classifying the 10 broad categories, 83.47\% records in the UCF-Testing dataset are correctly recognized; for the 25 major groups task, 77\% are correctly classified. Detail satisficing decision table can be found in source code repository published online (\texttt{/output} folder). The precision and recall for each category or group varies as Table \ref{tab:perf_bc} and Table \ref{tab:perf_mg} present. 

Our CNN classier approximates or even outperforms human coders on many broad categories (i.e., \textit{V}, \textit{VI}, \textit{VIII}) and major groups (i.e., \textit{A}, \textit{G}, \textit{J}, \textit{N}, \textit{S}, and \textit{W}). For example, the classifier outperforms human coders on broad category \textit{VII Public, Societal Benefit}: 76\% \textit{VII} organizations are identified, and among these identified organizations, 88\% are correct -- nearly 12\% higher than human coders' performance. For major group \textit{W Public, Society Benefit}: 86\% \textit{W} organizations are identified, and among these identified organizations, 87\% are correct -- 29\% higher than human coders' performance.

\begin{table}
\centering
\begin{threeparttable}
    \caption{\textsc{Performance of best model on broad category}} \label{tab:perf_bc}
    \begin{tabular}{r|r|r|r|r}
		\hline
			Code & H-Precision & Precision & Recall & $F_1$ \\
		\hline
			I & 88\% & 87\% & 85\% & 86\% \\
			II & 93\% & 85\% & 88\% & 86\% \\
			III & 87\% & 76\% & 90\% & 82\% \\
			IV & 92\% & 76\% & 87\% & 81\% \\
			V & 86\% & 85\% & 86\% & 85\% \\
			VI & 77\% & 59\% & 8\% & 14\% \\
			VII & 76\% & 88\% & 76\% & 81\% \\
			VIII & 87\% & 65\% & 77\% & 71\% \\
			IX & 90\% & 90\% & 85\% & 88\% \\
    	 \hline
    \end{tabular}
\begin{tablenotes}
\footnotesize
\item \emph{Notes}: H-Precision = Human Coder Precision, compiled from \textcite[153]{StengelGettingItRight1998}.
\end{tablenotes}
\end{threeparttable}
\end{table}


\begin{table}
\centering
\begin{threeparttable}
    \caption{\textsc{Performance of best model on major group}} \label{tab:perf_mg}
    \begin{tabular}{r|r|r|r|r}
		\hline
			Code & H-Precision & Precision & Recall & $F_1$ \\
		\hline
			A & 88\% & 80\% & 87\% & 83\% \\
			\hdashline
			B & 93\% & 85\% & 85\% & 85\% \\
			\hdashline
			C & 86\% & 65\% & 74\% & 69\% \\
			D & 90\% & 80\% & 90\% & 85\% \\
			\hdashline
			E & 92\% & 77\% & 78\% & 78\% \\
			F & 86\% & 51\% & 60\% & 55\% \\
			G & 65\% & 68\% & 68\% & 68\% \\
			H & 73\% & 55\% & 19\% & 28\% \\
			\hdashline
			I & 84\% & 71\% & 71\% & 71\% \\
			J & 72\% & 86\% & 67\% & 75\% \\
			K & 82\% & 63\% & 68\% & 66\% \\
			L & 83\% & 70\% & 76\% & 73\% \\
			M & 88\% & 87\% & 90\% & 88\% \\
			N & 88\% & 83\% & 93\% & 88\% \\
			O & 91\% & 65\% & 61\% & 63\% \\
			P & 88\% & 64\% & 57\% & 60\% \\
			\hdashline
			Q & 77\% & 43\% & 36\% & 39\% \\
			\hdashline
			R & 67\% & 46\% & 21\% & 28\% \\
			S & 75\% & 84\% & 79\% & 81\% \\
			T & 78\% & 66\% & 32\% & 43\% \\
			U & 76\% & 52\% & 22\% & 31\% \\
			V & 24\% & 0\% & 0\% & 0\% \\
			W & 58\% & 87\% & 86\% & 86\% \\
			\hdashline
			X & 87\% & 68\% & 71\% & 70\% \\
			\hdashline
			Y & 90\% & 84\% & 91\% & 88\% \\
			\hdashline
			Z & 10\% & -- & -- & -- \\
         \hline
    \end{tabular}
\begin{tablenotes}
\footnotesize
\item \emph{Note}: H-Precision = Human Coder Precision, compiled from \textcite[153]{StengelGettingItRight1998}. Dashed lines separate the ten broad categories.
\end{tablenotes}
\end{threeparttable}
\end{table}


\subsection{Python package for classifying texts}

We developed a Python package (\texttt{npoclass}) for classifying texts using NTEE codes, and scholars can use it free of charge.\footnote{https://github.com/****} Although the package can work on any texts, we expect it should perform best on nonprofit organization-related narratives. \texttt{npoclass} has the following features, and detail instructions are in the package's documentation:

\begin{enumerate}[nosep]
	\item Take a single string text or a list of of text descriptions as input;
	\item Return predicted codes of both broad category and major group;
	\item Return probability on each code of broad category and major group;
	\item Parallelize predicting process on multi-core computers.
\end{enumerate}


\section{Discussion}

We achieved 83\% overall accuracy for classifying the nonprofits into 10 broad categories according to their text descriptions, and 78\% for classifying them into 25 major groups. We detailed the caveats and built a workflow and benchmark of applying computational linguistics for future sociological studies. 

An encouraging conclusion of this study is, the ML algorithms, neural networks in particular, can substantially and reliably improve researchers' productivity as they can approximate or even outperform human coders on many categories. Therefore, human researchers can focusing on the categories on which the algorithms have poor performance. We provided a clear strategy and operationalization method on how machine-learning classifiers can ``complement'' human researchers \parencite[25]{NelsonFutureCodingComparison2018}.

This study also enables social scientists to examine social, political, and economic activities at grassroots level. As discussed in the introduction section, NTEE as a typology at organizational level is not accurate because one organization can have multiple programs, and the classification of programs has never been done before. This study can help researchers to code nonprofits' activities at program level, profiling a more accurate civil society.

Some practical suggestions to social scientists solving real-world problems. The results of performance in this paper indicates that, for social scientists who want to apply computational methods in their research should be ``cautiously confident.'' The key here supporting our confidence is a robust validation \parencite[271]{GrimmerTextDataPromise2013}. Otherwise, it will be ``garbage in, garbage out.'' Many factors can influence the validity of the algorithm. For example, the algorithm may have a poor performance on a datset that is structurally different from the training dataset. We strongly suggest that readers should check the annotations in our scripts posted online to understand the caveats and make necessary optimizations according to their own research question.

Social scientists should also take the advantage of high performance computing (HPC) research infrastructures \parencite[e.g.,][]{KeaheyChameleonScalableProduction2018}. The ML algorithms can achieve best performance only when trained with a large amount of data, and such training process consumes a huge amount of computing resources which is far beyond the capacity of the most advanced personal computers. At the grid search phase of this study, we used two most advanced GPU accelerators (NVIDIA Tesla P100) for NN training and six 48-CPU computing servers for NB and RF training. HPC infrastructures are widely used in natural sciences but it is still new to social scientists. We encourage methodology workshops to incorporate the introduction of HPC infrastructures as part of their syllabus.

Future studies can make numerous improvements based on the workflow and benchmark introduced in this paper. First, students on this topic can experiment with more classifiers and parameters. For example, using a more accurate nonprofit-specific glossary and stemmer \parencite{PaxtonNonprofitSpecificGlossaryStemmer2019}, and a large-scale competition is also in preparation.\footnote{https://***.} We also deposited our working directory with all datasets, source codes, and historical versions on GitHub, enabling future large-scale collaborations on this project possible. Second, scholars are invited to use the Python software package for their own empirical studies and provide any feedback. Third, computational social scientists can apply the workflow in this paper to other domains of inquiry. Last but not least, we are advancing a multi-lingual version of this project to assist the study of/with nonprofits/NGOs in non-English speaking countries. This step is essential to develop the global civil society studies further.


% \section*{Acknowledgments}

% Earlier version of this paper was presented at 2019 West Coast Nonprofit Data Conference. The authors acknowledge the Texas Advanced Computing Center at The University of Texas at Austin for providing cloud computing resources (Chameleon Cloud) that have contributed to the research results reported within this paper.



\sloppy
\printbibliography

\end{document}

